# El problema de la selección de características

En el presente capítulo abordamos el problema de la selección de características, su importancia y desafíos. En ese marco, repasaremos ciertas dificultades asociadas a los datos, como por ejemplo la alta dimensionalidad y el desbalance de clases, y veremos cómo ellas impactan en la selección de características. Luego describiremos brevemente distintos enfoques para la selección de características, como así también ventajas y desventajas de cada uno. A lo largo de todo el capítulo, vincularemos la selección de características con el aporte fundamental de nuestro trabajo asociado a la generación sintética de datos. 

## Estrés, ignorancia y selección de características

Un punto de partida difícil de controvertir en el mundo actual del aprendizaje automático es que la cantidad de información disponible para investigar ha crecido dramáticamente en los últimos veinte años. Conforme la vida se digitaliza, y la información almacenada en los sistemas aumenta, la generación de conocimiento parece menos determinada por el viejo problema de la escasez de datos y más por su nueva situación de abundancia. 

Esta *explosión* [@liWhenChatGPTComputer2023] de información claramente ha hecho particularmente relevante las observaciones de Fischer de hace 90 años, cuando reprochaba el *estrés* con el que se buscaba conocimiento manipulando variables aisladas en vez de indagar las interacciones que se dan entre ellas. Su aguda intuición sobre la *investigación experimental* en el sentido que cuanto más *completa y abarcativa* también es más *eficiente* por el conocimiento que se puede obtener, se ha convertido en un poderoso impulso para considerar espacios de búsqueda cada vez más amplios. [@fisherDesignExperiments1935]

En el dominio del aprendizaje automático, el desafío de trabajar con grandes dimensiones ha motivado la aparición de una serie de técnicas dirigidas a generar conocimiento y resultados precisos en escenarios de alta complejidad por la abundancia de información. Así, bajo el nombre de *selección de características* nació un area de estudio que busca resolver, precisamente, el problema de discernir *sistemáticamente* la información relevante de aquella que no lo es cuando se trabaja con muchas variables.[^bolon-canedo_origen]

[^bolon-canedo_origen] Según Bolón-Canedo y ot., el origen de este campo se remonta a los años '60, cuando Hughes, usando un modelo paramétrico, estudió la precisión de un clasificador bayesiano en función del número de características. En dicho estudio se concluye -señalan las autoras- que: "la selección, reducción y combinación de mediciones no se proponen como técnicas ya desarrolladas. Más bien, son ilustrativas de un marco para investigaciones futuras".[@bolon-canedoFeatureSelectionHighDimensional2015].

Nótese aquí que el acento en lo *sistemático* es definitorio. Gran parte del esfuerzo en la selección de características se centra en disponer de un método para identificar de manera precisa y controlada la información relevante en un problema cuando se ignora cuáles y cómo interactúan los factores que lo determinan. Ciertamente, que esa ignorancia no implica un desconocimiento total del fenómeno en estudio, sino más bien resalta una premisa del método experimental, donde  -como diría Fischer- a *pesar de nuestros fuertes presupuestos  ignoramos qué factores, entre innumerables de ellos, pueden resultar valiosos y dignos de estudio* [@fisherDesignExperiments1935, p.97]. 

En efecto, la sofisticación de las herramientas actuales de investigación ha permitido ampliar los objetos de estudio de una manera impensable años atrás. Hoy no es extraño que tales objetos superen las decenas de miles de dimensiones, como sucede en el cammpo de la investigación biomédica y diseño de materiales, donde los microarrays de ADN y las simulaciones computacionales son ejemplos representativos. [@almugrenSurveyHybridFeature2019] 

Entonces, siguiendo a Bolón-Canedo y ot. [@bolon-canedoFeatureSelectionHighDimensional2015], podemos definir a la *selección de características* como *el proceso de detectar las características relevantes y descartar aquellas irrelevantes o redundantes, con el objetivo de obtener un subconjunto que describa adecuadamente un problema dado con una degradación mínima del rendimiento*. Aquí rendimiento se refiere a la medida de evaluación empleada para juzgar los resultados del proceso de selección. Los distintos enfoques para selección de características adhieren a la premisa de que podemos separar la información relevante de aquella que no lo es para predecir una variable objetivo. 

Bajo esa perspectiva, la selección de características busca determinar un subconjunto de atributos que satisfaga uno de los siguientes criterios[@VignoloGerardFeatureSelection2012]: 

1. El subconjunto con un tamaño especificado que maximice la precisión de la predicción.
2. El subconjunto de menor tamaño que satisfaga un requisito de precisión mínima.
1. El subconjunto que logre el mejor compromiso entre dimensionalidad y precisión.

El criterio a elegir dependerá de los objetivos del estudio y de las características del problema. Mas allá de eso, es fácil advertir que la selección de características supone como ventaja la reducción del espacio de búsqueda, y en cierto sentido es un remedio a la alta dimensionalidad. 

En efecto, en el contexto del aprendizaje automático es común enfrentar problemas representados por grandes conjuntos de variables, vicisitud que se asocia con la *maldición de la dimensionalidad*[@bolon-canedoFeatureSelectionHighDimensional2015]. En general, este fenómeno se presenta por la alta complejidad computacional y costos asociados con la optimización de dichos espacios, volviendo a ciertos problemas intratables desde el punto de vista práctico. Para abordarlo, una alternativa posible es la reducción de dimensionalidad, que consiste en encontrar o construir matrices con menor número de columnas que las originales pero la misma carga de información relevante. Esas matrices de menor dimensión, puede usarse de manera más eficiente para modelar el problema objetivo, facilitando su interpretación y comunicación. El proceso de encontrar estas matrices se llama *reducción de dimensionalidad*, y uno de los métodos para lograrlo es la selección de características. 

Dicho lo anterior, es preciso reconocer que la abundancia de información trae consigo una serie de desafíos que impactan en la selección de características. En el siguiente apartado describiremos aquellos más relevantes para nuestro estudio.

## Las multiples cara de los datos problemáticos

### Alta dimensionalidad y escasez muestral

En primer lugar, un problema frecuente en el aprendizaje automático se presenta cuando disponemos de pocas muestras en un espacio vectorial de alta dimensionalidad, es decir, cuando el número de muestras ($m$) es menor que el número de características ($n$), siendo $n$ particularmente grande. Esta situación, que combina alta dimensionalidad y escasez muestral, es común en campos como el análisis genético, el procesamiento de información médica, entre otros. Como veremos mas adelante, tres de los datasets elegidos para nuestro estudio -vinculados al ámbito biomédico- se encuentran en esta situación. 

La escasez de muestras dificulta el tratamiento de la información, ya que la cantidad de datos disponibles para entrenar un modelo es insuficiente para capturar la variabilidad inherente a los datos. En tales circunstancias, el proceso de aprendizaje tiende a sufrir severas limitaciones, bien sea sobreajustándose a las observaciones disponibles, lo que puede llevar a modelos poco generalizables, o bien resultando incapaz de capturar la estructura subyacente de los datos, lo que puede llevar a modelos poco precisos. 

Por ejemplo, en el campo de la clasificación de perfiles de expresión génica, el problema se considera difícil de resolver debido a la complejidad que supone identificar los genes que contribuyen a la aparición de una enfermedad. [@almugrenSurveyHybridFeature2019] Dada la gran cantidad de genes presentes en estos supuestos (muchos de los cuales pueden ser irrelevantes o tener una influencia mínima en el diagnóstico clínico), basar un modelo en todos ellos puede conducir a resultados erróneos. Este desafío se ve acentuado por la baja densidad de datos —que suele disponer de unas pocas decenas de muestras— y por el hecho de que los genes se encuentran altamente correlacionados. Dichas características dificultan el adecuado funcionamiento de los métodos clásicos de aprendizaje automático, cuyo rendimiento se asocia a la cantidad de muestras disponibles.

Ante este desafío, la selección de características tiene severas limitaciones, ya que la alta dimensionalidad y bajo número de muestras dificultan la distinción entre información relevante y irrelevante. 

### Desbalance de clases

El desbalance de clases se presenta cuando la distribución de las clases en el conjunto de datos es altamente desigual. Típicamente, una clase mayoritaria supera ampliamente las observaciones de una o más clases minoritarias. Este problema puede presentarse en combinación con alta dimensionalidad y escacez muestral, agravando la dificultad de selección de características (tal es el caso de los datasets vinculados al ámbito biomédico elegidos en nuestro estudio, ver capítulo 2).

El desbalance de clases es un problema relevante en aplicaciones donde la o las clases minoritarias son precisamente las de mayor interés, como por ejemplo: el diagnóstico de enfermedades raras, la detección de fraudes bancarios, la identificación de intrusiones en redes y la predicción de fallos en equipos técnicos. En supuestos como estos, los clasificadores que no contemplen un tratamiento explícito del problema tienden a sesgarse hacia la clase mayoritaria, pudiendo alcanzar una alta precisión global mientras fallan en la detección de los casos más importantes pero menos frecuentes.

Para atacar este problema, se han propuesto diversas soluciones, que pueden categorizarse en tres grupos:

1. **Muestreo de datos**: Este enfoque modifica las muestras de entrenamiento para producir una distribución de clases más balanceada. Las técnicas tradicionales incluyen: submuestreo (*undersampling*) donde se cre un subconjunto del conjunto original eliminando instancias; sobremuestreo (*oversampling*) donde se gnera un superconjunto replicando instancias existentes o creando nuevas, y finalmente, métodos híbridos que combinan ambas técnicas de muestreo.

2. **Modificación algorítmica**: Este enfoque adapta los métodos de aprendizaje para que sean más sensibles a los problemas de desbalance. Por ejemplo, el uso de la distancia de Hellinger como criterio de división en árboles de decisión, que ha demostrado ser insensible al sesgo, capturando la divergencia en las distribuciones sin ser dominada por las probabilidades previas de clase.

3. **Aprendizaje sensible al costo**: Esta solución puede incorporar elementos a nivel de datos, a nivel de algoritmos, o ambos a la vez, asignando costos más altos a la clasificación errónea de ejemplos de la clase positiva (minoritaria). Así, en el diagnóstico médico, resulta más importante reconocer la presencia de una enfermedad rara que su ausencia, por ello el costo de un falso negativo es mayor que el de un falso positivo.

El desbalance de clases también supone un desafío para la selección de características en la medida que los métodos de selección estarán sesgados a favorecer aquellas que maximicen la separación de clases. Esto ocurre porque, en escenarios con distribución fuertemente desequilibrada, las técnicas de evaluación tienden a priorizar la identificación de la clase mayoritaria, reduciendo la relevancia de las características que serían valiosas para discriminar a las minoritarias. Como resultado, se corre el riesgo de descartar información relevante para el diagnóstico o predicción de los casos poco frecuentes, dificultando la capacidad del modelo para generalizar y capturar patrones importantes.

### Complejidad 

Siguiendo a Bolón-Canedo y ot. [@bolon-canedoFeatureSelectionHighDimensional2015], la complejidad de los datos puede manifestarse en tres aspectos fundamentales, y todos ellos pueden coexistir en un mismo problema:

1. **Ambigüedad de clases**: Surge cuando los casos no pueden distinguirse utilizando las características disponibles, ya sea porque los conceptos de clase están mal definidos y por lo tanto son intrínsecamente indistinguibles, o porque las características seleccionadas son insuficientes para discriminar las clases.

2. **Complejidad de fronteras**: Se refiere a situaciones donde los límites entre clases requieren una descripción extensa de la frontera de decisión, llegando a demandar, en el caso extremos de complejidad, la enumeración exhaustiva de todos los puntos con sus etiquetas de clase. Este aspecto de dificultad se debe a la naturaleza del problema y no a la muestra o selección de características, indicando que una completa separación entre clases es un problema difícil de resolver.

3. **Dispersión de muestras**: La combinación de pocas muestras y alta dimensionalidad genera una dispersión que dificulta la generación de fronteras de decisión. 

Anque los casos 1 y 2 son problemas ante los cuales la selección de características no puede ofrecer una solución efectiva, el caso 3, asociado con alta dimensionalidad y escasez muestral, podría ser mitigado por estrategias de aumentación de datos. 

### Ruido 

Los datos del mundo real siempre están expuestos a imperfecciones, ruido, proveniente de entornos dinámicos donde las dimensiones de interés de un fenómeno coexisten en espacios de interacciones permanentes. Así, aún considerando un escenario artificial, completamente libre de interferencia, la imperfección puede provenir de diversas fuentes como dispositivos de medición defectuosos o limitados, errores de transcripción o irregularidades en la transmisión de información. 

Ante tales circunstancias, existen cuatro enfoques principales para abordar estas imperfecciones en los conjuntos de datos:

1. **Conservación del ruido**: En este enfoque, se mantiene el conjunto de datos tal como está, con sus instancias ruidosas. Los algoritmos que utilizan los datos se diseñan para ser robustos, es decir, capaces de tolerar cierta cantidad de ruido. Una estrategia común es desarrollar algoritmos que eviten el sobreajuste al modelo (como sucede en el caso de los árboles de decisión) mediante técnicas de poda.

2. **Limpieza de datos**: Este método implica descartar las instancias que se consideran ruidosas según ciertos criterios de evaluación. El clasificador se construye utilizando únicamente las instancias retenidas en un conjunto de datos más pequeño pero más limpio. Sin embargo, este enfoque presenta dos debilidades significativas:
   - Al eliminar instancias completas, se puede descartar información potencialmente útil, como valores de características no corrompidos.
   - Cuando existe una gran cantidad de ruido, la información restante en el conjunto de datos limpio puede resultar insuficiente para construir un clasificador eficaz.

3. **Transformación de datos**: Este enfoque busca corregir las instancias ruidosas en lugar de eliminarlas. Las instancias identificadas como ruidosas se reparan reemplazando los valores corrompidos por otros más apropiados, y luego se reintroducen en el conjunto de datos.

4. **Reducción de datos**: Esta estrategia implica reducir la cantidad de datos mediante la agregación de valores o la eliminación y agrupación de atributos redundantes. La reducción de dimensionalidad -y consecuentemente la selección de características- es una de las técnicas más populares para eliminar características ruidosas (es decir, irrelevantes) y redundantes.

Vistos los problemas que enfrentamos con los datos, pasemos a describir, brevemente, los distintos enfoques que se han propuesto para sortearlos empleando estrategias de selección de características.

### Distintos enfoques para la selección de características

Podemos agrupar los métodos de selección de características en tres grandes grupos:

1. **Filtros**: Se basan en las características generales de los datos de entrenamiento y realizan la selección de características como un paso de preprocesamiento independiente del algoritmo de aprendizaje. El análisis de relevancia de una característica se realiza considerando sus propiedades intrínsecas, sin determinar sus relaciones posibles con otras. El resultado de dicho análisis es una lista de características ordenadas por relevancia (ranking), donde el subconjunto final de características se selecciona según ese orden. Este enfoque es ventajoso por su bajo costo computacional, rapidez y escalabilidad, pero deja sin resolver el problema apuntado por Fisher sobre la interacción entre variables. [^metodos_filtro]

[^metodos_filtro] Los metodos de filtro se pueden sub-clasificar en univariados y multivariados [@solorio-fernandezReviewUnsupervisedFeature2020]. Los primeros analizan cada característica de manera independiente con el fin de obtener una lista ordenada. Este tipo de métodos puede identificar y eliminar eficazmente características irrelevantes, pero no son capaces de eliminar las redundantes, ya que no consideran posibles dependencias entre las características. Ejemplos de estos métodos son: la evaluación utilizando la distribución *chi-cuadrado* , la *ganancia de información*, entre muchos otros  [@bolon-canedoFeatureSelectionHighDimensional2015]. Los métodos filtro multivariados evalúan la relevancia de las características de forma conjunta en lugar de hacerlo individualmente. Por ejemplo, el método de selección hacia adelante (forward selection) y hacia atrás (backward selection) son métodos de filtro multivariados que sucesivamente agregan y eliminan características para obtener un subconjunto óptimo. Los métodos multivariados pueden manejar características redundantes e irrelevantes; por lo tanto, en muchos casos, la precisión alcanzada por los modelos empleando subconjuntos seleccionados con estos métodos puede ser mayor. Otros métodos filtro multivariados puden ser el *método basado en el análisis de correlación* y *algoritmo ReliefF*, entre otros [@bolon-canedoFeatureSelectionHighDimensional2015]. 

1. **Wrappers o métodos envolventes**: Involucran un algoritmo de aprendizaje como caja negra y consisten en usar su resultado para evaluar la utilidad relativa de subconjuntos de características. Aquí, el algoritmo de selección utiliza el método de aprendizaje como una subrutina, para encontrar los subconjuntos de características más relevantes. Esta estrategia es capaz de reconocer variables relevantes y capturar dependencias entre ellas, pero a un costo computacional mayor que los otros métodos. En ese sentido enfrenta desafíos importante cuando el conjunto de datos es de alta dimensionalidad, ya que evaluar $2^n$ subconjuntos de características puede resultar en un problema intratable. Por esa razón, se recurre comunmente a heurísticas de búsqueda capaces de encontrar soluciones adecuadas sin explorar todo el espacio del problema [@zhangFeatureSelectionMultiview2019].

3. **Embedded o métodos embebidos**: Realizan la selección de características durante el proceso de entrenamiento de un modelo de aprendizaje, y suelen ser específicos para determinados algoritmos (e.g. árboles de decisión y métodos derivados de ellos, eliminación recursiva de características mediante SVM, entre otros). A diferencia de los métodos de filtro y wrappers, los métodos embebidos no separan la selección de características del proceso de entrenamiento, sino que la realizan de manera simultánea. En este caso, optimizan una función de pérdida regularizada con respecto a dos conjuntos de parámetros: los parámetros del algoritmo de aprendizaje y los parámetros que indican las características seleccionadas.[@bolon-canedoFeatureSelectionHighDimensional2015]. Este enfoque es capaz de capturar dependencias a un costo computacional menor que los wrappers. 

![Diagrama de enfoques para la selección de características](diagrama_seleccion_características.png)

Bolón-Canedo y ot. nos invitan a pensar que no existe un método que sea *superior a los otros* de manera general, sino que cada uno tiene sus ventajas y desventajas, y se ajustan distinto a diferentes tipos de contexto. De los tres enfoques, los métodos de filtro son los únicos que son independientes del algoritmo de aprendizaje, lo que les permite ser rápidos y eficientes computacionalmente. Sin embargo, los métodos de filtro no consideran la correlación entre características, lo que puede llevar a la selección de características irrelevantes o redundantes. Los métodos de wrappers y embedded, por su parte, consideran la correlación entre características y etiquetas de clase, lo que les permite identificar patrones relevantes correctamente durante la fase de aprendizaje. Sin embargo, los métodos de wrappers y embedded son más complejos computacionalmente y requieren evaluación iterativa del subconjunto seleccionado de características.

### Selección de características y generación sintética de datos

En el ámbito de la selección de características, los algoritmos genéticos (AGs) se han propuesto como un mecanismo de exploración eficiente para la búsqueda de soluciones cercanas al óptimo en espacios de gran dimensionalidad.[@bolon-canedoFeatureSelectionHighDimensional2015] Inspirados en los principios de la evolución natural —que presentaremos en el capítulo 4—, estos algoritmos son capaces de iterar sobre múltiples generaciones de individuos (posibles subconjuntos de características) produciendo soluciones cada vez más relevantes. 

Sin embargo, su eficacia depende en gran medida de la disponibilidad de suficientes datos para evaluar el valor de cada subconjunto. En efecto, la falta de datos suficiente para estimar de forma confiable el desempeño de características seleccionadas afecta la función objetivo, agregando incertidumbre al proceso de selección. Cabe destacar que esta función está representada por un modelo de aprendizaje automático con el que se evalúa el desempeño de los subconjuntos, por tal motivo está sometida a las mismas limitaciones que cualquier proceso de optimización.

Para abordar esta limitación, se han desarrollado estrategias que mitigan el impacto de la escasez de datos, y entre ellas destaca la aumentación mediante Autoencodificadores Variacionales (AVs). Estos modelos generativos, basados en la estimación de distribuciones latentes, son capaces de reconstruir patrones inherentes a los datos originales y generar muestras sintéticas que retienen características estadísticamente relevantes del conjunto real. Así, los AVs se presentan como una herramienta potencialmente transformadora en la etapa de selección de características, al permitir la creación de muestras adicionales que refuerzan la capacidad de discriminación de los modelos basados en AGs.

Asimismo, en escenarios de desbalance de clases —especialmente críticos en problemas multiclase con distribuciones asimétricas—, la generación sintética de datos puede ofrecer la posibilidad de reequilibrar las proporciones de cada categoría. En efecto, al proporcionar suficientes muestras representativas de las clases minoritarias, la generación sintética de datos mejora la discriminación entre categorías poco representadas. Con esta estrategia, se busca reducir el sesgo introducido por la disparidad de muestras y, por ende, robustecer el rendimiento del modelo en su conjunto.

Por último, es importante destacar que la generación sintética de datos mediante AVs también puede contribuir a mitigar el efecto del ruido en los modelos de aprendizaje. Durante el proceso de codificación-decodificación implementado por estos modelos, el proceso de reconstrucción identifica y proyecta las características esenciales de los datos, lo cual favorece la reducción de información irrelevante. Este mecanismo, analizado en mayor detalle en el capítulo 3, abonaría la importancia de los AVs en la creación de muestras sintéticas no solo para reducir el impacto negativo del ruido sobre el proceso de entrenamiento, sino también para mejorar el desempeño de los métodos evolutivos en la búsqueda de características óptimas.

Siguiendo estas ideas, la tesis se organiza de la siguiente manera: 

- Capítulo 2: Se realiza un análisis preliminar, describiendo los modelos de aprendizaje automático utilizados y las características de los conjuntos de datos con los que trabajaremos. Además, se presentan las métricas de evaluación aplicadas a lo largo del estudio y se analizan potenciales dificultades que enfrentaremos.
- Capítulo 3: Aborda la teoría de los Autocodificadores Variacionales (AVs) y la motivación para usarlos en la generación de datos sintéticos. Se describen los experimentos de aumentación de datos y se presentan los resultados obtenidos, enfatizando la forma en que estos modelos capturan la estructura subyacente de la distribución original y potencian la calidad de los datos sintéticos.
- Capítulo 4: Presenta la teoría de los Algoritmos Genéticos (AGs) y luego describe la integración de los AVs como etapa de preprocesamiento en la selección de características con AGs. Se detallan los experimentos realizados tanto con datos originales como con datos aumentados, demostrando el impacto que la generación sintética ejerce en el rendimiento de los métodos de selección de características.
- Capítulo 5: Expone las conclusiones de la investigación, destacando la relevancia de la generación sintética de datos para la selección de características en escenarios complejos, y ofrece perspectivas de trabajo futuro donde las técnicas propuestas podrían ampliarse o refinarse.

Bajo este planteo, la presente tesis pretende aportar una estrategia integral que vincula la selección de características con la generación sintética de datos, ofreciendo soluciones potenciales a problemas donde la dimensionalidad, el desbalance de clases, la escasez de muestras y el ruido representan barreras habituales en el campo del aprendizaje automático.