# Autocodificadores Variacionales y datos sintéticos {#sec-Capitulo3}

En este capítulo presentamos la arquitectura del Autocodificador Variacional (AV) que emplearemos para la generación de datos sintéticos: exponemos brevemente sus fundamentos teóricos, los pasos que hemos seguido en su implementación y las variaciones introducidas para su apropiada aplicación a los problemas abordados. También presentamos las métricas empleadas para evaluar los modelos y las configuraciones más relevantes para la generación de datos sintéticos. Finalmente, concluimos el capítulo comentando los hallazgos más destacados de todo este desarrollo.

## Modelos generativos

Los modelos generativos (MG) son un amplio conjunto de algoritmos de aprendizaje automático que buscan modelar la distribución de probabilidad de datos observados $p_\theta(x)$. A diferencia de los modelos discriminantes (MD), cuyo objetivo es aprender un predictor a partir de los datos, en los modelos generativos el objetivo es *resolver un problema más general vinculado con el aprendizaje de la distribución de probabilidad conjunta de todas las variables*. Así, siguiendo a Kingma, podemos decir que *un modelo generativo simula la forma en que los datos son generados en el mundo real* [@kingmaIntroductionVariationalAutoencoders2019]. Dada estas propiedades, estos modelos permiten crear nuevos datos que se asemejan a los originales, y se aplican en tareas de generación de datos sintéticos, imputación de datos faltantes, reducción de dimensionalidad y selección de características, entre otros.

Los modelos generativos pueden tener como *inputs* diferentes tipos de dato, como imágenes, texto, audio, información tabular, etc. Por ejemplo, las imágenes son un tipo de dato para los cuales los MG han demostrado gran efectividad. En este caso, cada dato de entrada $x$ es una imagen que puede estar representada por un vector de miles de elementos  que corresponden a los valores de píxeles. El objetivo de un modelo generativo es aprender las dependencias [@doerschTutorialVariationalAutoencoders2021]  entre los píxeles (e.g. pixeles vecinos tienden a tener valores similares) y poder generar nuevas imágenes que se asemejen a las imágenes originales.

Podemos formalizar esta idea asumiendo que tenemos ejemplos de datos $x$, distribuidos según una distribución de probabilidad conjunta no conocida que queremos modelo $p_\theta(x)$ para que sea capaz de generar datos similares a los originales. 

## Autocodificadores

Los autocodificadores son un tipo de MG especializado en la representación de un espacio de características dado en un espacio de menor dimensión [@delatorreAutocodificadoresVariacionalesVAE2023].  El objetivo de esta transformación es obtener una representación de baja dimensionalidad y la mayor fidelidad posible del espacio original. Para ello el modelo aprende a preservar la mayor cantidad de información relevante en un vector denso de menos dimensiones que las originales, y descarta -al mismo tiempo- lo irrelevante. Luego, a partir de esa información codificada, se busca reconstruir los datos observados según el espacio original.

Los autocodificadores se componen de dos partes: un *codificador* y un *decodificador*. El *codificador* es una función no lineal que opera sobre una observación o patrón $x_i$ y la transforma en un vector de menor dimensión $z$, mientras que el *decodificador* opera a partir del vector $z$ y lo transforma en una observación $x_i'$ (o patrón reconstruido), buscando que se asemeje a la observación original. Este vector de menor dimensión $z$ es conocido como *espacio latente*.

![Ejemplo esquemático de un autocodificador](autocodificadores.png)

El esquema anterior muestra un autocodificador que transforma un patrón $x$ en un vector denso $z$, con dimensiones menores a las originales. Este vector es usado luego por el decodificador para reconstruir el patrón original, dando lugar a un patrón reconstruido $x'$.

En el proceso de aprendizaje de un autocodificador, la red modela la distribución de probabilidad de los datos de entrada $x$ y aprende a mapearlos a un espacio latente $z$. Para ello, se busca minimizar la diferencia entre la observación original $x_i$ y la reconstrucción $x_i'$, diferencia que se denomina *error de reconstrucción*. Esta optimización se realiza a través de una *función de pérdida* que se define como la diferencia entre $x_i$ y $x_i'$, que permite la optimización simultánea del codificador y decodificador.

Formalmente, podemos establecer estas definiciones [@delatorreAutocodificadoresVariacionalesVAE2023]: 

- Sea $x$ el espacio de características de los datos de entrada y $z$ el espacio latente, ambos espacios son euclidianos, $x = \mathbb{R}^m$ y $z = \mathbb{R}^n$, donde $m > n$.
- Sea las siguientes funciones paramétricas $C_\theta: x \rightarrow z$ y $D_\phi: z \rightarrow x'$ que representan el codificador y decodificador respectivamente.
- Entonces para cada observación $x_i \in x$, el autocodificador busca minimizar la función de pérdida $L(x_i, D_\phi(E_\theta(x_i)))$. Ambas funciones $E_\theta$ y $D_\phi$ son redes neuronales profundas que se entrenan simultáneamente.

Para optimizar un autocodificador se requiere una función que permita medir la diferencia entre la observación original y la reconstrucción. Esta diferencia usualmente se basa en la *distancia euclidia* entre $x_i$ y $x_i'$, es decir, $||x_i - x_i'||^2$. La función de pérdida se define como la suma de todas las distancias a lo largo del conjunto de datos de entrenamiento. Tenemos entonces que: 

> $L(\theta, \phi)$ =  $argmin_{\theta, \phi} \sum_{i=1}^{N} ||x_i - D_\phi(C_\theta(x_i))||^2$

Donde $L(\theta, \phi)$ representa la función de pérdida que queremos minimizar: $\theta$ son los parámetros del codificador $C$ y $\phi$ son los parámetros del decodificados $D$.

## Autocodificadores y el problema de la generación de datos

En el proceso de aprendizaje antes descripto, la optimización no está sujeta a otra restricción mas que  minimizar la diferencia entre la observación original y la reconstrucción, dando lugar a espacios latentes generalmente discontinuos. Esto sucede porque la red puede aprender a representar los datos de entrada de manera eficiente sin necesidad de aprender una representación continua. En la arquitectura del autocodificador no hay determinantes para que dos puntos cercanos en el espacio de características se mapeen a puntos cercanos en el espacio latente. 

Esta discontinuidad en el espacio latente hace posible que ciertas regiones de este espacio no tengan  relación significativa con el espacio de características. Durante el entrenamiento el modelo simplemente no ha tenido que reconstruir datos cuyas distribuciones coincidan con estas regiones. Esto es un problema en la generación de datos, ya que la red podrá generar representaciones alejadas de los datos originales. Regularmente lo que se busca en los MG, no es simplemente una generación de datos completemante igual o totalmente distintos a los orginales, sino cierta situación intermedia donde los nuevos datos introducen variaciones en características específicas. 

![Discontinuidad del espacio latente](espacio_latente_discontinuo.png)

## Autocodificadores Variacionales

<!---
1. An Introduction to Variational Autocodificadors, KINGMA,2019
2. (Amazing) https://towardsdatascience.com/intuitively-understanding-variational-autocodificadors-1bfe67eb5daf
-->

Los Autocodificadores Variacionales (AV) buscan resolver los problemas de discontinuidad y falta de regularidad en el espacio latente de los Autocodificadores. Comparten con éstos la arquitectura *codificador-decodificador*, pero introducen importantes modificaciones en su diseño para crear un espacio latente continuo. 

Estos modelos, a diferencia de los autocodificadores que realizan transformaciones determinísticas de los datos de entrada (codificándolos como vectores *n*-dimensionales), buscan modelar la distribución de probabilidad de dichos datos aproximando la distribución *a posteriori* de las variables latentes $p_\theta(z|x)$. Para ello, la codificación se produce mediante la generación de dos vectores ($\mu$ y $\sigma$)  que conforman el espacio latente, a partir del cual se toman las muestras para la generación.

La red codificadora, también llamada *red de reconocimiento*, mapea los datos de entrada $x$ a los vectores $\mu$ de medias y $\sigma$ de desvíos estándar, que parametrizan una distribución de probabilidad en el espacio latente. Generalmente, esta distribución es una distribución  simple, como la distribución normal multivariada. La red decodificadora, también llamada *red generativa*, toma muestras de esta distribución para generar un vector, y lo transforma según la distribución de probabilidad preexistente del espacio de características. De esta manera, se generan nuevas instancias que respetan la distribución de probabilidad de los datos originales. Estas transformaciones implican que, incluso para el mismo dato observado (donde los parámetros de $z$ son iguales), el dato de salida podrá ser diferente debido al proceso estocástico de reconstrucción.

![Autocoficadores Variacionales](autocodificadores_variacionales.png)

Una forma de entender esta arquitectura sería relacionar los vectores que componen $z$ como 'referecias', donde el vector de medias controla el *centro* en torno al cual se distribuirán los valores codificados de los datos de entrada, mientras que el vector de los desvíos traza el *área* que pueden asumir dichos valores en torno al *centro*. 

Para indagar en estas intuiciones, veamos la solución que proponen los AV detenidamente, utilizando un enfoque formal. Así, dado un conjunto de datos de entrada $x = \{x_1, x_2, ..., x_N\}$, donde $x_i \in \mathbb{R}^m$, se asume que cada muestra es generada por un mismo proceso o sistema subyacente cuya distribución de probabilidad se desconoce. El modelo buscado procura aprender $p_\theta(x)$, donde $\theta$ son los parámetros de la función. Por las ventajas que ofrece el logaritmo[^ventajaslogaritmo] para el cálculo de las distribuciones de probabilidad tendremos la siguiente expresión: $\log p_\theta(x) = \sum_{x_i \in x} \log p_\theta(x)$[^flogverosimilitud].

[^ventajaslogaritmo]: El logaritmo convierte la probabilidad conjunta (que se calcula como el producto de las probabilidades condicionales) en una suma de logaritmos, facilitando el cálculo y evitando problemas de precisión numérica: $\log(ab) = \log(a)+\log(b)$. 

[^flogverosimilitud]:Esta función se lee como la log-verosimilitud de los datos observados $x$ bajo el modelo $p_\theta(x)$ y es igual a la suma de la log-verosimilitud de cada dato de entrada $x_i$. 

<!---
An Introduction to Variational Autocodificadors, KINGMA,2019
-->

La forma más común de calcular el parámetro $\theta$ es a través del estimador de *máxima verosimilitud*, cuya función de optimización es: $\theta^* = \arg \max_\theta \log p_\theta(x)$, es decir, buscamos los parámetros $\theta$ que maximizan la log-verosimilitud asignada a los datos por el modelo. 

En el contexto de los AV, el objetivo es modelar la distribución de probabilidad de los datos observados $x$ a través de una distribución de probabilidad conjunta de variables observadas y latentes: $p_\theta(x, z)$. Aplicando la regla de la cadena de probabilidad podemos factorizar la distribución conjunta de la siguiente manera: $p_\theta(x, z) = p_\theta(x|z) p_\theta(z)$. Aquí $p_\theta(x|z)$ es la probabilidad condicional de los datos observados dados los latentes, y $p_\theta(z)$ es la probabilidad *a priori*[^apriori] de los latentes.

[^apriori]: La expresión *a priori* alude a que no está condicionada por ningun dato observado.

Para determinar la distribución marginal respecto de los datos observados, es preciso integrar sobre todos los elementos de $z$, dando como resultado la siguiente función: $p_\theta(x) = \int p_\theta(x,z)dz$ [^zdiferenciacion]. 

[^zdiferenciacion]: Aquí $dz$ es el diferencial de $z$, por lo que la expresión indica la integración sobre todas las posibles configuraciones de la variable latente.

Esta distribución marginal puede ser extremadamente compleja, y contener un número indeterminable de dependencias [@kingmaIntroductionVariationalAutoencoders2019], volviendo el calculo de la verosimilitud de los datos observados intratable. Esta intratabilidad de $p_\theta(x)$ está determinada por la intratabilidad de la distribución *a posteriori* $p_\theta(z|x)$, cuya dimensionalidad y multi-modalidad pueden hacer difícil cualquier solución analítica o numérica eficiente. Dicho obstáculo impide la diferenciación y por lo tanto la optimización de los parámetros del modelo. 

Para abordar este problema, se acude a la inferencia variacional que introduce una aproximación $q_\phi(z|x)$ a la verdadera distribución *a posteriori* $p_\theta(z|x)$. Generalmente se emplea la distribución normal multivariada para aproximar la distribución *a posteriori*, con media y varianza parametrizadas por la red neuronal[^normalmultivariada]. Sin embargo, la elección de la distribución no necesariamente tiene que pasar por una distribución normal, el único requerimiento es que sea una distribución que permita la diferenciación y el cálculo de la divergencia entre ambas distribuciones (por ejemplo si $X$ es binaria la distribución $p_\theta(x|z)$ puede ser una distribución Bernoulli).

[^normalmultivariada]: En AV, se suele asumir que $z$ sigue una distribución normal multivariada: $p_\theta(z) = \mathcal{N}(z; 0, I)$, con media cero y matriz de covarianza identidad. La matriz de covarianza identidad es una matriz diagonal con unos en la diagonal y ceros en los demás lugares, y su empleo simplifica la implementación del modelo, permite que las variables latentes sean independientes (covarianza = 0) y varianza unitaria, evitando así cualquier complejidad vinculada a las dependencias entre dimensiones de $z$.

Así, en lugar de maximizar directamente el logaritmo de la verosimilitud (*log-verosimilitud*), se maximiza una cota inferior conocida como *límite inferior de evidencia* (*ELBO* por sus siglas en ingles). La derivación procede de la siguiente manera:

<!---
https://chatgpt.com/share/be24e1ef-14a3-40e4-a4c1-57ab925daed3
-->
1. log-verosimilitud marginal (intratable):   
   
   $\log p_\theta(x) = \log \left( \int p_\theta(x, z) \, dz \right)$,

2. aplicando inferencia variacional:   
   
   $\log p_\theta(x) = \log \left( \int q_\phi(z|x) \frac{p_\theta(x, z)}{q_\phi(z|x)} \, dz \right)$,

3. aplicando la desigualdad de Jensen[^desigualdadJensen]:   
   
   $\log p_\theta(x) \geq \mathbb{E}_{q_\phi(z|x)} \left[ \log \left( \frac{p_\theta(x, z)}{q_\phi(z|x)} \right) \right]$,

4. descomponiendo la fracción dentro del logaritmo:   
   
   $\log p_\theta(x) \geq \mathbb{E}_{q_\phi(z|x)} \left[ \log p_\theta(x|z) + \log p_\theta(z) - \log q_\phi(z|x) \right]$,

5. el resultando es el límite inferior de evidencia:       
   
> $\log p_\theta(x) \geq \mathbb{E}_{z \sim q_\phi(z|x)} [\log p_\theta (x|z)] - D_{\text{KL}}(q_\phi(z|x) \| p_\theta(z))$, 

[^desigualdadJensen]: Nótese que ese límite es siempre menor o igual y esto se deriva de una de las propiedades de las funciones convexas. Esta propiedad, denominada *desigualdad de Jensen*, establece que el valor esperado de una función convexa es siempre mayor o igual a la función del valor esperado. Es decir, $\mathbb{E}[f(x)] \geq f(\mathbb{E}[x])$. En el caso de funciones cóncavas, la desigualdad se invierte: $\mathbb{E}[f(x)] \leq f(\mathbb{E}[x])$. En este caso, la función logaritmo es cóncava, por lo que la desigualdad se expresa como: $\log(\mathbb{E}[x]) \geq \mathbb{E}[\log(x)]$.

donde:

- $\mathbb{E}_{z \sim q_\phi(z|x)} [\log p_\theta (x|z)]$ es el valor esperado (*esperanza*[^esperanza]) de la log-verosimilitud bajo la aproximación variacional, y determina la precision de la reconstrucción de los datos de entrada (un valor alto de esta esperanza indica que el modelo es capaz de reconstruir los datos de entrada con alta precisión a partir de los parámetros generados por $q_\phi(z|x)$).
- $D_{\text{KL}}(q_\phi(z|x) \| p_\theta(z))$ es la divergencia de Kullback-Leibler entre la distribución $q_\phi(z|x)$ y la distribución *a priori* de las variables latentes  $p_\theta(z)$, y determina la regularización del espacio latente.

<!---
https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained
-->
  
[^esperanza]:La esperanza es un promedio ponderado de todos los posibles valores que puede tomar una variable aleatoria, donde los pesos son las probabilidades de esos valores. En un AV, donde consideramos una distribución aproximada $q_\phi(z|x)$ para el espacio latente, la expresión citada es la esperanza de la log-verosimilitud bajo esta distribución.Aunque teóricamente esto implica un promedio sobre todas las posibles muestras $z$  de la distribución $q_\phi(z|x)$, en la práctica, esta esperanza se estima utilizando una única muestra durante el entrenamiento por razones de  eficiencia computacional. Esta única muestra permite calcular directamente $\log p_\theta (x_i|z_i)$, proporcionando una aproximación a la esperanza teórica y determinando la precisión de la reconstrucción de los datos de entrada.

Maximizando esta cota inferior (*ELBO*), se optimizan simultáneamente los parámetros $\theta$ del modelo y los parámetros $\phi$ de la distribución empleada en la aproximación, permitiendo una inferencia eficiente y escalable en modelos con $z$ de alta dimensionalidad [^cambiosignoencodigooptimizacion].

[^cambiosignoencodigooptimizacion]:En la teoría, cuando derivamos el objetivo de un AV, estamos maximizando la cota inferior variacional (ELBO), para que la aproximación sea lo más cercana posible a la verdadera distribución de los datos. LLevado el problema a una implementación práctica generalmente se emplean optimizadores (SGD, Adam, etc.) que minimizan una función de pérdida. Para convertir el problema de maximización del ELBO en un problema de minimización, simplemente negamos el ELBO, resultando que los términos de la ecuación se reescriben como suma de cantidades positivas. El error o pérdida de reconstrucción se mide, según los casos, mediante MSE o entropía cruzada. 

El objetivo de aprendizaje del AV se da entonces por:

> $\mathcal{L}_\theta,_\phi(x) = \max(\phi,\theta) \left( E_{z \sim q_\phi(z|x)} [\log p_\theta (x|z)] - D_{\text{KL}}(q_\phi(z|x) \| p_\theta(z)) \right)$.

Como puede apreciearse en la ecuación anterior la función de pérdida del AV se compone de dos términos: el primero es la esperanza de la log-verosimilitud bajo la aproximación variacional y el segundo es la divergencia de Kullback-Leibler relacionada a la reconstrucción de los datos y la regularización del espacio latente. Existe entre ambos términos una relación de compromiso que permite al AV aprender una representación adecuada de los datos de entrada y, al mismo tiempo, un espacio latente continuo y regularizado. 

En términos prácticos, la divergencia de Kullback-Leibler puede interpretarse como un factor que equilibra creatividad y precisión en el modelo. Por un lado, cuando esta divergencia es grande (KL >> 1), el espacio latente tiende a estar más regularizado y la distribución de probabilidad de los datos generados se vuelve más “suave”, pero el modelo puede terminar sobreajustándose a las muestras originales y perder capacidad de generalización. Por otro lado, si la divergencia es muy pequeña (KL << 1), el espacio latente queda menos regularizado y el modelo intenta reproducir con mayor fidelidad la distribución real de los datos de entrada, aunque a costa de generar muestras más ruidosas y posiblemente descuidar detalles esenciales. De esta manera, la elección del valor de KL refleja el compromiso entre capturar información detallada de los datos y mantener una representación latente estable y bien estructurada para la generación.

<!---
Girin, Dynamical Variational Autocodificadors: A Comprehensive Review
-->

## Presentación de nuestros modelos de AV y AVC

Luego del análisis precedente exponemos a continuación los modelos de AV y AVC que desarrollamos para la generación de datos sintéticos. El desarrollo de dichos modelos se cumplió en dos etapas: en la primera se centró el esfuerzo en el diseño y validación de las arquitecturas más apropiadas para los problemas abordados, mientras que en la segunda se enfocó en su optimización para la generación. A continuación describiremos brevemente este proceso y la configuración final de los modelos elegidos para los experimentos de aumentación. 

La primera etapa comenzó con la creación de una versión exploratoria de los modelos AV y AVC, con la finalidad de establecer una base sobre la cual iterar en mejoras sucesivas. Se optó por redes de 2 capas en el codificador y en el decodificador en ambos casos, siguiendo la estructura descripta precedentemente. 

El codificador incluyó dos capas lineales, cada una seguida de una activación ReLU, un diseño que sigue la lógica de la transformación no lineal en un espacio de menor dimensión para luego reconstruir la observación original a partir del espacio latente. Como se discutió en la primera parte, el codificador genera dos vectores, uno para la media y otro para la varianza logarítmica de la distribución latente, componentes críticos para el proceso de reparametrización que permite al modelo generar nuevas muestras en el espacio latente. El decodificador, encargado de reconstruir los datos originales a partir del espacio latente, fue diseñado con una estructura simétrica a la del codificador, utilizando nuevamente activaciones ReLU.

La función de pérdida de ambos modelos combinó la *divergencia Kullback-Leibler* (KLD) y el *error cuadrático medio* (MSE). La KLD se utilizó para medir la diferencia entre la distribución aprendida por el modelo y una distribución normal estándar. El MSE y MSE balanceado para los problemas multiclases se emplearon para evaluar el error de reconstrucción, es decir, qué tan bien el modelo era capaz de replicar los datos de entrada a partir del espacio latente. 

Cabe acalarar que, en escenarios orientados a la generación, consideramos particularmente apropiado emplear MSE para la función de reconstrucción, ya que permite aproximar cada dimensión de la muestra de manera continua. Métricas orientadas a la clasificación, como por ejemplo la entropía cruzada, penalizan con fuerza cuando la clase verdadera difiere de la predicha, mientras que la MSE interpreta la diferencia entre la muestra real y la reconstruida como una distancia. Entendemos que esta característica beneficia el proceso de entrenamiento porque se ajusta a la naturaleza continua de los datos generados y mantiene una penalización homogénea en cada dimensión.

Los modelos resultantes se probaron en la generación de datos sintéticos en un dataset de clases binarias: Madelon, y un dataset multiclases: GCM. Para evaluar los datos generados se realizaron experimentos de clasificación utilizando un Perceptron Multicapa (MLP), comparando los resultados obtenidos en el dataset original y en el dataset con muestras sintéticas. Los resultados de esta evaluación fueron satisfactorios, logrando una versión inicial de los modelos AV y AVC que permitían generar datos sintéticos. Sin perjuicio de ello, considerando el bajo desempeño del MLP sobre los datos sintéticos en comparación con el MLP sobre los datos originales, quedó en evidencia la baja calidad de reconstrucción que tenían ambos modelos. Circunstancia que nos llevó a buscar una arquitectura más adecuada.

Así, en respuesta a los problemas identificados previamente, se exploraron diferentes configuraciones de arquitectura para el AV y el AVC, determinándose que una configuración con tres capas lineales en el codificador y el decodificador, cada una con activaciones ReLU seguidas de normalización por lotes, brindaba los mejores resultados. 

La técnica de normalización por lotes fue seleccionada debido a su capacidad para estabilizar y acelerar el proceso de entrenamiento, promoviendo la rápida convergencia y mejorando la precisión de la reconstrucción. Al mitigar el problema de desplazamiento de covariables (*covariate shift*) durante el entrenamiento, la normalización por lotes estabiliza  las activaciones intermedias de la red y permite que la información relevante sea conservada a lo largo de las capas. 

Dado que los modelos fueron ajustados para capturar la estructura de los datos a través de capas lineales y normalización por lotes, mantuvimos la elección de ReLU como función de activación dada su eficiencia computacional.

Los modelos resultantes fueron entrenados con un optimizador Adam y una tasa de aprendizaje en el rango de [1e-5, 1e-3]. Se experimentó con diferentes tamaños del espacio latente, evaluando el equilibrio entre la calidad de reconstrucción y la capacidad de generalización del modelo. Se empleó un término de paciencia para detener el entrenamiento si no se observaba mejora en los datos de validación durante 10 épocas consecutivas. 

Estos experimentos fueron clave para ajustar los modelos generativos a las necesidades específicas de los conjuntos de datos utilizados en la investigación, permitiendo una generación de datos sintéticos que no solo replicara los patrones de los datos originales, sino que también capturara la variabilidad inherente a estos. Las arquitecturas finales de los modelos se detallan a continuación.

**Arquitectura del Autocodificador Variacional**

La arquitectura del AV está compuesta por tres capas lineales, cada una seguida de una normalización por lotes (Batch Normalization) y una activación ReLU. El proceso de codificación se realiza de la siguiente manera:

$h_1 = \text{ReLU}(\text{BatchNorm}(\mathbf{W_1}x + b_1))$    
$h_2 = \text{ReLU}(\text{BatchNorm}(\mathbf{W_2}h_1 + b_2))$     
$h_3 = \text{ReLU}(\text{BatchNorm}(\mathbf{W_3}h_2 + b_3))$     

Donde:

- $\mathbf{W_1}$ es una matriz de pesos que transforma el vector de entrada $x$ al espacio de características de dimensión $|h_1|$.   
- $\mathbf{W_2}$ transforma $h_1$ a un espacio de características de dimensión $|h_2|$.   
- $\mathbf{W_3}$ mantiene la dimensión $|h_2|$ mientras transforma $h_3$.    
- $b_1$, $b_2$, y $b_3$ son los sesgos correspondientes a cada capa.

Después de las tres capas, se generan los vectores latentes $\mu$ y $\log(\sigma^2)$ mediante dos capas lineales independientes que también aplican normalización por lotes.

**Reparametrización**

El vector latente $z$ se obtiene mediante la técnica de reparametrización, donde se introduce ruido gaussiano para permitir la retropropagación del gradiente:

$z = \mu + \sigma \times \epsilon$,

donde $\epsilon$ es una variable aleatoria con distribución normal estándar, y $\sigma$ se calcula a partir de $\log(\sigma^2)$.

**Decodificador**

El decodificador reconstruye el vector de entrada a partir del vector latente $z$ utilizando una arquitectura de tres capas lineales, cada una seguida por una normalización por lotes y una activación ReLU:

$h_4 = \text{ReLU}(\text{BatchNorm}(\mathbf{W_4}z + b_4)),$    
$h_5 = \text{ReLU}(\text{BatchNorm}(\mathbf{W_5}h_4 + b_5)),$    
$\hat{x} = \text{BatchNorm}(\mathbf{W_6}h_5 + b_6),$     

donde:

- $\mathbf{W_4}$ transforma el vector latente $z$ al espacio de características de dimensión $|h_4|$.     
- $\mathbf{W_5}$ transforma $h_4$ al espacio de características de dimensión $|h_5|$.   
- $\mathbf{W_6}$ transforma $h_5$ de regreso al espacio de la dimensión original de la entrada $|D_{in}|$.   
- $b_4$, $b_5$, y $b_6$ son los sesgos correspondientes a cada capa. 

Finalmente, la salida $\hat{x}$ es una aproximación reconstruida de la entrada original $x$.

### Modelo AVC para datos multiclase

Para abordar los dataset GCM y All Leukemia, que contiene multiples clases con distribuciones desiguales, se creó un AV Condicional (AVC) que combina la capacidad de generación de un AV tradicional con el condicionamiento explícito en las etiquetas de clase. El AVC propuesto permitió una modelización más precisa de los datos, al incorporar información de clase en el proceso de codificación y decodificación.

En escenarios donde los datasets están desbalanceados, los modelos generativos pueden tender a favorecer las clases mayoritarias, ignorando las minoritarias. Para abordar el desbalance de clases que presenta GCM, se implementó una estrategia de ponderación de clases dentro de la función de pérdida, penalizando de manera diferenciada los errores de reconstrucción en función de la clase, buscando mejorar así la capacidad del modelo para representar adecuadamente las clases minoritarias.

**Arquitectura del Autocodificador Variacional Condicional**

La arquitectura del AVC se basa en una modificación del AV tradicional para incorporar información adicional en forma de etiquetas. Esta información se concatena tanto en la fase de codificación como en la de decodificación, permitiendo que el modelo aprenda distribuciones condicionales.

**Codificador**

El codificador del AVC combina la entrada original con las etiquetas antes de ser procesadas por una secuencia de capas lineales (3 capas), cada una seguida por una normalización por lotes (Batch Normalization) y una activación ReLU. El proceso de codificación se realiza de la siguiente manera:

$h_1 = \text{ReLU}(\text{BatchNorm}(\mathbf{W_1}[x, y] + b_1)),$   
$h_2 = \text{ReLU}(\text{BatchNorm}(\mathbf{W_2}h_1 + b_2)),$   
$h_3 = \text{ReLU}(\text{BatchNorm}(\mathbf{W_3}h_2 + b_3)),$    

donde:

- $[x, y]$ es la concatenación del vector de entrada $x$ con las etiquetas $y$.
- $\mathbf{W_1}$ es una matriz de pesos que transforma el vector combinado $[x, y]$ al espacio de características de dimensión $|h_1|$.
- $\mathbf{W_2}$ transforma $h_1$ a un espacio de características de dimensión $|h_2|$.
- $\mathbf{W_3}$ mantiene la dimensión $|h_2|$ mientras transforma $h_3$.  
- $b_1$, $b_2$, y $b_3$ son los sesgos correspondientes a cada capa.

Al igual que en el AVC, se generan los vectores latentes $\mu$ y $\log(\sigma^2)$ mediante dos capas lineales independientes.

**Reparametrización**

El vector latente $z$ se obtiene mediante la técnica de reparametrización, similar al AV tradicional:

$z = \mu + \sigma \times \epsilon$,

donde $\epsilon$ es una variable aleatoria con distribución normal estándar, y $\sigma$ se calcula a partir de $\log(\sigma^2)$.

**Decodificador**

El decodificador del AVC reconstruye el vector de entrada a partir del vector latente $z$ y las etiquetas $y$, utilizando una arquitectura de tres capas lineales, cada una seguida por una normalización por lotes y una activación ReLU:

$h_4 = \text{ReLU}(\text{BatchNorm}(\mathbf{W_4}[z, y] + b_4)),$       
$h_5 = \text{ReLU}(\text{BatchNorm}(\mathbf{W_5}h_4 + b_5)),$     
$\hat{x} = \text{BatchNorm}(\mathbf{W_6}h_5 + b_6),$         

donde:

- $[z, y]$ es la concatenación del vector latente $z$ con las etiquetas $y$.
- $\mathbf{W_4}$ transforma el vector combinado $[z, y]$ al espacio de características de dimensión $|h_4| + \text{labels\_length}$.
- $\mathbf{W_5}$ transforma $h_4$ al espacio de características de dimensión $|h_5|$.
- $\mathbf{W_6}$ transforma $h_5$ de regreso al espacio de la dimensión original de la entrada $|D_{in}|$.  
- $b_4$, $b_5$, y $b_6$ son los sesgos correspondientes a cada capa.

Finalmente, la salida $\hat{x}$ es una aproximación reconstruida de la entrada original $x$, condicionada por las etiquetas $y$.

Elementos distintivos respecto a la arquitectura anterior:

- Incorporación de etiquetas: Tanto en el codificador como en el decodificador, se concatenan las etiquetas $y$ con las entradas y el vector latente, respectivamente.
- Dimensiones ajustadas: Se han ajustado las dimensiones de las capas para acomodar las etiquetas, reflejadas en las matrices de pesos y las normalizaciones por lotes.
- Capas adicionales en la fase de decodificación: Se añaden capas y ajustes para manejar las etiquetas adicionales en el proceso de decodificación.

## Parametrización de los modelos AV y AVC

La búsqueda y ajuste de hiperparámetros para los modelos de AV y AVC ha sido un proceso crucial para optimizar la generación de datos sintéticos. Asumíamos que este proceso era determinante para favorecer la estrategia de selección de características mediante el *algoritmo genético*, por esa razón analizamos distintas opciones para la configuración de los modelos.

Iniciamos esta tarea realizando una búsqueda de hiperparámetros, ajustando aspectos clave como el tamaño de las dimensiones latentes, la tasa de aprendizaje, y el número de neuronas en las diferentes capas. Se utilizaron tanto Grid Search como Optimización Bayesiana (BO). Cada una de estas técnicas tiene sus fortalezas, y la elección entre ellas depende en gran medida del objetivo de la búsqueda. Grid Search, por ejemplo, permite un control total sobre el espacio de búsqueda, lo que es útil para responder preguntas específicas, como la configuración óptima de la dimensión latente. Sin embargo, la BO demostró ser particularmente eficiente en la exploración de un espacio de hiperparámetros más amplio y menos definido, logrando un equilibrio entre la exploración y la explotación que resultó especialmente útil en nuestra investigación dado el tamaño del espacio de búsqueda.

También exploramos técnicas para evitar el sobreajuste, como un mecanismo de paciencia para detener el entrenamiento si no se observaba mejora durante 10 épocas consecutivas en datos de validación. Además, se experimentó con el uso de dropout como técnica de regularización; se probaron tasas de dropout en un rango de 0.05 a 0.5 en distintas configuraciones de AVC, tanto con arquitecturas pequeñas (100-500 neuronas por capa) como grandes (1000-7000 neuronas por capa). 

Testeamos distintas métricas de pérdida, como L1 y Categorical Cross Entropy para evaluar si podría ofrecer mejoras.

Para abordar el problema del desbalance en conjuntos de datos multiclases, se implementaron ajustes específicos en la configuración del modelo y en las estrategias de entrenamiento. El primero consistió en la asignación de pesos diferenciados a las clases dentro de la función de pérdida, con el objetivo de aumentar la penalización por errores de clasificación en las clases minoritarias. Como vimos en el primer capítulo, el uso de pesos de clase es una técnica comúnmente utilizada para abordar este problema. Estos pesos se calcularon de manera inversamente proporcional a la frecuencia de las clases en el conjunto de datos, lo que significa que las clases con menos instancias recibieron pesos más altos. Al incorporar estos pesos de clase en la función de pérdida, nos aseguramos que los errores en las clases con menor representación tuvieran un impacto mayor durante el proceso de optimización, logrando un aprendizaje más equilibrado.

Además de ajustar la función de pérdida para mitigar el desbalance, se implementó una estrategia de muestreo ponderado en el proceso de entrenamiento. A través de un muestreador aleatorio ponderado, nos aseguramos que cada mini-lote de datos durante el entrenamiento tuviera una representación equilibrada de cada clase, asignando mayor probabilidad de ser seleccionadas a las instancias de clases minoritarias. 

Para garantizar que la combinación de ambas estrategias no tuviera un efecto indeseado en el desempeño del modelo, se llevaron a cabo experimentos con el fin de validar la aplicación simultánea de ambas técnicas. Un efecto a evitar era, precisamente, que el modelo se volviera excesivamente sensible a las clases minoritarias, en detrimento de su capacidad para generalizar correctamente en clases mayoritarias. Los resultados de estos experimentos mostraron que, implementadas en conjunto, ambas estrategias lograban los mejores resultados.

## Resultados obtenidos en los experimentos

Los resultados obtenidos en nuestros experimentos revelaron que un Perceptron Multicapa (MLP) entrenado con datos sintéticos generados por un AV/AVC puede igualar o incluso superar en ciertos casos el rendimiento de un MLP entrenado con datos reales. 

Para dicha evaluacion, siguiendo a @fajardoOversamplingImbalancedData2021, *comparamos las métricas alcanzadas por un MLP entrenado con datos sintéticos y evaluado con datos reales de testeo, con las métricas obtenidas por un MLP entrenado con datos reales y también evaluado en los mismos datos de testeo*. En todos los casos se emplearon las particiones originales de los datos, a fin de propiciar la comparación de nuestros resultados con los obtenidos en la literatura. El indicador particular empleado para la comparación fue el de **exactitud**, que mide la proporción de predicciones correctas sobre el total de predicciones realizadas. 

Además, se emplearon otras métricas para completar nuestro análisis, a saber: la *precisión*, el *recall* y el *F1 Score*. Las métricas empleadas para dicha comparación fueron las siguientes: la *precisión* mide la proporción de predicciones positivas correctas sobre el total de predicciones positivas realizadas, es decir, qué tan preciso es el modelo al identificar casos positivos. El *recall* (o sensibilidad) indica la proporción de verdaderos positivos detectados sobre el total de positivos reales, reflejando la capacidad del modelo para capturar todos los casos positivos. El *F1 Score* es la media armónica de la precisión y el recall, proporcionando un equilibrio entre ambos y siendo especialmente útil cuando existe un desbalance en las clases. Otras métricas que utilizamos fueron el *Promedio Macro* y el *Promedio Ponderado*. Los indicadores de *Promedio Macro* calculan la media aritmética de las métricas individuales de cada clase sin considerar el número de muestras por clase, otorgando igual peso a todas las clases. Esto es útil para evaluar el rendimiento general en presencia de clases desbalanceadas. Por otro lado, el *Promedio Ponderado* tiene en cuenta el soporte (número de muestras) de cada clase al calcular la media, lo que significa que las clases con más muestras influyen más en el promedio general. Esto proporciona una visión más realista del rendimiento cuando hay clases dominantes.

Entonces en la siguiente figura se muestran los resultados obtenidos en los experimentos realizados para los cuatro conjuntos de datos estudiados, donde comparamos datos sintéticos con datos reales:  

![MLP con datos sintéticos vs MLP con datos reales](grafico_MLP-reales-vs-sintenticos.png)

Estos resultados son particularmente relevantes ya que sugieren que, bajo ciertas condiciones, los datos sintéticos pueden ser tan útiles como los datos reales para el entrenamiento de modelos predictivos. Como se aprecia en el gráfico, este fenómeno se observó de manera consistente en tres de los cuatro conjuntos de datos de nuestro estudio: Leukemia, Madelon y GCM, donde la precisión y la exactitud del modelo entrenado con datos sintéticos alcanzaron o superaron las métricas obtenidas con los datos originales. En el caso del dataset Gisette, el modelo entrenado con datos sintéticos estuvo muy cerca de igualar el rendimiento del modelo entrenado con datos reales.

Veamos a continuación los resultados particulares de cada conjunto de datos.

#### Leukemia

| Modelo                         | Clase | Precisión | Recall | F1 Score | Soporte |
|--------------------------------|-------|-----------|--------|----------|---------|
|   MLP con datos reales         | 0     | 0.76      | 1.00   | 0.87     | 13      |
|                                | 1     | 1.00      | 0.56   | 0.71     | 9       |
|   Exactitud                    |       |           |        | 0.81     | 22      |
|   Promedio Macro               |       | 0.88      | 0.78   | 0.79     | 22      |
|   Promedio Ponderado           |       | 0.86      | 0.82   | 0.80     | 22      |
|--------------------------------|-------|-----------|--------|----------|---------|
|   MLP con datos sintéticos     | 0     | 0.93      | 1.00   | 0.96     | 13      |
|                                | 1     | 1.00      | 0.89   | 0.94     | 9       |
|   Exactitud                    |       |           |        | **0.95** | 22      |
|   Promedio Macro               |       | 0.96      | 0.94   | 0.95     | 22      |
|   Promedio Ponderado           |       | 0.96      | 0.95   | 0.95     | 22      |


El cuadro anterior muestra el rendimiento de un MLP entrenado con datos reales frente a uno entrenado con datos sintéticos en el conjunto de datos Leukemia. Se observa que el modelo entrenado con datos sintéticos logra una mayor exactitud (**95%**) en comparación con el modelo entrenado con datos reales (**81%**). Las métricas de *precisión*, *recall* y *F1 Score* son superiores en casi todas las clases y promedios cuando se utiliza el modelo sobre datos sintéticos. 

Estos resultados indican que el modelo entrenado con datos sintéticos no solo es más preciso, sino que también tiene un rendimiento más equilibrado entre las clases, mejorando la capacidad de generalización y discriminación entre las categorías. La mejora en las métricas sugiere que el modelo es más eficaz en la identificación correcta de ambas clases, reduciendo tanto los falsos positivos como los falsos negativos. 

#### Madelon

| Modelo                       | Clase | Precisión | Recall | F1 Score | Soporte |
|------------------------------|-------|-----------|--------|----------|---------|
|   MLP con datos reales       | 0     | 0.56      | 0.55   | 0.55     | 396     |
|                              | 1     | 0.54      | 0.55   | 0.55     | 384     |
|   Exactitud                  |       |           |        | 0.55     | 780     |
|   Promedio Macro             |       | 0.55      | 0.55   | 0.55     | 780     |
|   Promedio Ponderado         |       | 0.55      | 0.55   | 0.55     | 780     |
|------------------------------|-------|-----------|--------|----------|---------|
|   MLP con datos sintéticos   | 0     | 0.56      | 0.72   | 0.63     | 396     |
|                              | 1     | 0.59      | 0.42   | 0.49     | 384     |
|   Exactitud                  |       |           |        | **0.57** | 780     |
|   Promedio Macro             |       | 0.58      | 0.57   | 0.56     | 780     |
|   Promedio Ponderado         |       | 0.58      | 0.57   | 0.56     | 780     |

El conjunto de datos   Madelon   presenta un caso donde el MLP entrenado con datos sintéticos logra una ligera mejora en la exactitud (**57%**) en comparación con el modelo entrenado con datos reales (**55%**). Los promedios macro y ponderado también muestran ligeras mejoras al utilizar datos sintéticos. 

#### Gisette

| Modelo                       | Clase | Precisión | Recall | F1 Score | Soporte |
|------------------------------|-------|-----------|--------|----------|---------|
|   MLP con datos reales       | 0     | 0.98      | 0.98   | 0.98     | 904     |
|                              | 1     | 0.98      | 0.98   | 0.98     | 896     |
|   Exactitud                  |       |           |        | **0.97** | 1800    |
|   Promedio Macro             |       | 0.98      | 0.98   | 0.98     | 1800    |
|   Promedio Ponderado         |       | 0.98      | 0.98   | 0.98     | 1800    |
|------------------------------|-------|-----------|--------|----------|---------|
|   MLP con datos sintéticos   | 0     | 0.95      | 0.97   | 0.96     | 904     |
|                              | 1     | 0.97      | 0.95   | 0.96     | 896     |
|   Exactitud                  |       |           |        | 0.95     | 1800    |
|   Promedio Macro             |       | 0.96      | 0.96   | 0.96     | 1800    |
|   Promedio Ponderado         |       | 0.96      | 0.96   | 0.96     | 1800    |

En el conjunto de datos   Gisette  , se observa que el modelo entrenado con datos reales supera al modelo entrenado con datos sintéticos. El MLP con datos reales alcanza una exactitud del **97%**, mientras que el modelo con datos sintéticos logra un **95%**. Sin perjuicio de ello, el modelo entrenado con datos sintéticos logró una buena performance en la mayoría de las métricas evaluadas. 

#### GCM

| Modelo                       | Clase | Precisión | Recall | F1 Score | Soporte |
|------------------------------|-------|-----------|--------|----------|---------|
|   MLP con datos reales       | 0     | 0.14      | 0.25   | 0.18     | 4       |
|                              | 1     | 0.00      | 0.00   | 0.00     | 1       |
|                              | 2     | 1.00      | 1.00   | 1.00     | 3       |
|                              | 3     | 1.00      | 0.33   | 0.50     | 6       |
|                              | 4     | 0.89      | 1.00   | 0.94     | 8       |
|                              | 5     | 0.40      | 0.67   | 0.50     | 3       |
|                              | 6     | 0.80      | 0.80   | 0.80     | 5       |
|                              | 7     | 0.50      | 0.75   | 0.60     | 4       |
|                              | 8     | 0.33      | 0.25   | 0.29     | 4       |
|                              | 9     | 0.25      | 0.67   | 0.36     | 3       |
|                              | 10    | 1.00      | 0.25   | 0.40     | 4       |
|                              | 11    | 1.00      | 0.67   | 0.80     | 3       |
|                              | 12    | 1.00      | 0.25   | 0.40     | 4       |
|                              | 13    | 1.00      | 0.20   | 0.33     | 5       |
|   Exactitud                  |       |           |        | **0.54** | 57      |
|   Promedio Macro             |       | 0.67      | 0.51   | 0.51     | 57      |
|   Promedio Ponderado         |       | 0.74      | 0.54   | 0.56     | 57      |
|------------------------------|-------|-----------|--------|----------|---------|
|   MLP con datos sintéticos   | 0     | 1.00      | 0.25   | 0.40     | 4       |
|                              | 1     | 0.00      | 0.00   | 0.00     | 1       |
|                              | 2     | 1.00      | 0.67   | 0.80     | 3       |
|                              | 3     | 1.00      | 0.17   | 0.29     | 6       |
|                              | 4     | 1.00      | 1.00   | 1.00     | 8       |
|                              | 5     | 0.43      | 1.00   | 0.60     | 3       |
|                              | 6     | 1.00      | 0.80   | 0.89     | 5       |
|                              | 7     | 0.10      | 0.25   | 0.14     | 4       |
|                              | 8     | 0.67      | 0.50   | 0.57     | 4       |
|                              | 9     | 0.50      | 0.33   | 0.40     | 3       |
|                              | 10    | 0.00      | 0.00   | 0.00     | 4       |
|                              | 11    | 1.00      | 0.67   | 0.80     | 3       |
|                              | 12    | 1.00      | 0.75   | 0.86     | 4       |
|                              | 13    | 0.21      | 0.60   | 0.32     | 5       |
|   Exactitud                  |       |           |        | **0.54** | 57      |
|   Promedio Macro             |       | 0.64      | 0.50   | 0.50     | 57      |
|   Promedio Ponderado         |       | 0.70      | 0.54   | 0.55     | 57      |

En el conjunto de datos GCM ambos modelos alcanzan la misma exactitud (**54%**). Sin embargo, el análisis detallado por clase revela diferencias en el rendimiento. El modelo entrenado con datos sintéticos muestra una mayor precisión en varias clases (0, 2, 3, 4, 6, 8, 11, 12), pero también presenta casos donde falla completamente (clases 1 y 10). Ambos modelos tienen dificultades con la clase 1, que tiene el menor soporte (1 muestra). El *Promedio Macro* y *Promedio Ponderado* son similares en ambos casos, lo que sugiere que, a pesar de las diferencias en el rendimiento por clase, los modelos son comparables en su capacidad general de clasificación. Estos resultados reflejan la complejidad inherente al trabajar con datos altamente desbalanceados y multiclase, donde la generación de datos sintéticos puede mejorar el rendimiento en algunas clases pero también puede introducir sesgos en otras.

## Otras consideraciones emergentes de los experimentos

Respecto de los hallazgos realizados a lo largo de todo el proceso de experimentación encontramos algunos aspectos dignos de mención.

Las pruebas con diferentes arquitecturas proporcionaron información valiosa. Como mencionamos antes, se exploraron modelos AV de tres y cuatro capas, así como AVC con múltiples capas, pero no se observaron mejoras significativas al aumentar la complejidad. En particular, se encontró que las configuraciones más simples (i.e. 3 capas), ofrecían resultados tan buenos o incluso mejores que sus contrapartes más complejas. Esta observación refuerza la idea de que, en algunos casos, la simplicidad puede ser preferible y que el sobredimensionamiento de la arquitectura no necesariamente se traduce en mejores resultados.

Un aspecto crítico de nuestros experimentos se relaciona con la dimensionalidad del espacio latente en los modelos generativos estudiados. Durante la exploración de hiperparámetros, se observó que la configuración óptima de la variable latente no se corresponde necesariamente con espacios de mayor dimensión. Contrario a la hipótesis generalmente aceptada —según la cual "cuanto mayor la dimensión del espacio latente, mejor" [@boomDynamicNarrowingVAE2021]— nuestros hallazgos indican que, en ciertos escenarios, una representación latente más compacta puede conducir a un rendimiento superior. En concreto, los experimentos con nuestro modelo AVC demostraron que una dimensión latente de 35 unidades superaba a configuraciones con más de 50, hallazgo que se alinea además con las prácticas reportadas en [@aiGenerativeOversamplingImbalanced2023]. Este resultado, aunque aparentemente contraintuitivo, evidencia que una dimensionalidad excesiva puede introducir ruido y redundancia, comprometiendo la capacidad de generalización del modelo.

Otra observación relevante asociada con la generación sintética de datos se vincula a la cantidad optima de muestras sintéticas. En relación a esto, se constataron resultados positivos en GCM incrementando las observaciones del conjunto de datos de entrenamiento de 190 a 3000 muestras balanceadas, con 214 observaciones por clase. Esta estrategia de aumentación, donde se igualan las cantidades de observaciones de todas las clases, es usada en @fajardoOversamplingImbalancedData2021 con buenos resultados. En nuestros experimentos el aumento resultó en una mejora significativa en la performance del modelo, logrando igualar los resultados obtenidos con el clasificador MLP entrenado con datos reales. Sin embargo, al continuar incrementando la cantidad de datos sintéticos a 6000 muestras, se observó una degradación en el rendimiento. 

![Exactitud según cantidad de muestras sintéticas](n_samples_accuracy_comparison.png)

Esto sugiere la existencia de un umbral en la cantidad de datos sintéticos que, una vez superado, introduce ruido en el modelo en lugar de aportar valor. Este ruido puede estar relacionado con el solapamiento de las fronteras de decisión en las muestras generadas, lo que aumenta el error y disminuye la precisión del modelo.

Finalmente, otros emergentes que podemos destacar son los siguientes:
- La implementación de un término de paciencia tuvo un impacto positivo en la calidad de los modelos, particularmente en el caso del AVC,
- Para el caso del AVC, el uso de L1_loss no proporciona un beneficio claro sobre el MSE para la tarea de generación de datos sintéticos, y
- El dropout no aporta beneficios en configuraciones ya optimizadas del AVC para este tipo de tareas. 

Estos resultados llevaron a una reflexión sobre la falta de impacto positivo de ciertos ajustes, como la introducción de L1_loss y dropout. Es probable que la estabilidad y el buen rendimiento de las configuraciones ya validadas de AV y AVC, alcanzados a través de numerosos experimentos, limiten el potencial de mejora adicional mediante estos métodos. 

## Conclusiones

En primer lugar pudimos determinar que los datos sintéticos generados por un AV/AVC pueden replicar la distribución de los datos reales en los conjuntos de datos estudiados, y por lo tanto, pueden ser tan útiles como aquellos para el entrenamiento de modelos predictivos. 

Por otro lado, la búsqueda de hiperparámetros y el ajuste de la arquitectura del AV y AVC revelaron la importancia de un enfoque balanceado que evite tanto la simplicidad excesiva como la complejidad innecesaria. Los resultados obtenidos muestran que, bajo ciertas configuraciones, los datos sintéticos pueden igualar o superar la utilidad de los datos reales en la formación de modelos predictivos, aunque la eficiencia y la calidad de estos resultados dependen en gran medida de la cuidadosa calibración de los hiperparámetros y de la adecuada elección de la arquitectura del modelo.

Asimismo, los experimentos parecen reflejar que los beneficios de la aumentación de datos tienen un límite. Superado este umbral, la generación adicional de datos no solo deja de ser útil, sino que puede ser perjudicial, como se evidenció en nuestros experimentos. Este fenómeno destaca la importancia de una cuidadosa calibración en la cantidad de datos sintéticos generados, especialmente en conjuntos de datos con características complejas y de grandes dimensiones. 


<!-- ## El uso de Autocodificadores Variacionales como técnica de aumentación


En @fajardoOversamplingImbalancedData2021 se investiga si los AV y las redes generativas antagónicas (GAN) pueden aumentar datos desbalanceados vía sobremuestreo de las clases minoritarias, y mejorar así el rendimiento de un clasificador. Para ello se crean versiones desbalanceadas de reconocidos datos multiclases tales como MNIST [@lecunGradientBasedLearningApplied1998] y Fashion MNIST [@xiaoFashionMNISTNovelImage2017], a los cuales, posteriormente, se los re-balancea agregándoles muestras sintéticas generadas por un AV condicionado por clase (AV Condicional). Para la tarea de clasificación se emplea un Perceptrón Multicapa (MLP), y se evalúa su desempeño promediando métricas de precisión, exhaustividad [controlar-concepto] y F1 score sobre distintos experimentos. La evaluación incluye la comparación de resultados del clasificador con datos aumentados por sobre-muestreo aleatorio, mediante SMOTE [@blagusSMOTEHighdimensionalClassimbalanced2013], GAN y AV. El resultado muestra a los AV -en su versión condicional- como el mejor modelo generativo para resolver el problema de datos desbalanceados mediante sobre-muestreo de las clases minoritarias.


@aiGenerativeOversamplingImbalanced2023 vuelve sobre los problemas planteados en @fajardoOversamplingImbalancedData2021, proponiendo una nueva metodología que superaría sus resultados. La propuesta en esta oportunidad plantea la aumentación de datos de la clase minoritaria condicionada a las características de la distribución que tienen los datos de la clase mayoritaria. El método se llama AV-Guiado-por-la-Mayoría (*Majority-Guided VAE* o MGVAE) y procura incorporar en la generación no solo información intra-clase sino también inter-clase, con el fin de propagar la diversidad y riqueza de la mayoría en la minoría, y mitigar así riesgos de sobre-ajuste en los modelos. Este modelo se pre-entrena utilizando muestras de la clase mayoritaria, y luego se ajusta con datos de la clase minoritaria para retener el aprendizaje de la etapa previa [@kirkpatrickOvercomingCatastrophicForgetting2017]. Para evaluar la eficacia de MGVAE, se realizaron experimentos en varios conjuntos de datos de imágenes y tabulares, utilizando diversas métricas de evaluación como Precisión Balanceada (B-ACC), Precisión Específica Promedio por Clase (ACSA) y Media Geométrica (GM). Los resultados muestran que MGVAE supera a otros métodos de sobre-muestreo en tareas de clasificación.


Un problema diferente es tratado en @khmaissiaConfidenceGuidedDataAugmentation2023 donde se emplean AV para aumentar datos en una tarea de clasificación con enfoque semi-supervisado. Aquí el desafío no pasa por el desbalance entre clases, sino en la búsqueda de mejorar el clasificador en regiones del espacio de características con bajo desempeño (ratios de error altos). Para eso, se mapea el espacio de características entrenando un modelo de WideResNet [@zagoruykoWideResidualNetworks2017] y luego se seleccionan las muestras mal clasificadas o con bajo nivel de confianza en la clasificación. Estas muestras se utilizan para entrenar un AV y generar datos sintéticos. Finalmente, las imágenes sintéticas se usan junto con las imágenes originales etiquetadas para entrenar un nuevo modelo de manera semi-supervisada. Se evalúan los resultados sobre STL10 y CIFAR-100 obteniendo mejoras en la clasificación de imágenes en comparación con los enfoques supervisados.


Finalmente, antecedente interesante es el presentado por @martinsVariationalAutoencodersEvolutionary2022b, pues pese a no estar directamente vinculado a la aumentación de datos, incluye la generación sintética de muestras mediante AV y la selección de características por AG. En efecto, el artículo propone la generación de individuos y optimización de características orientados al diseño de proteínas (específicamente variantes de Luciferasa bacteriana *luxA*). Partiendo de muestras de ADN de proteínas obtenidas de un subconjunto de datos de la base InterPro (identificados bajo el código "IPR011251") se generan conjuntos de individuos combinando datos originales, datos muestreados de la capa latente -*codificador*- del AV (configurado como MSA-AV para procesar sequencias alineadas de ADN) y datos optimizados por aplicación del AG. En el caso del algoritmo genético se emplean dos enfoques de optimización: de objetivo único y multiobjetivos, con funciones asociadas a la búsqueda de propiedades deseables en las muestras de ADN (solubilidad, síntesis, estabilidad y agregación de proteínas). El resultado de los experimentos realizados muestra que el diseño de proteínas guiado por la optimización mediante AG resultó en mejores soluciones que las obtenidas mediante muestreo directo, y que por su parte la optimización multiobjetivos permitió la selección de proteínas con el mejor conjunto de propiedades.


Los casos mencionados en el apartado nos ofrecen un conjunto de experiencias significativas a considerar al momento de resolver el problema planteado en este trabajo. Otras experiencias, como por ejemplo la configuración evolutiva de un AV [@wuEVAEEvolutionaryVariational2023], el ensamble de AV [@leelarathnaEnhancingRepresentationLearning2023], por mencionar algunas novedosas, escapan al recorte que hemos fijado. -->
