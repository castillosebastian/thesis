
<!-- 
Elementos del capítulo 1

1) Selección de características Fisher (Fisher, The Experimental Design, cap.VI, The factorial design in experimentation, 37 The Single factor): 
"In expositions of the scientific use of experimentation it is frequent to find an excessive stress laid on the importance of varying the essential conditions only one at a time. The experimenter interested in the causes which contribute to a certain effect is supposed, by a process of abstraction, to isolate these causes into a number of elementary ingredients, or factors, and it is often supposed, at least for purposes of exposition, that to establish controlled conditions in which all of these factors except one can be held constant, and then to study the effects of this single factor, is the essentially scientific approach to an experimental investigation. This ideal doctrine seems to be more nearly related to expositions of elementary physical theory than to laboratory practice in any branch of research. In experiments merely designed to illustrate or demonstrate simple laws, connecting cause and effect, the relationships ofi which with the laws relating to other causes are already known, it provides a means by which the student may apprehend the relationship, with which he is to familiarise himself, in as simple a manner as possible. By contrast, in the state of knowledge or ignorance in which genuine research, intended to advance knowledge, has to be carried on, this simple formula is not very helpful. We are usually ignorant which, out of innumerable possible factors, may prove ultimately to be the most important, though we may have strong presuppositions that some few of them are particularly worthy of study. We have usually no knowledge that any one factor will exert its effects independently of all others that can be varied, or that its effects are particularly simply related to variations in these other factors. On the contrary, if single factors are chosen for investigation, it is not because we anticipate that the laws of nature can be expressed with any particular simplicity in terms of these variables, but because they are variables which can be controlled or measured with comparative ease. If the investigator, in these circumstances, confines his attention to any single factor, we may infer either that he is the unfortunate victim of a doctrinaire theory as to how experimentation should proceed, or that the time, material, or equipment at his disposal are too limited to allow him to give attention to more than one narrow aspect of his problem.
The modifications possible to any complicated apparatus, machine, or industrial process must always be considered as potentially interacting with one another, and must be judged by the probable effects of such interactions. If they have to be tested one at a time this is not because to do so is an ideal scientific procedure, but because to test them simultaneously  would sometimes be too troublesome, or too costly."


2) Selección de características Vignolo-Gerard: 
In any case, the purpose is the improvement of a machine learning model, either in terms of learning speed, computational complexity, simplicity/interpretability of the representation or generalization capability. 
The existing FS techniques can be categorized according to three aspects [2]: the search strategy; the evaluation criterion; and the methodology for the assessment of the performance. The first corresponds to the search strategy which is applied to select candidate solutions among the possible feature combinations. The last two refer to the methods and measures used to evaluate each feature subset, and based on this the FS strategies can be divided in two categories: wrapper methods and filter methods. Wrappers have the advantage that the  predictive performance of the selected subset is correlated with the relevance measure. The problem for wrapper methods is that the predictive performance has to be assessed (a classifier has to be trained) for each feature subset tested, being therefore impossible to evaluate all the possible combinations of features. This issue can be overcome using search heuristics like genetic algorithms (GA), particle swarm (PSO) and ant colony optimization (ACO), which are able to find adequate solutions without exploring the entire search space [3]. However, while ACO and PSO typically use encoding based on real numbers, GAs use binary codification, reason why these are more suitable for feature selection problems.


3) Selección de características Verónica Bolón-Canedo, Noelia Sánchez-Maroño, Amparo Alonso-Betanzos (Feature Selection for High-Dimensional Data, Sringer 2015): 
"
0. The Need for Feature Selection  
In recent years, most enterprises and organizations have stored large amounts of data in a systematic way, without any clear potential use. In addition, with the rapid growth of the Internet, lots of data come in different formats—text, multimedia, etc.—from many different sources—systems, sensors, mobile devices, etc.—and they require new tools in order to be analyzed and processed so as to enable the extraction of useful information. Most of these data have been generated in the last few years while we continue to create a quintillion bytes daily [1]. Therefore, big data of large volume and ultrahigh dimensionality have emerged in various machine learning applications, such as text mining and information retrieval [2]. For instance, Weinberger et al. [3] have studied a collaborative email/spam filtering task with 16 trillion unique features. These datasets have brought an interesting challenge to the research community, and, citing [4], as researchers, “our task is to find a needle in a haystack, teasing the relevant information out of a vast pile of glut.” The ultrahigh dimensionality not only incurs unbearable memory requirements and high computational cost in training, but also deteriorates the generalization ability because of the “curse of dimensionality” issue. According to [4], the colorful phrase the “curse of dimensionality” was apparently coined by Richard Bellman in [5], in connection with the difficulty of optimization by exhaustive enumeration on product spaces. This phenomena arises when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings. Any dataset is usually represented by a matrix where the rows are the instances or samples that have been recorded and the columns are the attributes or features that are required to represent the problem at hand. Therefore, to tackle the “curse of dimensionality” problem, the dataset can be summarized by finding “narrower” matrices that in some sense are close to the original. These narrow matrices have only a small number of samples and/or a small number of attributes, and therefore can be used much more efficiently than the original large matrix can. The process of finding these narrow matrices is called dimensionality reduction.  Among the dimensionality reduction techniques, feature extraction addresses the problem of finding the most compact and informative set of features of a given problem, to improve the efficiency or data storage or processing. The problem of feature extraction is decomposed into two steps: feature construction and feature selection. Feature construction methods complement the human expertise to convert “raw” data into a set of useful features. It may be considered a preprocessing transformation that may include standardization, normalization, discretization, signal enhancement, extraction of local features, etc. Some construction methods do not alter the space dimensionality, while others enlarge it, reduce it or can act in either direction. But one should be aware of not losing information at the feature construction stage. 1.2 When Features Are Born 3  Guyon and Elisseeff [6] argued that it is always better to err on the side of being too inclusive rather than risking discarding useful information. Adding many features seems reasonable but it comes at a price: it increases the dimensionality of the patterns and thereby immerses the relevant information into a sea of possibly irrelevant, noisy or redundant features. Feature selection is the process in which the number of initial features is reduced and a subset of those that retain enough information for obtaining good, or even better, performance results is selected. Given the growing number of high-dimensional datasets, feature selection has gained increasing interest in the field of machine learning, addressing many of its disciplines, such as clustering [7, 8], regression [9, 10] and classification [11, 12], in both supervised and unsupervised ways. This book would be too long if all of them were to be addressed, which is why we have focused on the application of feature selection in supervised classification problems.

1. Intrinsic Characteristics of Data
1.1 Small Sample Size  The small sample size problem is when “only m samples are available in an ndimensional vector space with m < n,” citing Fukunaga [20]. This author says that this problem is often encountered in pattern recognition, particularly when n is very large. Nowadays, high dimension, low sample size (HDLSS) data are becoming increasingly common in various fields. These include the genetic microarrays previously presented, chemometrics, medical imaging, text recognition, face recognition, finance, and so on. Many of the features of these problems do not facilitate an adequate classification; rather they hinder it. To avoid this problem, the importance of feature selection has been stressed and several methods of feature selection for HDLSS data have been proposed in the literature..
1.2 Class Imbalance  Data are said to suffer the Class Imbalance Problem when the class distributions are highly imbalanced. This occurs when a dataset is dominated by a major class or by classes which have significantly more instances than the other rare or minority classes in the data. For the two-class case, without loss of generality, one assumes that the minority or rare class is the positive class, and the majority class is the negative class. Often the minority class is very infrequent, such as 1% of the dataset. If one applies the most traditional classifiers on the dataset, they are likely to predict everything as negative (the majority class) [25]. However, typically, people have more interest in learning about rare classes. For example, applications such as medical diagnosis prediction of rare but important disease, such as cancer, where it is common to have fewer cancer patients than healthy patients. Similar situations are observed in other areas, such as detecting fraud in banking operations, detecting network intrusions, managing risk, predicting technical equipment failure, e-mail foldering and face recognition [26].
1.3 Data Complexity  According to [35], difficulties in classification can be traced to three sources:  1. Class ambiguity refers to a situation where there are cases in a classification problem whose classes cannot be distinguished by any classification algorithm using the given features. Classes can be ambiguous for two reasons: a) the class concepts are poorly defined and, so, intrinsically inseparable, and b) the features chosen to represent the different samples are not sufficient for indicating differences between instances. An example of the former would be the lowercase letter “l” and the numeral “1”, which are the same in many fonts. 2. Boundary complexity: a class boundary is complex if it requires a long description, possibly including a listing of all the points together with their class labels. This aspect of difficulty comes from the nature of the problem, and is unrelated to the sampling process. It exists even if a complete sample is given and if the classes are well defined. 3. Sample sparsity: small sample size and dimensionality-induced sparsity introduce another layer of difficulty through a lack of constraints of the generalization rules.
1.4 Dataset Shift
These datasets can suffer what is known as dataset shift, which is defined as “a challenging situation where the joint distribution of inputs and outputs differs between the training and test stages”
1.5 Noisy Data  It is almost inevitable that there is some noise in most of the collected data, except in the most structured and synthetic environments. This “imperfect data” can be due to many sources, for instance, faulty measuring devices, transcription errors, and transmission irregularities. However, the accuracy of a classifier may greatly depend on the quality of the data used during the training phase, so a classifier built from a noisy training set might be less accurate and less compact than one built from the noise-free version of the same dataset using an identical algorithm [40]. Imperfections in a dataset can be dealt with in four broad ways: 1) leave the noise in, 2) data cleaning, i.e., filter the noise out, 3) data transformation, i.e, correct the noise, and 4) data reduction, that is, reduce the amount of data by aggregating values or removing and clustering redundant attributes [41]. In the first approach, the dataset is taken as is, with the noisy instances left in place. Algorithms that make use of the data are designed to be robust; that is, they can tolerate a certain amount of noise in the data. One approach is to develop robust algorithms that allow for noise by avoiding overfitting the model to the data, for example, using pruning techniques in decision trees [42]. In the second approach, instances that are suspected of being noisy according to certain evaluation criteria are discarded. A classifier is then built using only the retained instances in a smaller but cleaner dataset. [43] explores four techniques intended for data cleaning to enhance data analysis in the presence of high noise levels. However, there are two weaknesses in using this approach. First, by eliminating the whole instance, potentially useful information could also be discarded, such as the uncorrupted feature values. Secondly, when there is a large amount of noise in the dataset, the amount of information in the remaining clean dataset may not be sufficient for building a good classifier. These drawbacks lead us to the third approach that attempts to correct the tuples, so the noisy instances are identified, but instead of tossing these instances out, they are repaired by replacing the corrupted values with more appropriate ones. These corrected instances are then reintroduced into the dataset. Regarding the fourth approach, dimensionality reduction—and consequently feature selection—is one of the most popular techniques to remove noisy (i.e., irrelevant) and redundant features. However, while feature selection has been the target of much research, very little study has been done to systematically address the issue of feature relevance in the presence of noisy data.
1.6 Outliers  Hawkins defines an outlier as “an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism” [44]. Outlier detection algorithms are useful in areas such as machine learning, data mining, pattern recognition, data cleansing and data warehousing [45]
1.7 Feature Cost  Test costs and misclassification costs are the two most important types of cost in cost-sensitive learning. The test cost is money, time, or other resources we pay for collecting a data item of an object. The misclassification cost is the penalty we receive after deciding that an object belongs to an incorrect class [47]. Feature selection works have mainly considered misclassification costs. However, most feature selection methods proposed in the literature do not cope with the test cost, although acquiring features in real-world applications may vary notably from one feature to another. For example, in medical applications, the feature acquisition can demand a clinical test that, in addition to its economics cost, poses a risk to the patient, or demand demographic data obtained from giving him a short questionnaire. In image texture analysis, the feature extraction process represents a significant computational cost that also depends on the method used [48].

1. Feature Selection  
Feature selection can be defined as the process of detecting the relevant features and discarding the irrelevant and redundant ones with the goal of obtaining a subset of features that describes properly the given problem with a minimum degradation of performance. It has several advantages [3], such as:  • Improving the performance of the machine learning algorithms. • Data understanding, gaining knowledge about the process and perhaps helping to visualize it. • General data reduction, limiting storage requirements and perhaps helping in reducing costs. • Feature set reduction, saving resources in the next round of data collection or during utilization. • Simplicity, possibility of using simpler models and gaining speed.  To all this, we will add that in the present scenario of Big Data analytics, feature selection plays a central role.
Feature Relevance  Intuitively, it can be determined that a feature is relevant if it contains some information about the target.

1. Feature Selection Methods  Feature selection methods can be divided according to two approaches: individual evaluation and subset evaluation [5]. Individual evaluation is also known as feature ranking and assesses individual features by assigning them weights according to their degrees of relevance. On the other hand, subset evaluation produces candidate feature subsets based on a certain search strategy. Each candidate subset is evaluated by a certain evaluation measure and compared with the previous best one with respect to this measure. While the individual evaluation is incapable of removing redundant features because redundant features are likely to have similar rankings, the subset evaluation approach can handle feature redundancy with feature relevance. 16 2 Foundations of Feature Selection  However, methods in this framework can suffer from an inevitable problem caused by searching through all feature subsets required in the subset generation step, and thus, both approaches are worth studying. Aside from this classification, three major approaches can be distinguished based upon the relationship between a feature selection algorithm and the inductive learning method used to infer a model [3]:  • Filters, which rely on the general characteristics of training data and carry out the feature selection process as a preprocessing step with independence of the induction algorithm. This model is advantageous for its low computational cost and good generalization ability. • Wrappers, which involve a learning algorithm as a black box and consist of using its prediction performance to assess the relative usefulness of subsets of variables. In other words, the feature selection algorithm uses the learning method as a subroutine with the computational burden that comes from calling the learning algorithm to evaluate each subset of features. However, this interaction with the classifier tends to give better performance results than filters. • Embedded methods, which perform feature selection in the process of training and are usually specific to given learning machines. Therefore, the search for an optimal subset of features is built into the classifier construction and can be seen as a search in the combined space of feature subsets and hypotheses. This approach is able to capture dependencies at a lower computational cost than wrappers.

3.1 Filter Methods  Filter methods are based on performance evaluation metrics calculated directly from the data, without direct feedback from predictors that will finally be used on data with a reduced number of features [3]. As mentioned above, these algorithms are usually computationally less expensive than wrappers or embedded methods. In this subsection, the most popular filters are described, which will be used throughout this book.
TableSummary of filters  Uni/Multivariate Ranker/Subset  Chi-Squared Univariate Ranker Information Gain Univariate Ranker ReliefF Multivariate Ranker mRMR Multivariate Ranker Md Multivariate Ranker CFS Multivariate Subset FCBF Multivariate Subset INTERACT Multivariate Subset Consistency Multivariate Subset
3.2 Embedded Methods  In contrast to filter and wrapper approaches, embedded methods do not separate the learning from the feature selection part. Embedded methods include algorithms, which optimize a regularized risk function with respect to two sets of parameters: 2.2 Feature Selection Methods 25  the parameters of the learning machine and the parameters indicating which features are selected. The embedded methods used are subsequently described.
3.3 Wrapper Methods  The idea of the wrapper approach is to select a feature subset using a learning algorithm as part of the evaluation function. Instead of using subset sufficiency, entropy or another explicitly defined evaluation function, a kind of “black box” function is used to guide the search. The evaluation function for each candidate feature subset returns an estimate of the quality of the model that is induced by the learning algorithm. This can be rather time consuming, since for each candidate feature subset evaluated during the search, the target learning algorithm is usually applied several times (e.g., in the case of ten-fold cross-validation used to estimate model quality). In this book the wrapper described as follows will be used; which can be combined with any learning algorithm.


Metáforas:
- la aguja en el pajar
- El problema siempre será pertinente pues la información disponible siempre será mayor que la información relevante.
 -->


# El problema de la selección de características

En el presente capítulo abordamos el problema de la selección de características, su importancia y desafíos. En ese marco, repasaremos las dificultades asociadas a los datos y cómo estas impactan en la selección de características. Luego describiremos brevemente distintos enfoques para la selección de características, como así también ventajas y desventajas de cada uno. Finalmente, vincularemos la selección de características con el aporte fundamental de nuestro trabajo asociado a la generación sintética de datos. 

## Estrés, ignorancia y selección de características

Un punto de partida difícil de controvertir en el mundo actual del aprendizaje automático es que la cantidad de información disponible para investigar ha crecido dramáticamente en los últimos veinte años. Conforme la vida se digitaliza, y la información almacenada en los sistemas aumenta, la generación de conocimiento parece menos determinada por el viejo problema de la escasez de datos y más por su nueva situación de abundancia. Esta *explosión* [@liWhenChatGPTComputer2023] hace más pertinente hoy las observaciones de Fischer de hace 90 años, cuando reprochaba el *estrés* con el que se investigaba manipulando un grupo de variables aisladas en vez de indagar las interacciones que se dan en los sistemas complejos. En cualquier caso, resignifica su aguda intuición acerca de la *investigación experimental* en el sentido que *al mismo tiempo que se la hace completa y abarcativa, se la hace también más eficiente, entendiendo por tal aquella que provee más conocimiento y un mayor grado de precisión con la misma cantidad de observaciones*.[@fisherDesignExperiments1935, p.98]

Así las cosas, desde aquellas ideas de Fisher hasta nuestros días, el problema de la investigación *completa, abarcativa y eficiente*, ha sido un tema de indagación permanente en distintas ramas de la ciencia. En el dominio del aprendizaje automático, dicho tema ha motivado la aparición de una serie de técnicas dirigidas a generar conocimiento y resultados precisos en escenarios de alta complejidad por la abundancia de información. Así, bajo el nombre de *selección de características* nació un area de estudio que busca resolver precisamente el problema de discernir *sistemáticamente* la información relevante de aquella que no lo es cuando se trabaja con muchas variables.[^bolon-canedo_origenSC]

[^bolon-canedo_origenSC] Según Bolón-Canedo y ot., el origen de este campo se remonta a los años '60, cuando Hughes, usando un modelo paramétrico, estudió la precisión de un clasificador bayesiano en función del número de características. En dicho estudio se concluye -señalan las autoras- que: “la selección, reducción y combinación de mediciones no se proponen como técnicas ya desarrolladas. Más bien, son ilustrativas de un marco para investigaciones futuras”.[@bolon-canedoFeatureSelectionHighDimensional2015].

Nótese aquí que el acento en lo *sistemático* es definitorio en la medida que gran parte del esfuerzo en la selección de características se centra en disponer de un método para identificar de manera precisa la información relevante cuando se desconoce las interacciones entre factores y su impacto en el problema. Es decir, asumimos que cierta información en un campo problemático puede ser relevante para resolverlo, pero ignoramos qué factores y con qué alcance contribuyen a esa resolución. Este desconocimiento no implica la ausencia de toda comprensión acerca del fenómeno en estudio y sus posibles dimensiones, sino más bien resalta una premisa del método experimental donde -según Fisher- *usualmente estamos en estado de ignorancia acerca de cuál, entre innumerables factores, podría resultar finalmente el más importante* [@fisherDesignExperiments1935, p.97]. La sofisticación de las herramientas actuales de investigación ha permitido ampliar los objetos de estudio de una manera impensable en la época de nuestro autor, contribuyendo a superar anteriores restricciones. Hoy no es extraño concebir objetos de investigación donde el conjunto de dimensiones a estudiar supera por decenas de miles los objetos de la época de Fisher. En tales circunstancias, la sistematicidad se vuelve una rasgo esencial del proceso de selección de características permitiendo el análisis controlado y metódico de tales objetos.

Siguiendo a Bolón-Canedo y ot. [@bolon-canedoFeatureSelectionHighDimensional2015], podemos definir a la *selección de características* como *el proceso de detectar las características relevantes y descartar aquellas irrelevantes o redundantes, con el objetivo de obtener un subconjunto que describa adecuadamente un problema dado con una degradación mínima del rendimiento*. Aquí rendimiento se refiere a la medida de evaluación empleada para medir los resultados del proceso de selección. Los distintos enfoques para selección de características adhieren a la premisa de que podemos separar la información relevante de aquella que no lo es para predecir una variable objetivo. Bajo esa perspectiva, la selección de características busca determinar un subconjunto de atributos que satisfaga uno de los siguientes criterios[@VignoloGerardFeatureSelection2012]: 

1. El subconjunto con un tamaño especificado que maximice la precisión de la predicción.
2. El subconjunto de menor tamaño que satisfaga un requisito de precisión mínima.
1. El subconjunto que logre el mejor compromiso entre dimensionalidad y precisión.

El criterio a elegir dependerá de los objetivos del estudio y de las características del problema. Mas allá de eso, es fácil advertir que la selección de características ofrece la ventaja de reducir el espacio de búsqueda, superando así diversos problemas asociados con la alta dimensionalidad. En el contexto del aprendizaje automático contemporáneo es común enfrentar problemas representados por grandes conjuntos de variables, vicisitud que se asocia con la *maldición de la dimensionalidad*.[@bolon-canedoFeatureSelectionHighDimensional2015] En general, este fenómeno se presenta por la alta complejidad computacional y costos asociados con la optimización de dichos espacios. Para abordarlo, una alternativa posible es la reducción de dimensionalidad, que consiste en encontrar o construir matrices con menor número de columnas que las originales pero que aproximan el total de información relevante que éstas proveen. Esas matrices, con una fracción de los atributos originales, puede usarse de manera más eficiente para modelar el problema, sin mencionar que además facilitan su interpretación y comunicación. El proceso de encontrar estas matrices se llama *reducción de dimensionalidad*, y supone seleccionar un subconjunto de características. 

La abundancia de información a la que venimos refiriéndonos trae consigo una serie de desafíos que impactan directamente en la selección de características. En el siguiente apartado describiremos, siguiendo a Bolón-Canedo y ot., los principales problemas que enfrentamos al trabajar con conjuntos de datos.

## Las multiples cara de los datos problemáticos

En el contexto actual del aprendizaje automático, los datos presentan una serie de desafíos que impactan directamente en la selección de características. Entre los principales problemas que enfrentamos al trabajar con conjuntos de datos, podemos destacar:

### Alta dimensionalidad y escasez muestral

Un problema frecuente se presenta cuando disponemos de pocas muestras en un espacio vectorial de alta dimensionalidad, es decir, cuando el número de muestras ($m$) es menor que el número de características ($n$), siendo $n$ particularmente grande. Esta situación, que combina alta dimensionalidad y escasez muestral, es común en campos como el análisis genético, reconocimiento facial y procesamiento de información médica. La escasez de muestras dificulta el tratamiento de la información, ya que la cantidad de datos disponibles para entrenar un modelo es insuficiente para capturar la variabilidad inherente a los datos. En tales circunstancias, el proceso de aprendizaje tiende a sobreajustarse a las observaciones disponibles, lo que puede llevar a modelos poco generalizables. 

Como veremos mas adelante, tres de los datasets elegidos para nuestro estudio -vinculados al ámbito biomédico- se encuentran en esta situación. En tales circunstancias, la selección de características tiene severas limitaciones, ya que la alta dimensionalidad de los datos dificulta la distinción entre información relevante y irrelevante. Es aquí donde la generación sintética de datos puede ofrecer una solución, ya que al permitir aumentar la cantidad de datos disponibles para el aprendizaje, podría mitigar el problema de la escasez muestral.

### Desbalance de clases

El desbalance de clases se presenta cuando la distribución de las clases en el conjunto de datos es altamente desigual. Típicamente, una clase mayoritaria tiene más observaciones que una o más clases minoritarias. Este problema puede presentarse en combinación con la alta dimensionalidad y escacez muestral, agravando la dificultad de selección de características.

El desbalance de clases es un problema relevante en aplicaciones donde la o las clases minoritarias son precisamente las de mayor interés, como por ejemplo: el diagnóstico de enfermedades raras, la detección de fraudes bancarios, la identificación de intrusiones en redes y la predicción de fallos en equipos técnicos. En supuestos como estos, los clasificadores que no contemples un tratamiento explícito del problema tienden a sesgarse hacia la clase mayoritaria, pudiendo alcanzar una alta precisión global mientras fallan en la detección de los casos más importantes pero menos frecuentes.

Para atacar este problema, se han propuesto diversas soluciones, que pueden categorizarse en tres grupos:

1. **Muestreo de datos**: Este enfoque modifica las muestras de entrenamiento para producir una distribución de clases más balanceada. Las técnicas tradicionales incluyen:
   - Submuestreo (*undersampling*): Crea un subconjunto del conjunto original eliminando instancias.
   - Sobremuestreo (*oversampling*): Genera un superconjunto replicando instancias existentes o creando nuevas. 
   - Métodos híbridos: Combinan ambas técnicas de muestreo.

2. **Modificación algorítmica**: Este enfoque adapta los métodos de aprendizaje para que sean más sensibles a los problemas de desbalance. Por ejemplo, el uso de la distancia de Hellinger como criterio de división en árboles de decisión, que ha demostrado ser insensible al sesgo, capturando la divergencia en las distribuciones sin ser dominada por las probabilidades previas de clase.

1. **Aprendizaje sensible al costo**: Esta solución puede incorporar elementos a nivel de datos, a nivel de algoritmos, o ambos a la vez, asignando costos más altos a la clasificación errónea de ejemplos de la clase positiva (minoritaria). Como mencionamos antes, en diversos escenarios de datos desbalanceados el costo de falsos positivos es mayor que el de pasos negativos. Por ejemplo, en el diagnóstico médico, resulta más importante reconocer la presencia de una enfermedad rara que su ausencia, por ello el costo de un falso negativo es mayor que el de un falso positivo.

Ante el problema del desbalance de clases, la generación sintética de datos puede ofrecer una solución alternativa a los métodos tradicionales, ya se por la generación de muestras sintéticas en proporciones que permitan re-balancear las clases, o por la generación un cantidad de muestras tal que permita que el modelo aprenda a discriminar entre las clases pese a su desigual distribución.

### Complejidad 

La complejidad de los datos puede manifestarse en tres aspectos fundamentales, y todos ellos pueden coexistir en un mismo problema:

1. **Ambigüedad de clases**: Surge cuando los casos no pueden distinguirse utilizando las características disponibles, ya sea porque los conceptos de clase están mal definidos y por lo tanto son intrínsecamente indistinguibles, o porque las características seleccionadas son insuficientes para discriminar las clases.

2. **Complejidad de fronteras**: Se refiere a situaciones donde los límites entre clases requieren una descripción extensa de la frontera de decisión, llegando a demandar, en el caso extremos de complejidad, la enumeración exhaustiva de todos los puntos con sus etiquetas de clase. Este aspecto de dificultad se debe a la naturaleza del problema y no a la muestra o selección de características, indicando que una completa separación entre clases es un problema difícil de resolver.

3. **Dispersión de muestras**: La combinación de pocas muestras y alta dimensionalidad genera una dispersión que dificulta la generación de fronteras de decisión. 

Los casos 1 y 2 son problemas ante los cuales la selección de características no puede ofrecer una solución efectiva. El caso 3, asociado con la alta dimensionalidad, podría ser mitigado por la generación de muestras sintéticas debido a que éstas permitirían aumentar la densidad de puntos en el espacio de características, facilitando así la identificación de patrones y la construcción de fronteras de decisión más robustas.

### Ruido 

Los datos del mundo real siempre están expuestos a imperfecciones, ruido, proveniente de entornos dinámicos donde las dimensiones de interés de un fenómeno coexisten en espacios de interacciones permanentes. Así, aún considerando un escenario artificial, completamente libre de interferencia, la inperfección puede provenir de diversas fuentes como dispositivos de medición defectuosos o limitados, errores de transcripción o irregularidades en la transmisión de información.

Ante tales circunstancias, existen cuatro enfoques principales para abordar estas imperfecciones en los conjuntos de datos:

1. **Conservación del ruido**: En este enfoque, se mantiene el conjunto de datos tal como está, con sus instancias ruidosas. Los algoritmos que utilizan los datos se diseñan para ser robustos, es decir, capaces de tolerar cierta cantidad de ruido. Una estrategia común es desarrollar algoritmos que eviten el sobreajuste al modelo (como sucede en el caso de los árboles de decisión) mediante técnicas de poda.

2. **Limpieza de datos**: Este método implica descartar las instancias que se consideran ruidosas según ciertos criterios de evaluación. El clasificador se construye utilizando únicamente las instancias retenidas en un conjunto de datos más pequeño pero más limpio. Sin embargo, este enfoque presenta dos debilidades significativas:
   - Al eliminar instancias completas, se puede descartar información potencialmente útil, como valores de características no corrompidos.
   - Cuando existe una gran cantidad de ruido, la información restante en el conjunto de datos limpio puede resultar insuficiente para construir un clasificador eficaz.

3. **Transformación de datos**: Este enfoque busca corregir las instancias ruidosas en lugar de eliminarlas. Las instancias identificadas como ruidosas se reparan reemplazando los valores corrompidos por otros más apropiados, y luego se reintroducen en el conjunto de datos.

4. **Reducción de datos**: Esta estrategia implica reducir la cantidad de datos mediante la agregación de valores o la eliminación y agrupación de atributos redundantes. La reducción de dimensionalidad -y consecuentemente la selección de características- es una de las técnicas más populares para eliminar características ruidosas (es decir, irrelevantes) y redundantes.

Ante esta vicisitud, la generación sintética de datos -como veremos en el capítulo 3- puede ofrecer un herramienta de mitigación del ruido, debido a que el proceso de codificación-decodificiación que lo hace posible implica la reconstrucción de los rasgos relevantes de los datos reales, y consecuentemente una reducción del ruido que contienen. 

### Métodos de selección de características

Los métodos de selección de características pueden dividirse según dos enfoques: evaluación individual y evaluación de subconjuntos. La evaluación individual, también conocida como ranking de características, asigna pesos a las características individuales según su grado de relevancia. Por otro lado, la evaluación de subconjuntos produce candidatos basados en una estrategia de búsqueda determinada. Cada subconjunto candidato se evalúa mediante una medida específica y se compara con el mejor anterior respecto a esta medida. 

Mientras que la evaluación individual es incapaz de eliminar características redundantes (porque estas tienden a tener rankings similares), el enfoque de evaluación de subconjuntos puede manejar tanto la redundancia como la relevancia de las características. Sin embargo, los métodos en este marco pueden sufrir un problema inevitable causado por la necesidad de buscar entre todos los subconjuntos posibles, razón por la cual ambos enfoques merecen ser estudiados.

Más allá de esta clasificación, se pueden distinguir tres enfoques principales basados en la relación entre el algoritmo de selección de características y el método de aprendizaje inductivo usado para inferir un modelo:

1. **Filtros**: Se basan en las características generales de los datos de entrenamiento y realizan la selección de características como un paso de preprocesamiento, independiente del algoritmo de inducción. Este modelo es ventajoso por su bajo costo computacional y buena capacidad de generalización.

2. **Wrappers**: Involucran un algoritmo de aprendizaje como caja negra y consisten en usar su rendimiento predictivo para evaluar la utilidad relativa de subconjuntos de variables. En otras palabras, el algoritmo de selección de características utiliza el método de aprendizaje como una subrutina, con la carga computacional que implica llamar al algoritmo de aprendizaje para evaluar cada subconjunto de características. Sin embargo, esta interacción con el clasificador tiende a dar mejores resultados de rendimiento que los filtros.

3. **Métodos embebidos**: Realizan la selección de características durante el proceso de entrenamiento y suelen ser específicos para determinadas máquinas de aprendizaje. Por lo tanto, la búsqueda de un subconjunto óptimo de características está integrada en la construcción del clasificador y puede verse como una búsqueda en el espacio combinado de subconjuntos de características e hipótesis. Este enfoque es capaz de capturar dependencias a un costo computacional menor que los wrappers.

