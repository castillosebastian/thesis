Voy a presentarte la cr√≥nica de las tareas realizadas en el contexto de mi trabajo de Tesis de Maestr√≠a. Dicho trabajo ten√≠a las siguientes caracter√≠sticas: "# Introducci√≥n {#sec-Capitulo1}

En el campo del aprendizaje autom√°tico y la miner√≠a de datos, la selecci√≥n de caracter√≠sticas es una tarea cr√≠tica que puede determinar el √©xito de un modelo predictivo. La alta dimensionalidad y la complejidad inherente a muchos conjuntos de datos hacen que la selecci√≥n de un subconjunto √≥ptimo de caracter√≠sticas sea un desaf√≠o significativo. Este desaf√≠o se ve agravado cuando se dispone de un n√∫mero limitado de muestras, lo que puede afectar negativamente la capacidad de un algoritmo para generalizar y ofrecer resultados robustos. 

Los algoritmos gen√©ticos (AG) se han consolidado como una herramienta poderosa para abordar problemas de optimizaci√≥n complejos, incluyendo la selecci√≥n de caracter√≠sticas. Estos algoritmos, inspirados en la evoluci√≥n natural, son capaces de explorar grandes espacios de b√∫squeda de manera efectiva, proporcionando soluciones cercanas al √≥ptimo en una variedad de contextos. Sin embargo, la eficacia de los AGs depende en gran medida de la calidad y cantidad de datos disponibles para evaluar las posibles soluciones, lo que subraya la importancia de contar con un conjunto de datos adecuado.

En este contexto, surge la necesidad de investigar estrategias que puedan mitigar las limitaciones impuestas por la disponibilidad limitada de datos. Una de las t√©cnicas emergentes en este √°mbito es la aumentaci√≥n de datos mediante autoencoders variacionales (AVs). Los AVs, como modelos generativos, tienen la capacidad de crear muestras sint√©ticas que mantienen las propiedades fundamentales de los datos originales, lo que los convierte en una herramienta prometedora para mejorar la capacidad de los AGs en la selecci√≥n de caracter√≠sticas.

## 1.2 Planteamiento del Problema

El problema central de esta investigaci√≥n radica en la limitaci√≥n que impone la disponibilidad de datos en el proceso de selecci√≥n de caracter√≠sticas utilizando AGs. Esta limitaci√≥n es especialmente cr√≠tica en escenarios de alta dimensionalidad y bajo n√∫mero de muestras, donde la funci√≥n objetivo, que gu√≠a la b√∫squeda de soluciones √≥ptimas, puede degradarse significativamente. La pregunta que gu√≠a este trabajo es: ¬øc√≥mo puede la aumentaci√≥n de datos mediante autoencoders variacionales mejorar el desempe√±o de los algoritmos gen√©ticos en la selecci√≥n de caracter√≠sticas?

## 1.3 Importancia del Problema

La selecci√≥n de caracter√≠sticas no solo impacta en la precisi√≥n de los modelos predictivos, sino que tambi√©n afecta la eficiencia computacional y la interpretabilidad de los resultados. En problemas de alta dimensionalidad, la capacidad de reducir el n√∫mero de caracter√≠sticas relevantes sin perder informaci√≥n crucial es de vital importancia. Mejorar este proceso mediante la integraci√≥n de t√©cnicas de aumentaci√≥n de datos podr√≠a tener un impacto significativo en diversas aplicaciones pr√°cticas, desde la biolog√≠a molecular hasta la ingenier√≠a y las ciencias sociales.

## Objetivos

**General**:

Evaluar el impacto de la aumentaci√≥n de datos mediante autoencoders variacionales sobre el desempe√±o de estrategias de selecci√≥n de caracter√≠sticas basados en algoritmos gen√©ticos.

2.  Integrar estrategias de aumentaci√≥n de datos basadas en AVs en los AGs.

3.  Evaluar el desempe√±o de las estrategias propuestas empleando conjuntos de datos de diferente complejidad.

4.  Comparar el desempe√±o de AGs con datos aumentados mediante las estrategias desarrolladas frente a implementaciones sin aumentaci√≥n.". La cr√≥nica de actividades para cumplir estos objectivos debes extraerla de las siguiente conversaci√≥n que mantuve con mis Directores de tesis, que te detallo a continuaci√≥n: ---------------------------------------------- > """ 

Comunicaciones con mis Directores de Tesis

-------
Avances
Inbox

Claudio Sebasti√°n Castillo <castilloclaudiosebastian@gmail.com>
Attachments
Mon, Nov 27, 2023, 9:17‚ÄØAM
to Matias, Leandro

Buen d√≠a Mat√≠as y Leandro, c√≥mo est√°n?
Estoy empezando el trabajo de investigaci√≥n de mi tesis. Estoy en la etapa de evaluar c√≥mo se desempe√±an modelos tradicionales de ML en los 4 dataset analizados, teniendo en cuenta AUC y F1, y tiempo de entrenamiento. Les adjunto un adelanto de las evaluaciones. Destaco que trabaj√© con 18 modelos, salvo para el caso de GCM que trabaj√© con 10, debido a su dimensionalidad). Hice un ajuste fino basado de GridSearch de los mejores hiperpar√°metros, como para disponer de m√©tricas de comparaci√≥n realistas. Trabaj√© en la nube con una VM de 24 vCPU y 30 RAM. Todo funcion√≥ bastante bien. Ahora estoy terminando de correr un √∫ltimo experimento sobre GCM. Tengo todos los experimentos documentos en mi repo (https://github.com/castillosebastian/ealab/tree/main/exp). Quer√≠a llegar a tener una reuni√≥n con ustedes y tener en la mano algo armado. De todas formas podemos repetir los experimentos las veces que ustedes digan.   
Si les parece, podemos armar una reuni√≥n para ver estas cosas y los pr√≥ximos pasos. Tengo muchas ganas de meterme con AV y su integraci√≥n a AG, pero asumo que esta primera parte tiene que estar solida y bien armada para que luego sean v√°lidas y ricas las comparaciones. 
Entonces, espero que me digan cu√°ndo podr√≠an juntarse unos minutos. Tendr√≠a que ser despues de las 18.30, hora que salgo de trabajo. Les mando un saludo cordial
PD. tengo una gran noticia para cuando nos juntemos :)
------
Avances
Inbox

Claudio Sebasti√°n Castillo <castilloclaudiosebastian@gmail.com>
Mon, Feb 5, 7:31‚ÄØPM
to Matias, Leandro

Mat√≠as y Leandro, buen d√≠a, espero que hayan pasado muy bien y descansado. 
Bueno, conforme lo conversado, les comento que estuve trabajando en la primera versi√≥n de nuestro VAE que pueden consultar aqu√≠.  Y tiene varios experimentos vinculados. 
Digo una primera versi√≥n, porque aunque estoy generando datos sint√©ticos y ensayando algunas pruebas de aumentaci√≥n (con GCM y MADELON) no estoy teniendo mejoras en los resultados de clasificaci√≥n utilizando un MLP. 
Estuve yendo y viniendo con los scripts varias veces, y estoy convencido que el VAE que estoy trabajando no alcanza para codificar y reproducir las distribuciones subyacentes a los datos de entrenamiento. Pude confirmar esto luego de que intentara hacer una clasificaci√≥n enteramente sobre datos sint√©ticos y obtuviera un resultado casi nulo (que se puede consultar aqu√≠, l√≠nea 158 y siguientes). Ese simple experimento me demuestra que el problema est√° en la arquitectura del VAE. 
Puesto a ver esta arquitectura con m√°s detalle advierto que durante el entrenamiento el VAE no est√° mejorando sus m√©tricas de evaluaci√≥n. Por lo que no esta ser√≠a la se√±al confirmatoria que tengo que ajustar el VAE. 
En este momento estoy viendo justamente eso. 
Ten√≠a pensado poder dedicarle unos d√≠as a este problema para llegar a la primera reuni√≥n con ustedes con las cosas pendientes terminadas:

- eleccion de modelos     
- corrida base con busqueda de hyperpar√°metros () y divisi√≥n original!    
- implementar aumentaci√≥n de datos x AVs    
- experimentar dataset aumentados con modelos seleccionados

Al mismo tiempo, estoy documentando todo esto como para ir perfilando los puntos iniciales de la tesis. 
Bueno, los mantengo al tanto y les aviso cuando est√© listo en los pr√≥ximos d√≠as. 
Cordialmente
Claudio
--------
RESPUESTA DE MI DIRECTOR MATIAS GERARD
Buenas tardes Claudio!!

Si si, la verdad que fue un muy merecido descanso (aunque creo que me merezco un poco m√°s... jajajajaja)
Qu√© bueno que hayas podido avanzar!!
Disculp√° la demora, pero quer√≠a tomarme el tiempo de mirar bien lo que mandaste antes de responderte.
Antes que desesperes üòÖ, te comento que estuve mirando el c√≥digo y pareciera que est√° todo ok.
Las dificultades que est√°s teniendo son naturales, y tienen que ver con el modelado del problema (por varios factores):
Puede ser que la arquitectura usada sea peque√±a y no alcance a capturar la estructura de los datos. Incluso, que las funciones de transferencia no sean las m√°s adecuadas. Hay que seguir trabajando en eso ü§∑
Puede ser que el uso de la Binary Cross Entropy no sea la mejor opci√≥n, ya que esta medida determina cu√°n lejos est√°s del 0/1, y no el error cometido respecto de un valor de referencia real (las features de tu patr√≥n, el cual quer√©s aprender). En el caso de im√°genes, puede ser entendible que quieras hacer que un pixel valga 0 o 1 (por ejemplo, cuando se aprende una m√°scara), pero en este caso estamos queriendo predecir los valores reales de las features de un patr√≥n (que particularmente est√°s normalizando en el rango 0-1). Posiblemente convenga cambiar el c√°lculo del error para considerar el error absoluto o cuadr√°tico en lugar de la BCE.
Puede ser que la normalizaci√≥n de los datos te est√© distorsionando la forma en que ve la distribuci√≥n. Pens√° que est√°s transformando cada "batch" de manera independiente, ya que no se aprenden los par√°metros para ser aplicados al siguiente set de datos. No estoy seguro del efecto de esta parte, pero seguramente en algo debe estar influyendo.
Fundamentalmente, el principal problema que vas a encontrar es que est√°s modelando las distribuciones (cada clase) de forma independiente. VAE trabaja aprendiendo la distribuci√≥n de todos los datos (una suerte de suma de gausianas) y luego permite generar nuevos datos a partir de la distribuci√≥n aprendida. Suponete que ten√©s 2 clases balanceadas y no solapadas (bien separadas). En ese caso, uno esperar√≠a poder aprender las clases de forma independiente, ya que los datos no se pisan, y deber√≠a ser posible generar datos de cada clase sin problemas. Sin embargo, cuando las distribuciones se solapan, no es claro donde termina una y empieza la otra. Esto hace que al aprender por separado las distribuciones, se desconozcan los l√≠mites de cada una. Sin embargo, si se aprenden de forma simult√°nea, pod√©s modelar mejor la distribuci√≥n de los datos, aunque a costa de perder la capacidad de orientar la s√≠ntesis de datos a la clase que te interesa. Por eso es que pensamos en el uso de CVAE, que agrega una feature adicional que es la informaci√≥n de la clase que se est√° aprendiendo (y que tambi√©n se usa para sintetizar), podr√≠a ayudar a salvar esta limitaci√≥n. Este modelo deber√≠a ser capaz de aprender la distribuci√≥n de los datos condicionada a cada clase, lo que permitir√≠a orientar la s√≠ntesis a lo que nos interese.

Para entender mejor todo este proceso, te propongo que explores el funcionamiento del VAE (y CVAE) usando datos artificiales. Pod√©s usar la funci√≥n make_blob de sklearn para generar 2 distribuciones en 2D y jugar con el efecto del solapamiento y del desbalance. Con esos datos trat√° de entrenar un VAE (y un CVAE) y fijate como se distribuyen los datos originales y los sintetizados por los diferentes modelos. La ventaja de usar dats en 2D es que vas a poder ir graficando los resultados del modelo y coloreando las clases para ver qu√© pasa. Cuando tengas algunas pruebas hechas podemos juntarnos y analizar lo que fuiste obteniendo. Una vez que tengamos clara toda esta parte, podemos empezar probando con datos para problemas binarios y luego extenderlo a multiclase (es decir, aumentar la complejidad de manera progresiva, teniendo claro como uno esperar√≠a que funcione). De paso te adelanto que antes de correr los clasificadores podr√≠a ser interesante buscar alguna medida o test estad√≠stico que nos permita evaluar si los datos sintetizados provienen de la misma distribuci√≥n para cada clase (estoy pensando en voz alta).

Por √πltimo, te dejo algunos enlaces que pueden ser √∫tiles para ayudar a entender todo esto que te coment√© m√°s arriba. En lo personal, me parecen super did√°cticos! 
Variational AutoEncoder: https://www.cs.us.es/~fsancho/Blog/posts/VAE.md
Understanding Conditional Variational Autoencoders: https://towardsdatascience.com/understanding-conditional-variational-autoencoders-cd62b4f57bf8
Conditional Variational Autoencoder -- Intuition and Implementation: https://agustinus.kristia.de/techblog/2016/12/17/conditional-vae/

Bueno, eso ser√≠a todo por ahora.
Te propongo que hagas las pruebitas que te coment√© m√°s arriba y, antes de seguir, coordinemos una reuni√≥n de 30 min o 1 hora para charlar los resultados y definir los siguientes pasos. M√°s que grandes avances y reuniones largas, tal vez convenga tener reuniones cada 15 d√≠as e ir avanzando de a poquito, as√≠ te podemos discutir los avances m√°s r√°pido y orientar mejor los esfuerzos.
Seguimos en contacto.
Saludos.

Matias

P/D: Me parece perfecto que vayas documentando todo. Va a ser de gran utilidad al momento de escribir la tesis!! (y alguna publicaci√≥n üòâ)
------
COMENTARIO A LA RESPUESTA DE MI DIRECTOR 
Matias, Leandro
Impecable Matias y Leandro, muy de acuerdo con tus observaciones. Estaba en la pista de CVAE para investigar, as√≠ que despu√©s de lo que me comentas creo que es fundamental mirar esa implementaci√≥n. Se me pas√≥ lo de BCE, ya lo corrijo. Voy a hacer la exploraci√≥n que me recomendas Mat√≠as para formar una primera intuici√≥n sobre esto. Creo que lo de agregar un test estad√≠stico para ver las distribuciones es muy valioso, lo le√≠ por ah√≠ en una implementaci√≥n, lo voy a recuperar. Acuerdo con el aproach, se v√© que te viniste con toda del descanso :) . Pongo manos a la obra que las recomendaciones y los mantengo al tanto. Gracias nuevamente a ambos!
-----
AVANCES
Claudio Sebasti√°n Castillo <castilloclaudiosebastian@gmail.com>
Mon, Mar 4, 9:17‚ÄØAM
to Matias, Leandro

Mat√≠as y Leando:

Buen d√≠a, bueno conforme a sus propuestas implement√© distintos cambios para si ten√≠a mejores resultados. Efectivamente como sospechaban aumentar un orden de magnitud en GCM rindi√≥ sus frutos. Llev√© el dataset sint√©tico a 3000 observaciones, balanceado, con 214 muestras por clase, y optuve el mismo resultado en datos de test que los que obtengo entrenando un MLP con datos reales. 
Implement√© por otro lado el L1 loss y drop_out, con varias pruebas y aqu√≠ no tuve mejoras. Qui≈∫as en este case la explicaci√≥n pase por el hecho que ya estoy trabajando con modelos que tienen buenos resultados (es decir estoy partiendo de una configuraci√≥n de CVAE que tienen buenos valores de reconstrucci√≥n),y que por ello no hay mucho espcio para variaci√≥n. El dropout lo prob√© con un CVAE chico y otro m√°s grande. 

Les paso el link del informe: informe: https://docs.google.com/document/d/1gWb-ZSH4YTOX-Dy3M3r5mJBqivUpua53VP44cAskPR4/edit?usp=sharing

Me pongo a trabajar en la primera versi√≥n de AG sobre leukemia. Los mantengo al tanto. 

Buena semana, claudio
-------
AVANCES
Implementado el AG para leukemia, madelon y gisette
Inbox

Claudio Sebasti√°n Castillo <castilloclaudiosebastian@gmail.com>
Tue, Mar 12, 8:56‚ÄØAM
to Matias, Leandro

Matias y Leandro buen d√≠a les paso reporte de la implementaci√≥n del algoritmo gen√©tico. Espero sus comentarios para avanzar con: 1) un algoritmo gen√©tico para el datset de GCM y 2) la integraci√≥n del VAE o CVAE en el AG, primera versi√≥n. 

https://docs.google.com/document/d/1FR382uvjXNNoPcQtaGlQ9LPCH0IjvX_IdNdq2rArGCY/edit?usp=sharing   

Cordialmente
Claudio
------
AVANCES
Resultados primer GA+VAE en leukemia
Inbox

Claudio Sebasti√°n Castillo <castilloclaudiosebastian@gmail.com>
Mon, Mar 25, 2:39‚ÄØPM
to Matias, Leandro

Matias y Leandro, les mando el acceso a la notebook donde est√°n resumidos el primer par de experimentos realizados sobre 'Leukemia' con datset original y con dataset aumentado en 100 observaciones generadas con un VAE. 
Encuentro que los datos son muy parecidos en todos los aspectos, como podr√°n observar en la tabla de 'Resumen Estad√≠stico' y en los Gr√°ficos de la Evoluci√≥n (De las 30 pruebas realizadas, les presento dos gr√°ficos para el experimento sin aumentar -'0001'- y dos gr√°ficos para el experimento con aumentaci√≥n -'0002'-). Tambi√©n encuentro que hay mucha variabilidad en las caracter√≠sticas seleccionadas, pese a que en todos los casos el 'acc' es casi optimo. Se deduce de eso que las caracter√≠sticas est√°n altamente correlacionadas? Habr√≠a que plantear un nuevo experimento, aumentando 1000 observaciones? O quiz√°s ya pasamos a otra estrategia de aumentaci√≥n como las que discutimos al principio (ejemplo: aumentaci√≥n en la descendencia)? Bueno, si les parece armamos una reuni√≥n para mirar los experimentos y discutir los resultados. Cordialmente, Claudio.
https://github.com/castillosebastian/ealab/blob/main/expga1/report_results.ipynb

------
AVANCES
Avances
Inbox

Claudio Sebasti√°n Castillo <castilloclaudiosebastian@gmail.com>
Attachments
Sun, May 12, 9:08‚ÄØPM
to Leandro, Matias

Buen d√≠a, Matias y Leando, bueno, ac√° le comparto un informe con los resultados de las pruebas realizadas. Creo que, para ir de lo general a lo particular, podr√≠a tener resultados m√°s estables en las t√©cnicas que estamos empleando (VAE+GA), compar√°ndola con un GA base, con dataset sin aumentaci√≥n. 

Dicho resultado se puede apreciar en el siguiente gr√°fico donde comparo 180 experimentos (90 con aumentaci√≥n y 90 sin aumentaci√≥n) sobre el dataset leukemia (con distintos par√°metros de configuraci√≥n del cromosoma). En el documento se pueden apreciar estas diferencias considerando los subgrupos de experimentos. 

image.png

Gr√°fico con subgrupos de experimentos

image.png

- Respecto de la selecci√≥n de caracter√≠sticas en el documento adjunto podr√°n ver las dos representaciones (experimentos con aumentaci√≥n y sin aumentaci√≥n). Sobre este tema estuve trabajando bastante pues pude confirmar que leukemia presenta muchas features de alta correlaci√≥n: el 32% del dataset tiene correlaci√≥n arriba de 0.7/-0.7. correlation_matrix.png
En este sentido, hice experimentos con la configuraci√≥n de individuos en nuestro algoritmo gen√©tico con probabilidad de genes activos de 0.01 y 0.005 (dando lugar a n_genes en un rango de 15 hasta 85), sin mayor consecuencia en el accuracy. 
- Ya tengo realizados experimentos de Madelon y Gisette con buenos resultados, pero me gustar√≠a conversar con ustedes estos hallazgos en leukemia para armar eventualmente nuevos experimentos con estos dos datasets.
Les mando un saludo cordial, esta semana estoy disponible para juntarme cuando digan.
Un abrazo 
Claudio 




 3 Attachments
  ‚Ä¢  Scanned by Gmail

Matias Gerard <mgerard@sinc.unl.edu.ar>
Wed, May 15, 6:59‚ÄØPM
to me, Leandro

Buenas noches Claudio,

Primero que nada y como siempre, muy detallado y completo el informe!
La verdad es que es una buena noticia que los experimentos muestren, la menos para este dataset, un mejor desempe√±o con el dataset aumentado.
Lo otro interesante es que se mantiene un muy buen desempe√±o cuando se fuerza a que el cromosoma seleccione pocos genes.
Esta semana es complicada para reunirme. Si les parece, podemos ver si coordinamos para la semana que viene. Podemos charlar un poquito de estos resultados y ver c√≥mo seguimos.
Avisen nom√°s cuando pueden as√≠ vamos definiendo d√≠a/horario. Por mi parte, el mi√©rcoles despu√©s de las 18 hs no tendr√≠a inconveniente en reunirme.
Aguardo sus respuestas.
Saludos.

Matias

P/D: Definimos algo de seguir con la siguiente etapa de inclusi√≥n de los VAE en los algoritmos evolutivos? O sobre trabajar con los otros datasets? (no recuerdo üòÖ)
---------

AVANCES
Resultados, primeros an√°lisis generales
Inbox

Claudio Sebasti√°n Castillo <castilloclaudiosebastian@gmail.com>
Fri, Jun 21, 7:08‚ÄØPM
to Matias, Leandro

Mat√≠as y Leandro, buen d√≠a. Bueno les cuento que, seg√∫n nuestra √∫ltima reuni√≥n, avanc√© sobre los experimentos en GCM, y ya tengo resultados. Dichos resultados y los obtenidos en los anteriores experimentos los comparto en documento adjunto donde hago un balance de c√≥mo se desempe√±√≥ la estrrategia de aumentaci√≥n en la implementaci√≥ de AG. Creo que los resultados generales son muy buenos, y que incluso donde la estrategia parece ser debil (el caso de GCM) hay un camino muy claro de lo que podr√≠a implementarse en pr√≥ximos trabajos. 
Conforme lo conversado con Mat√≠as, ya estoy escribiendo la tesis. El √≠ndice ser√≠a:
1. Introducci√≥n
2. Presentaci√≥n del problema, datos y contexto. 
3. Autocodificadores Variacionales, teor√≠a e implementaci√≥n.
4. Algoritmos Gen√©ticos: teor√≠a e implementaci√≥n
5. Experimentos y Resultados. 
Estoy terminando el cap√≠tulo 3. Ni bien lo cierre si les parece se los comparto. 
Bueno espero sus comentarios y la posibilidad para reunirnos a conversar los resultados. 
Un saludo cordial 
Claudio
Reporte: 
https://docs.google.com/document/d/1-I1gwvV1mCojANXwh0Rnau_1uhTO9q8Uo2F8vp4H-ZY/edit?usp=sharing
------
ULTIMO AVANCE
Novedades
Inbox

Claudio Sebasti√°n Castillo <castilloclaudiosebastian@gmail.com>
Fri, Aug 2, 8:31‚ÄØPM
to Matias, Leandro

Matias y Leandro, les paso los resultados de GCM en una nueva serie de experimentos donde se parte de una nuevas particiones de entrenamiento y testeo, y donde NUNCA! se mezclan dichas particiones: el CVAE y el AG solo se entrenan/corren con 'train' y evaluan en 'test'. 
Viendo el gr√°fico entiendo que estamos ante un resultado claramente positivo en los casos donde los individuos ten√≠an cromosomas con 750 y 450 genes activos en promedio, mientra que en el caso de una reducci√≥n m√°s dr√°stica la diferencia es menor, pero se mantiene tambien.

image.png

image.png


Ni bien me den el OK, me pongo a saltar. 
Cordialmente, Claudio





Matias Gerard
Mon, Aug 5, 7:47‚ÄØPM
to me, Leandro

Buenas noches Claudio.

En base a los resultados, creo que da para saltar ü§≠
Me llama la atenci√≥n que caiga tanto el desempe√±o al hacer m√°s estricto el n√∫mero de caracter√≠sticas a usar, porque no termino de entender qu√© est√° pasando.
En el boxplot se ve que la dispersi√≥n para los datos sint√©ticos es menor que para el dataset original, pero igual me sorprende la ca√≠da del desempe√±o.
Fuera de eso, me parece que es un muy buen resultado!
Tal vez agregar√≠a un punto intermedio entre 45 y 450 (podr√≠a ser 150 o 200) como para ver c√≥mo se produce la ca√≠da en el desempe√±o, y determinar si es o no abrupta.
Bien, para ver c√≥mo seguir les propongo que nos reunamos el pr√≥ximo lunes 12/8, a las 18 hs. Creo que con un ratito alcanza para organizarnos y acordar los siguientes pasos.
Les pido que confirmen si pueden as√≠ creo el evento y les mando la invitaci√≥n.
Saludos.

Matias

--
Matias F. Gerard
-------

""". TU TRABAJO ES: realizar una descripci√≥n resumira, clara , y con lenguaje t√©cnico de los pasos realizados en este proceso de trabajo, haciendo incapi√© en, los estudios iniciales, el estudio, dise√±o y contrucci√≥n un modelo gen√©rico de Autocodificador Variacional, la adaptici√≥n del AV a los dataset elegidos en el trabajo y la b√∫squeda de los mejores hiperpar√°metros, luego la creaci√≥n de una estructura combinada de AV y Algoritmo Gen√©tico para la selecci√≥n de Caracter√≠sticas, y, finalmente, la experimentaci√≥n final y resultados. 

############### RESPUESTA #############################3

Aqu√≠ tienes una descripci√≥n resumida del proceso de trabajo en tu tesis de Maestr√≠a:

---

### Descripci√≥n T√©cnica del Proceso de Trabajo

1. **Estudios Iniciales**:
   Se comenz√≥ evaluando el desempe√±o de modelos de aprendizaje autom√°tico tradicionales en cuatro conjuntos de datos (datasets) distintos. El objetivo fue medir m√©tricas como AUC, F1, y el tiempo de entrenamiento. Se trabaj√≥ con 18 modelos, optimizando hiperpar√°metros mediante GridSearch, utilizando una m√°quina virtual en la nube con 24 vCPUs y 30 GB de RAM.

2. **Dise√±o y Construcci√≥n de un Modelo Gen√©rico de Autocodificador Variacional (VAE)**:
   Se desarroll√≥ una primera versi√≥n de un VAE para generar datos sint√©ticos que se utilizaron para la aumentaci√≥n de datos en los datasets GCM y MADELON. Aunque la arquitectura inicial del VAE no logr√≥ capturar las distribuciones subyacentes de los datos de manera efectiva, se identificaron problemas en la codificaci√≥n y reproducci√≥n de las distribuciones, lo que llev√≥ a revisar la arquitectura y los m√©todos de evaluaci√≥n utilizados.

3. **Adaptaci√≥n del VAE a los Datasets Elegidos y B√∫squeda de Hiperpar√°metros**:
   Se realizaron ajustes en la arquitectura del VAE, incrementando la magnitud de datos sint√©ticos generados, lo que result√≥ en mejoras notables en el desempe√±o de los modelos cuando se entrenaron con estos datos. Tambi√©n se experiment√≥ con la p√©rdida L1 y dropout, aunque sin mejoras significativas en ciertos casos, posiblemente debido a configuraciones ya optimizadas del modelo CVAE.

4. **Creaci√≥n de una Estructura Combinada de VAE y Algoritmo Gen√©tico (GA) para la Selecci√≥n de Caracter√≠sticas**:
   Se implement√≥ un algoritmo gen√©tico en combinaci√≥n con el VAE en datasets como leukemia, madelon y gisette. Los experimentos iniciales mostraron que el desempe√±o del GA mejoraba cuando se integraban datos aumentados por el VAE, especialmente en t√©rminos de selecci√≥n de caracter√≠sticas.

5. **Experimentaci√≥n Final y Resultados**:
   Los experimentos finales se centraron en GCM, donde se observaron resultados positivos al utilizar datasets aumentados con CVAE y evaluados con GA, especialmente cuando los cromosomas conten√≠an un n√∫mero elevado de genes activos. Se continu√≥ experimentando con distintas configuraciones de genes activos para evaluar el impacto en el desempe√±o, identificando √°reas para mejorar y futuros trabajos.

---

Este resumen t√©cnico refleja los pasos clave en la ejecuci√≥n de tu trabajo de investigaci√≥n, destacando las iteraciones necesarias en la construcci√≥n y adaptaci√≥n del modelo de VAE, la integraci√≥n con algoritmos gen√©ticos, y la evaluaci√≥n a trav√©s de experimentos rigurosos.