# El problema de la selección de características

En el presente capítulo abordamos el problema de la selección de características, su importancia y desafíos. En ese marco, repasaremos ciertas dificultades asociadas a los datos, como por ejemplo la alta dimensionalidad y el desbalance de clases, y veremos cómo ellas impactan en la selección de características. Luego describiremos brevemente distintos enfoques para la selección de características, como así también ventajas y desventajas de cada uno. Finalmente, vincularemos la selección de características con el aporte fundamental de nuestro trabajo asociado a la generación sintética de datos. 

## Estrés, ignorancia y selección de características

Un punto de partida difícil de controvertir en el mundo actual del aprendizaje automático es que la cantidad de información disponible para investigar ha crecido dramáticamente en los últimos veinte años [@liWhenChatGPTComputer2023]. Conforme la vida se digitaliza, y la información almacenada en los sistemas aumenta, la generación de conocimiento parece menos determinada por el viejo problema de la escasez de factores y más por su nueva situación de abundancia. 

En este contexto, el desafío de trabajar con datos de alta dimensionalidad ha motivado la aparición de una serie de técnicas dirigidas a generar conocimiento y resultados precisos en escenarios complejos debido a la abundancia de información. Así, bajo el nombre de *selección de características* nació un area de estudio que busca resolver, precisamente, el problema de discernir *sistemáticamente* la información relevante de aquella que no lo es cuando se trabaja con muchas variables.

Según Bolón-Canedo y ot., el origen de este campo se remonta a los años '60, cuando Hughes, usando un modelo paramétrico, estudió la precisión de un clasificador bayesiano en función del número de características utilizado para predecir una variable objetivo [@bolon-canedoFeatureSelectionHighDimensional2015]. Por nuestra parte, entendemos que incluso se puede ir más lejos, a los años '30, cuando Fisher reprochaba el *estrés* con el que se buscaba conocimiento manipulando variables aisladas en lugar de mirar sus interacciones, proponiendo una *investigación experimental* más *abarcativa* y *eficiente* que permitiera obtener mayor conocimiento con la misma cantidad de observaciones [@fisherDesignExperiments1935].

Más allá de su origen, el poderoso impulso de considerar espacios de búsqueda cada vez más amplios y problemas cada vez más complejos, ha contribuido a que la selección de características se convierta en un campo de estudio activo e importante. 

En efecto, hoy no es extraño encontrar investigaciones donde los objetos de estudio superen las decenas de miles de dimensiones, como sucede en la investigación biomédica. Allí, los microarrays de ADN son ejemplos representativos de datos de alta dimensionalidad, que constituyen fuentes vitales de información en problemas que involucran expresión génica [@almugrenSurveyHybridFeature2019]. En sentido similar, datos de video -presentes en multiples aplicaciones-, datos financieros, información vinculada a interacciones sociales, entre otros, son ejemplos del tipo de problema que justifican el uso de técnicas de selección de características [@el-hasnonyImprovedFeatureSelection2020]. 

Podemos definir a la *selección de características* como *el proceso de detectar las características relevantes y descartar aquellas irrelevantes o redundantes, con el objetivo de obtener un subconjunto que describa adecuadamente un problema dado con una degradación mínima del rendimiento* [@bolon-canedoFeatureSelectionHighDimensional2015]. Aquí rendimiento se refiere a la medida de evaluación empleada para juzgar los resultados del proceso de selección. Los distintos enfoques para selección de características adhieren a la premisa de que podemos separar la información relevante de aquella que no lo es para predecir una variable objetivo, y por ende mejorar el desempeño de los modelos de aprendizaje. 

Nótese aquí que el *proceso* de selección de características es un proceso sistemático. Gran parte de su esfuerzo se centra en disponer de un método para discriminar, de manera precisa y controlada, la información relevante de la irrelevante o redundante. La precisión y el control destacan como dos características fundamentales debido a que la selección de características tiene lugar en un escenarios donde se ignora cual es el aporte de cada variable a la resolución del problema. Por eso, resulta necesario contar con un criterio de evaluación claramente definido que permitan examinar de forma objetiva y cuantificable el valor real de cada variable y sus interacciones. 

Bajo esa perspectiva, la selección de características busca determinar un subconjunto de atributos que satisfaga uno de los siguientes criterios[@vignoloEvolutionaryLocalImprovement2017]: 

1. El subconjunto con un tamaño especificado que maximice la precisión de la predicción.
2. El subconjunto de menor tamaño que satisfaga un requisito de precisión mínima.
3. El subconjunto que logre el mejor compromiso entre dimensionalidad y precisión.

El criterio a elegir dependerá de los objetivos del estudio y de las características del problema. Mas allá de eso, es fácil advertir que la selección de características supone como ventaja la reducción del espacio de búsqueda, y en cierto sentido es un remedio a la alta dimensionalidad. 

En efecto, en el contexto del aprendizaje automático es común enfrentar problemas representados por grandes conjuntos de variables, vicisitud que se asocia con la *maldición de la dimensionalidad* [@bolon-canedoFeatureSelectionHighDimensional2015]. En general, este fenómeno se presenta por la alta demanda computacional y costos asociados con la optimización de dichos espacios, volviendo a ciertos problemas intratables desde el punto de vista práctico. Para abordarlo, una alternativa posible es la reducción de dimensionalidad, que consiste en encontrar o construir matrices con menor número de columnas que las originales pero la misma carga de información relevante. Esas matrices de menor dimensión, puede usarse de manera más eficiente para modelar el problema objetivo, facilitando su interpretación y comunicación. El proceso de encontrar estas matrices se llama *reducción de dimensionalidad*, y uno de los métodos para lograrlo es la selección de características. 

Dicho lo anterior, es preciso reconocer que la abundancia de información trae consigo una serie de desafíos que impactan en la selección de características. En el siguiente apartado describiremos aquellos más relevantes para nuestro estudio.

## Las multiples cara de los datos problemáticos

### Alta dimensionalidad y escasez muestral

En primer lugar, un problema frecuente en el aprendizaje automático se presenta cuando disponemos de pocas muestras en un espacio vectorial de alta dimensionalidad. Es decir, cuando el número de muestras ($m$) es menor que el número de características ($n$), siendo $n$ particularmente grande. Como vimos, esta situación es común en el campo del análisis genético, el procesamiento de información médica, procesamiento de video, entre otros. Como veremos mas adelante, tres de los datasets elegidos para nuestro estudio -vinculados al ámbito biomédico- se encuentran en esta situación. 

La escasez de muestras dificulta el tratamiento de la información, ya que la cantidad de datos disponibles para entrenar un modelo es insuficiente para capturar la variabilidad inherente a los datos. En tales circunstancias, el proceso de aprendizaje tiende a sufrir severas limitaciones, bien sea sobreajustándose a las observaciones disponibles (llevando a modelos poco generalizables), o bien resultando incapaz de capturar la estructura subyacente de los datos (lo que puede llevar a modelos poco precisos). 

Por ejemplo, en el campo de la clasificación de perfiles de expresión génica, el problema se considera difícil de resolver debido a la complejidad que supone identificar los genes que contribuyen a la aparición de ciertas condiciones - como por ejemplo: cáncer, diabetes, entre otros [@almugrenSurveyHybridFeature2019]. Dada la gran cantidad de genes presentes en estos supuestos (muchos de ellos irrelevantes), entrenar un modelo con todos ellos puede conducir a resultados erróneos. Este desafío, comúnmente se ve acentuado por la baja cantidad de observaciones disponibles y por el hecho de que los genes se encuentran altamente correlacionados. Dichas características dificultan el adecuado funcionamiento de los métodos clásicos de aprendizaje automático, cuyo rendimiento se asocia a la cantidad de muestras disponibles.

Ante estos problemas, la selección de características tiene severas limitaciones, ya que la alta dimensionalidad y bajo número de muestras dificultan la distinción entre información relevante y irrelevante. 

### Desbalance de clases

El desbalance de clases constituye otro problema importante y frecuente en los desafíos actuales que se presentan para el aprendizaje automático. El mismo sucede cuando la distribución de las clases en el conjunto de datos es altamente desigual: típicamente una clase mayoritaria supera ampliamente las observaciones de una o más clases minoritarias. Este problema puede darse en combinación con alta dimensionalidad y escacez muestral, agravando la dificultad de selección de características. 

El desbalance de clases es un problema relevante en aplicaciones donde la o las clases minoritarias son precisamente las de mayor interés, como por ejemplo: el diagnóstico de enfermedades raras, la detección de fraudes bancarios, la identificación de intrusiones en redes y la predicción de fallos en equipos técnicos. En supuestos como estos, los clasificadores que no contemplen un tratamiento explícito del problema tienden a sesgarse hacia la clase mayoritaria, pudiendo alcanzar una alta precisión global mientras fallan en la detección de los casos más importantes pero menos frecuentes.

Para atacar este problema, se han propuesto diversas soluciones, que pueden categorizarse en tres grupos:

1. **Muestreo de datos**: Este enfoque modifica las muestras de entrenamiento para producir una distribución de clases más balanceada. Las técnicas tradicionales incluyen: submuestreo (*undersampling*) donde se cre un subconjunto del conjunto original eliminando instancias; sobremuestreo (*oversampling*) donde se gnera un superconjunto replicando instancias existentes o creando nuevas, y finalmente, métodos híbridos que combinan ambas técnicas de muestreo.

2. **Modificación algorítmica**: Este enfoque adapta los métodos de aprendizaje para que sean más sensibles a los problemas de desbalance. Por ejemplo, el uso de la distancia de Hellinger como criterio de división en árboles de decisión, que ha demostrado ser insensible al sesgo, capturando la divergencia en las distribuciones sin ser dominada por las probabilidades previas de clase.

3. **Aprendizaje sensible al costo**: Esta solución puede incorporar elementos a nivel de datos, a nivel de algoritmos, o ambos a la vez, asignando costos más altos a la clasificación errónea de ejemplos de la clase positiva (minoritaria). Así, en el diagnóstico médico, resulta más importante reconocer la presencia de una enfermedad rara que su ausencia, por ello el costo de un falso negativo es mayor que el de un falso positivo.

El desbalance de clases también supone un desafío para la selección de características. Ello es así, en la medida que dichos métodos suponen la búsqueda de aquellas dimensiones que maximizan la separación de clases en su conjunto, sin ponderar sus diferencias para capturar información para cada clase en particular. Esto ocurre porque, en escenarios con distribución fuertemente desequilibrada, las técnicas de evaluación tienden a priorizar la identificación de la clase mayoritaria, reduciendo la relevancia de las características que serían valiosas para discriminar a las minoritarias. Como resultado, se descarta información relevante para la predicción de los casos poco frecuentes, y se seleccionan características que no son representativas para todas las clases por igual.

Siguiendo a Bolón-Canedo y ot., junto a los problemas mencionados sobre alta dimensionalidad y desbalance de clases, la complejidad y el ruido de los datos también representan desafíos importantes en el tratamiento de la información que debemos considerar en el presente estudio. 

### Complejidad

Respecto a la complejidad, se refiere a la dificultad de identificar las fronteras de decisión entre clases, que pueden manifestarse en tres aspectos fundamentales (todos ellos pueden coexistir en un mismo problema): 

1. **Ambigüedad de clases**: Surge cuando los casos no pueden distinguirse utilizando las características disponibles, ya sea porque los conceptos de clase están mal definidos y por lo tanto son intrínsecamente indistinguibles, o porque las características seleccionadas son insuficientes para discriminar las clases.

2. **Complejidad de fronteras**: Se refiere a situaciones donde los límites entre clases requieren una descripción extensa de la frontera de decisión, llegando a demandar, en el caso extremos de complejidad, la enumeración exhaustiva de todos los puntos con sus etiquetas de clase. Este aspecto de dificultad se debe a la naturaleza del problema y no a la muestra o selección de características, indicando que una completa separación entre clases es un problema difícil de resolver.

3. **Dispersión de muestras**: La combinación de pocas muestras y alta dimensionalidad genera una dispersión que dificulta la generación de fronteras de decisión. 

Anque los casos 1 y 2 son problemas ante los cuales la selección de características no puede ofrecer una solución efectiva, el caso 3, asociado con alta dimensionalidad y escasez muestral, podría ser mitigado por estrategias de aumentación de datos. 

### Ruido 

Los datos del mundo real siempre están expuestos a imperfecciones, ruido, proveniente de entornos dinámicos donde las dimensiones de interés de un fenómeno coexisten en espacios de interacciones permanentes. Así, aún considerando un escenario artificial, completamente libre de interferencia, la imperfección puede provenir de diversas fuentes como dispositivos de medición defectuosos o limitados, errores de transcripción o irregularidades en la transmisión de información. 

Ante tales circunstancias, existen cuatro enfoques principales para abordar estas imperfecciones en los conjuntos de datos:

1. **Conservación del ruido**: En este enfoque, se mantiene el conjunto de datos tal como está, con sus instancias ruidosas. Los algoritmos que utilizan los datos se diseñan para ser robustos, es decir, capaces de tolerar cierta cantidad de ruido. Una estrategia común es desarrollar algoritmos que eviten el sobreajuste al modelo (como sucede en el caso de los árboles de decisión) mediante técnicas de poda.

2. **Limpieza de datos**: Este método implica descartar las instancias que se consideran ruidosas según ciertos criterios de evaluación. El clasificador se construye utilizando únicamente las instancias retenidas en un conjunto de datos más pequeño pero más limpio. Sin embargo, este enfoque presenta dos debilidades significativas:
   - Al eliminar instancias completas, se puede descartar información potencialmente útil, como valores de características no corrompidos.
   - Cuando existe una gran cantidad de ruido, la información restante en el conjunto de datos limpio puede resultar insuficiente para construir un clasificador eficaz.

3. **Transformación de datos**: Este enfoque busca corregir las instancias ruidosas en lugar de eliminarlas. Las instancias identificadas como ruidosas se reparan reemplazando los valores corrompidos por otros más apropiados, y luego se reintroducen en el conjunto de datos.

4. **Reducción de datos**: Esta estrategia implica reducir la cantidad de datos mediante la agregación de valores o la eliminación y agrupación de atributos redundantes. La reducción de dimensionalidad -y consecuentemente la selección de características- es una de las técnicas más populares para eliminar características ruidosas (es decir, irrelevantes) y redundantes.

Vistos los problemas que enfrentamos con los datos, pasemos a describir, brevemente, los distintos enfoques que se han propuesto para sortearlos empleando estrategias de selección de características.

## Distintos enfoques para la selección de características

Podemos agrupar los métodos de selección de características en tres grandes grupos:

- Filtros,
- Wrappers o métodos envolventes, y
- Embedded o métodos embebidos

Los métodos de **Filtro** se basan en las características generales de los datos de entrenamiento y realizan la selección de características como un paso de preprocesamiento independiente del algoritmo de aprendizaje. El análisis de relevancia de una característica se realiza considerando sus propiedades intrínsecas, sin determinar sus relaciones posibles con otras. El resultado de dicho análisis es una lista de características ordenadas por relevancia (ranking), donde el subconjunto final de características se selecciona según ese orden. Este enfoque es ventajoso por su bajo costo computacional, rapidez y escalabilidad, pero deja sin resolver el problema apuntado por Fisher sobre la interacción entre variables. Los metodos de filtro se pueden sub-clasificar en univariados y multivariados [@solorio-fernandezReviewUnsupervisedFeature2020]. Los primeros analizan cada característica de manera independiente con el fin de obtener una lista ordenada. Este tipo de métodos puede identificar y eliminar eficazmente características irrelevantes, pero no son capaces de eliminar las redundantes, ya que no consideran posibles dependencias entre las características. Ejemplos de estos métodos son: la evaluación utilizando la distribución *chi-cuadrado* , la *ganancia de información*, entre muchos otros  [@bolon-canedoFeatureSelectionHighDimensional2015]. Los métodos filtro multivariados evalúan la relevancia de las características de forma conjunta en lugar de hacerlo individualmente. Por ejemplo, el método de selección hacia adelante (forward selection) y hacia atrás (backward selection) son métodos de filtro multivariados que sucesivamente agregan y eliminan características para obtener un subconjunto óptimo. Los métodos multivariados pueden manejar características redundantes e irrelevantes; por lo tanto, en muchos casos, la precisión alcanzada por los modelos empleando subconjuntos seleccionados con estos métodos puede ser mayor. Otros métodos filtro multivariados puden ser el *método basado en el análisis de correlación* y *algoritmo ReliefF*, entre otros [@bolon-canedoFeatureSelectionHighDimensional2015]. 

Los métodos **Wrappers o Envolventes** involucran un algoritmo de aprendizaje como caja negra y consisten en usar su resultado para evaluar la utilidad relativa de subconjuntos de características. Aquí, el algoritmo de selección utiliza el método de aprendizaje como una subrutina, para encontrar los subconjuntos de características más relevantes. Esta estrategia es capaz de reconocer variables relevantes y capturar dependencias entre ellas, pero a un costo computacional mayor que los otros métodos. En ese sentido enfrenta desafíos importante cuando el conjunto de datos es de alta dimensionalidad, ya que evaluar $2^n$ subconjuntos de características puede resultar en un problema intratable. Por esa razón, se recurre comunmente a heurísticas de búsqueda capaces de encontrar soluciones adecuadas sin explorar todo el espacio del problema [@zhangFeatureSelectionMultiview2019].

Los métodos **Embedded o Embebidos** realizan la selección de características durante el proceso de entrenamiento de un modelo de aprendizaje, y suelen ser específicos para determinados algoritmos (e.g. árboles de decisión y métodos derivados de ellos, eliminación recursiva de características mediante SVM, entre otros). A diferencia de los métodos de filtro y wrappers, los métodos embebidos no separan la selección de características del proceso de entrenamiento, sino que la realizan de manera simultánea. En este caso, optimizan una función de pérdida regularizada con respecto a dos conjuntos de parámetros: los parámetros del algoritmo de aprendizaje y los parámetros que indican las características seleccionadas.[@bolon-canedoFeatureSelectionHighDimensional2015]. Este enfoque es capaz de capturar dependencias a un costo computacional menor que los wrappers. 

![Diagrama de enfoques para la selección de características](diagrama_seleccion_características.png)

Bolón-Canedo y ot. nos invitan a pensar que no existe un método que sea *superior a los otros* de manera general, sino que cada uno tiene sus ventajas y desventajas, y se ajustan distinto a diferentes tipos de contexto. De los tres enfoques, los métodos de filtro son los únicos que son independientes del algoritmo de aprendizaje, lo que les permite ser rápidos y eficientes computacionalmente. Sin embargo, los métodos de filtro no consideran la correlación entre características, lo que puede llevar a la selección de características irrelevantes o redundantes. 

Los métodos de envolventes y embebidos, por su parte, consideran la correlación entre características y etiquetas de clase, lo que les permite identificar patrones relevantes correctamente durante la fase de aprendizaje. Sin embargo, estos métodos son más complejos computacionalmente y requieren evaluación iterativa del subconjunto seleccionado de características.

En el caso de los métodos envolventes, a medida que el número de características aumenta, el costo computacional se incrementa exponencialmente, lo que limita su aplicación en problemas de alta dimensionalidad [@bolon-canedoFeatureSelectionHighDimensional2015]. En línea con esta dificultad, se han propuesto distintas formas de superar el problema, como el uso de heurísticas de búsqueda para evitar exploraciones exhaustivas. Entre ellas, se han propuesto a los métodos evolutivos como herramientas eficaces para la búsqueda de soluciones óptimas [@vignoloEvolutionaryLocalImprovement2017]. El propósito de este enfoque es explorar el espacio de soluciones de manera eficiente, permitiendo la identificación de subconjuntos de características que sean óptimos en términos de rendimiento. 


## Selección, algoritmos genéticos y datos sintéticos

Hasta aquí hemos visto que problemas tales como la dimensionalidad, el desbalance de clases, el ruido y la complejidad plantean verdaderos desafíos para los procesos de aprendizaje automático, y de qué manera la selección de características puede ser una herramienta para mitigarlos.  Asimismo, hemos descrito los distintos enfoques para la selección de características, resaltando el hecho de que los métodos envolventes utilizan algoritmos genéticos para realizar búsquedas eficientes en espacios de gran dimensionalidad.  Seguidamente, veamos de qué manera la generación sintética de datos puede contribuir al funcionamiento de los algoritmos genéticos, y de esa forma mejorar los procesos de selección de características basados en métodos envolventes. Este será el aporte de la presente tesis.

Como dijimos, en el contexto de los métodos envolventes, los algoritmos genéticos representan una solución el problema de la evaluación iterativa de subconjuntos de características en datos de alta dimensionalidad. Tal evaluación debe replicarse por cada subconjunto a lo largo del espacio de búsqueda, lo que resulta en un costo computacional elevado cuando el conjunto de variables a evaluar es grande [@zhangFeatureSelectionMultiview2019]. 

Inspirados en los principios de la evolución natural, los algoritmos genéticos son capaces de realizar esa exploración eficientemente, iterando sobre múltiples generaciones de individuos (subconjuntos de características), encontrando combinaciones cada vez más relevantes. Exploraremos esto en detalle en el capítulo 4, sin embargo, cabe señalar desde ahora que dicha capacidad está determinada en gran medida por el conjunto de operadores evolutivos con los que trabajan los algoritmos genéticos: la **selección**, paso crítico del proceso de búsqueda, otorga más oportunidades de reproducción a los individuos mejor evaluados (en este caso, los subconjuntos de características con mayor capacidad de discriminación, cuyo desempeño se determina mediante un modelo de aprendizaje automático); el **cruce**, que combina partes de la solución de dos individuos para generar descendientes potenciamente mejores; y la **mutación**, que introduce variaciones aleatorias y evita la convergencia prematura hacia subóptimos locales. A través de iteraciones sucesivas, estos operadores permiten explorar de manera equilibrada tanto regiones prometedoras del espacio de búsqueda como soluciones novedosas, optimizando la búsqueda de subconjuntos de características incluso en escenarios con alta dimensionalidad y complejidad.

Dicho lo anterior, es preciso advertir que la eficacia de los algoritmos genéticos depende en gran medida de la disponibilidad de datos suficientes para evaluar el valor de los individuos (subconjunto de características). En efecto, la falta de datos para estimar de forma confiable el desempeño de las características analizadas, agrega incertidumbre al proceso de **selección**, que es el paso crítico del proceso de búsqueda. Esta estimación es realizada por un modelo de aprendizaje automático con el que se evalúa el desempeño de los subconjuntos (por ejemplo, un clasificador basado en árboles de decisión, máquinas de soporte vectorial, o un perceptrón multicapa). Por eso, con respecto a este componente en particular, el algoritmo genético se encuentra en la misma situación que cualquier otro método de optimización, en el sentido de que la escasez de datos afecta directamente la calidad de las predicciones utilizadas para guiar la búsqueda. Cuando los datos son insuficientes o no representan adecuadamente la variabilidad del problema, aumenta la incertidumbre en la fase de evaluación de los individuos, comprometiendo la confiabilidad del proceso de selección y la eficacia general del algoritmo genético. 

En este escenario, la generación sintética de datos surge como una estrategia prometedora para enriquecer la información disponible y mitigar la incertidumbre durante la evaluación de cada subconjunto de características [@gmComprehensiveSurveyAnalysis2020]. Fortaleciendo la robustez del modelo de aprendizaje automático, la generación de datos sintéticos o artificiales mejoraría la capacidad de los AGs para explorar y discriminar soluciones óptimas en espacios de gran dimensionalidad. 

Existen varias técnicas de aumentación de datos, entre las que destacan los Autocodificadores Variacionales. Estos modelos, con arquitectura encoder-decoder, han adquirido popularidad, superando en desempeño a métodos tradicionales (ej. sobremuestreo [@blagusSMOTEHighdimensionalClassimbalanced2013]) y a otros modelos generativos [@fajardoOversamplingImbalancedData2021]. Los AVs han demostrado ser especialmente adecuados para escenarios donde se busca preservar la estructura estadística subyacente de los datos y, a la vez, generar muestras lo suficientemente diversas para captar la variabilidad del problema. Profundizaremos en esta arquitectura en el capítulo 3, sin embargo, cabe adelantar aquí que estos modelos se basan en la estimación y reconstrucción de distribuciones latentes continuas, lo que les permite reconocer patrones relevantes y producir muestras sintéticas similares a los datos originales. Por eso, los AVs se han usado para aumentación de datos en el campo del tratamiento de imágenes [@fajardoOversamplingImbalancedData2021; @aiGenerativeOversamplingImbalanced2023; @khmaissiaConfidenceGuidedDataAugmentation2023; @kwarciakDeepGenerativeNetworks2023], texto [@zhangImproveDiverseText2019] , habla [@blaauwModelingTransformingSpeech2016; @latifVariationalAutoencodersLearning2020] y música [@robertsHierarchicalLatentVector2019], y distintos formatos de datos: tabulares [@leelarathnaEnhancingRepresentationLearning2023], longitudinales [@ramchandranLearningConditionalVariational2022] y grafos [@liuConstrainedGraphVariational2018]. 

La versatilidad y capacidad de los AVS, los convierte en una herramienta potencialmente transformadora para los procesos de selección de características basada en AGs, abriendo la posibilidad de aplicaciones en escenarios muy diversos, y problemas que involucran distintas modalidades de datos. 

Volviendo a los problemas señalados anteriormente, la generación de muestras sintéticas puede realizar un importante aporte al problema del desbalance de clases. En escenarios críticos con problemas multiclase y distribuciones asimétricas, la generación sintética de datos puede ofrecer la posibilidad de reequilibrar las proporciones de cada categoría. En efecto, creando suficientes muestras representativas de las clases minoritarias, la generación sintética de datos puede mejorar la discriminación entre categorías poco representadas. Con esta estrategia, se reduciría el sesgo introducido por la disparidad de muestras y, por ende, se mejoraría el proceso de selección de características.

Por último, es importante destacar que la generación sintética de datos mediante AVs también puede contribuir a mitigar el efecto del ruido en los modelos de aprendizaje. Durante el proceso de codificación-decodificación implementado por estos modelos, el proceso de reconstrucción identifica y proyecta las características esenciales de los datos, favoreciendo la reducción de información irrelevante. Este mecanismo, analizado en mayor detalle en el capítulo 3, abonaría la importancia de los AVs en la creación de muestras sintéticas no solo para reducir el impacto negativo del ruido sobre el proceso de entrenamiento, sino también para mejorar el desempeño de los métodos evolutivos en la búsqueda de características óptimas.

Todas estas razones respaldan la importancia de la generación sintética de datos para la selección de características con AGs. En la presente tesis se propone una estrategia integral que vincula la selección de características y la generación sintética de datos, enfocándose en solucionar problemas habituales en aprendizaje automático, tales como la alta dimensionalidad, el desbalance de clases y la escasez de muestras.

Siguiendo esta línea, la tesis se organiza de la siguiente manera:

- En el capítulo 2, se realiza un análisis preliminar de los datos con los que trabajaremos, se describen los modelos de aprendizaje automático evaluados para servir de función objetivo en el Algoritmo Genético que desarrollaremos, y se presentan las métricas de evaluación aplicadas a lo largo del estudio. Además, se discuten potenciales dificultades asociadas a la naturaleza de los datos.
- En el capítulo 3, se profundiza en la teoría de los Autocodificadores Variacionales (AVs) y en la motivación para usarlos como técnica de aumentación de datos. Se detallan los experimentos de generación sintética, los resultados obtenidos y la forma en que estos modelos capturan la estructura subyacente de la distribución original, contribuyendo así otorgar calidad a los datos generados.
- En el capítulo 4, se presenta el Algoritmo Genético desarrollado, describiéndose la integración de los AVs como etapa de preprocesamiento para enriquecer la selección de características. Asimismo, se exponen los experimentos realizados tanto con datos originales como con datos aumentados, demostrando el impacto de la generación sintética en el rendimiento de la selección de características.
- Finalmente, en el capítulo 5, se exponen las conclusiones de la investigación, resaltando la relevancia de la generación sintética de datos para la selección de características en escenarios complejos, y se ofrecen perspectivas de trabajo futuro donde las técnicas propuestas podrían ampliarse o refinarse.

De este modo, se establece un marco metodológico completo que explora, integra y evalúa la sinergia entre Algoritmos Genéticos, Autocodificadores Variacionales y selección de características, sirviendo como aporte innovador para atender las dificultades impuestas por la dimensionalidad, el desbalance, y la escasez de datos en el ámbito del aprendizaje automático.