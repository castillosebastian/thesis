Quiero que escribas el capítulo donde se describen los experimentos realizados  en el marco de mi investigación.
Debes realizar una descripción detallada de los experimentos, resaltando la metodologías empleadas sin mencionar aún los resultados.
Cuando expliques la metodología deber clarificar el razonamiento detrás de cada decisión tomada. 
El lenguaje debe ser técnico y preciso, dirigido a un público especializado en el campo del aprendizaje automático.
Puedes agregar información que consideres relevante en cada caso. 
Debe desarrollar paso a paso los trabajos realizados en el  dataset:

- leukemia,
- gisette,
- madelon,
- gcm.

La información que debes incluir en el desarrollo de cada experimento es la siguiente:

### Contenido general de la investigación:

'''
En el campo del aprendizaje automático, la selección de características es una tarea crítica que puede determinar el éxito o fracaso de un modelo predictivo. La alta dimensionalidad y la complejidad inherente a los conjuntos de datos reales hacen que la selección de un subconjunto óptimo de características sea, muchas veces, un paso ineludible para el aprendizaje efectivo.

En este contexto, los Algoritmos Genéticos (AGs) se han consolidado como una herramienta poderosa para resolver problemas de optimización complejos, incluida la selección de características. Estos algoritmos, inspirados en la evolución natural, son capaces de explorar grandes espacios de búsqueda de manera efectiva, proporcionando soluciones cercanas al óptimo en una variedad de escenarios. Por ello, los AGs han sido ampliamente utilizados en problemas de selección, demostrando su eficacia en la identificación de subconjuntos relevantes de características en datos de alta dimensionalidad.

Sin embargo, la eficacia de los AGs depende de la disponibilidad de suficientes datos para evaluar las soluciones en competencia. En contextos donde los datos son limitados, los AGs pueden verse afectados en su capacidad discriminativa, produciendo soluciones subóptimas o inestables. Esta limitación es especialmente crítica en problemas de alta dimensionalidad y bajo número de muestras, donde la función objetivo que guía la búsqueda de soluciones puede degradarse significativamente.

Por esta razón, la investigación de estrategias que mitiguen las limitaciones impuestas por la escasez de datos se ha convertido en un área de interés creciente en el subcampo de la selección de características. Una de las técnicas emergentes en este ámbito es la aumentación de datos mediante Autoencodificadores Variacionales (AVs). Los AVs, como modelos generativos, tienen la capacidad de crear muestras sintéticas que mantienen las propiedades fundamentales de los datos originales, conviertiéndolos en una herramienta prometedora para mejorar la capacidad de los AGs en la selección de características.

El problema central de la tesis que aquí presentamos giró, precísamente, en la restricciones que la escacez de datos impone a los AGs, y cómo superarlas usando AVs. La pregunta que guíó nuestro trabajo ha sido: ¿cómo puede la aumentación de datos mediante autoencodificadores variacionales mejorar el desempeño de los algoritmos genéticos en la selección de características?

Cabe destacar que este desafío y su eventual resolución son importantes por varias razones. La selección de características no solo condiciona la precisión de los modelos predictivos, sino también afecta la eficiencia computacional y la interpretabilidad de los resultados. En problemas de alta dimensionalidad, la capacidad de reducir el número de características relevantes sin perder información útil puede marcar la diferencia entre un modelo efectivo y uno ineficaz, entre uno interpretable y uno de caja negra. Por lo tanto, mejorar este proceso mediante la integración de técnicas de aumentación de datos puede tener un impacto significativo en diversas aplicaciones prácticas, desde la biología molecular hasta la ingeniería y las ciencias sociales.

La hipótesis que hemos llevado a prueba ha sido *que la aumentación de datos mediante AVs mejora la capacidad discriminativa de los AGs, permitiendo la identificación de subconjuntos de características más relevantes y estables en contextos de escasez muestral*. Para evaluarla, hemos propuesto un trabajo experimental que exploró la integración de estas dos técnicas en un marco unificado. La idea de combinar la generación de datos sintéticos mediante AVs con la selección de características mediante AGs, estaba orientada a buscar combinaciones sinérgicas y eficaces entre modelos que mejorasen la selección de características. A estos fines, trabajamos con cuatro conjuntos de datos de referencia, representativos de distintos contextos y niveles de complejidad, para evaluar el desempeño de los modelos propuestos.

A lo largo de este documento, describiremos el proceso de investigación llevado a cabo, desde los estudios iniciales hasta los experimentos finales, pasando por el diseño y construcción de un modelo genérico de AV, y su adaptación a los datasets elegidos y la creación de una estructura combinada de AV + AG para la selección de características. Los resultados obtenidos en cada etapa se analizarán y discutirán en detalle, con el objetivo de identificar las ventajas y limitaciones de la propuesta, así como posibles áreas de mejora y futuros trabajos.

Al finalizar el documento, esperamos poder justificar la eficacia de la aumentación de datos mediante AVs en la selección de características, demostrando que esta técnica puede mejorar significativamente el desempeño de los AGs en contextos de escasez. Además, esperamos identificar las condiciones y contextos en los que esta técnica es más efectiva.

El documento está estructurado de la siguiente manera: en el capítulo 2 se presenta una revisión del estado del arte, destacando las principales contribuciones en el ámbito de la aumentación de datos y la selección de características, así como los resultados de modelos clásicos aplicados a los datasets seleccionados para este trabajo. El capítulo 3 describe el diseño y construcción de nuestro modelo de Autoencodificador Variacional, desde una revisión teórica hasta la implementación particular que hemos desarrollado. En el capítulo 4 se ofrece una breve descripción de los Algoritmos Genéticos, resaltando los aspectos más relevantes de esta técnica aplicada a la selección de características. En el capítulo 5 se presentan y analizan los resultados obtenidos a partir de los experimentos realizados, incluyendo la evaluación de los modelos propuestos. En el capítulo 6, se exponen las conclusiones de la investigación, así como posibles líneas futuras de trabajo.
'''

### Experimento 1: Leukemia 
'''
Comparar original vs. aumentacion con igual  configuraciíon. El objectivo era tener una primera intuición de los resultados de ambos 
experimentos.  
0001 
0002
PROB_MUT = 1/IND_SIZE(0.0001); PX = 0.75, cromosoma activo es p=0.1, GMAX=20. Los parametros fueron los mismos para los dos grupos de experimentos (con/sin aumento).La autmentación generó 100 muestras. 
#RESULTADOS#: 
Resultados similares en ambos grupos de experimentos respecto de ACC y  NGENES.  
La velocidad de convergencia hacia el máximo (1.0) no muestra diferencia sustancial entre los dos grupos de experimentos.
La cantidad de características seleccionadas al menos una vez en todas las pruebas realizadas en cada uno de los dos grupos de 
experimentos llega a 6779 (datos aumentados) y 6772 (datos originales), quiere decir que la mayoría de las características apareció 
al menos una vez entre las características encontradas por el GA, para los distintos escenarios de experimentación. 
Esto sucede porque hay mucha correlación entre las características. Esto me llevó a observar: 
“Los features están muy correlacionados. Con el 10% de los datos se resuelve el problema, y no importa cuál 10%?” 
La correlación que tienen las características del dataset muestra estos resultados, para :
-Número de correlaciones significativas: 80742 (Este es el número total de pares de características que tienen una correlación absoluta mayor a 0.7.
-Número total de pares posibles de características: 25407756.0
-Porcentaje de correlaciones significativas: 0.32%
Los resultados indican que hay una cantidad notable de características altamente correlacionadas, pero estas representan una pequeña 
fracción del total de pares posibles.
Se observa, más allá de las semejanzas de resultados entre ambos grupos de experimento,  menor dispersión en los resultados. 
/home/sebacastillo/ealab/expga1/R1_report_results.ipynb
------
En este meta-conjunto de experimentos se intervino de manera más activa, buscando distintas configuracion de hiperparámetros para 
poner a prueba la arquitectura. 
Datos Originales
0012
0013
Datos Aumentados
0003
0006
0007
0008
0008
0011
Grupos de experimentos exploratorios donde investigué distintas configuraciones de la arquitectura construida (AV+GA), y el correspondiente experimento de control con el dataset original. 
Grupo de experimentos con dataset original.
- '0012'  original, cromosoma activo 0.01 y alpha 0.5, 30 pruebas.
- '0013'  original, cromosoma activo 0.005 y alpha 0.5, 30 pruebas.
Grupo de experimentos con dataset aumentado.
- '0002  aumentado en 100 observaciones , cromosoma activo 0.1, alpha 0.5, 30 pruebas.  
- 0003  aumentado en 1000 observaciones , cromosoma activo 0.1, alpha 0.5, 30 pruebas. 
- 0006  aumentado en 100 observaciones , cromosoma activo 0.01 y alpha 0.3, 10 pruebas.
- 0007  aumentado en 100 observaciones , cromosoma activo 0.01 y alpha 0.5, 10 pruebas.
- 0008  aumentado en 100 observaciones , cromosoma activo 0.01 y alpha 0.2, 30 pruebas.
- 0009  aumentado en 100 observaciones , cromosoma activo 0.01 y alpha 0.5, 30 pruebas.
- 0011  aumentado en 100 observaciones , cromosoma activo 0.005 y alpha 0.5, 30 pruebas.
------
Generación de un dataset de muchas observaciones por clase n=1000 x clase.
0003
Aumento en 1000 observaciones sintéticas para cada clase. 
#RESULTADOS#:
El acc 1.0, como los demás grupos de experimentos, sin mejora de la velocidad de convergencia, y con un incremento del costo computacional
 debido a la cantidad de observaciones. 
------
Reducción del cromosoma activo a 0.01
0006 Aumentado
0009 
0012 Original
Aumentado en 100 observaciones por clase, y con cromosoma activo es p=0.01 (n_genes entre [63, 79]). 
El acc sigue siendo alto pese a la agresiva reducción del espacio de búsqueda. 
Data set aumentado la convergencia no es más rápida que en el dataset sin aumentación.
------
Reducción del cromosoma activo 0.005.
0011 Aumentado
0013 Original
Cromosoma activo de 0.005 ([24,46]), con acc alto, entre 0.99/1.0. Los resultados son similares en ambos grupos de experimentos. 
'''

### Experimento 2: Gisette
'''
Comparación de original vs aumentado.
0020 Original
0022 Original
0005 Aumentado

PROB_MUT = 1/IND_SIZE(0.0002); PX = 0.75, cromosoma activo es p=0.1, GMAX=30. 
Los parametros fueron los mismos para los dos grupos de experimentos (con/sin aumento). Se generaron 600 datos sintéticos.
Los resultados no variarion en lo que respecta a la precisión en la clasificación.
Sin hay una reducción en la cantidad de características seleccionadas.
------
Comparación de original vs aumentado.
Reducción agresiva del espacio de búsqueda
0024 Original
0025 Aumentado
Reducción agresiva del tamaño del cromosoma: Ind p = 0.01, aumentado en 6000 observaciones. Genes [36,60]. 
El dataset aumentado se desenvuelve mejor. Ver scatterplot gisette.
/home/sebacastillo/ealab/expga1/R3_report_feature_gisette_madelon.ipynb
'''

### Experimento 3: Madelon
'''
Comparación de original vs aumentado.
0014 Aumentado
0023 Original
0017 Original
Dataset con 5 características relevante, 15 combinaciones lineales de aquellas y las demás reuido.
PROB_MUT = 1/IND_SIZE(0.002); PX = 0.75, cromosoma activo es p=0.1, GMAX=30. Los parametros fueron los mismos para los dos grupos de experimentos (con/sin aumento).La autmentación generó 2000 muestras. 
 #RESULTADOS#:
El acc para experimentos con aumentación es clarament superior, incluso es superior a todos los intentos de clasificación con los modelos tradicionales. 
madelon_original     0.749939
madelon_synthetic    0.827800
Incremento porcentual en la precisión del 10.4%
No hay variación sustancial en la velocidad de convergencia del algoritmo al máximo. 
/home/sebacastillo/ealab/expga1/R3_report_feature_gisette_madelon.ipynb
/home/sebacastillo/ealab/expga1/R5_report_feature_analyze copy.ipynb
'''

### Experimento 4: GCM
'''
Comparación original vs aumentado
0026 Original
0037
0038
0039
0040
El experimento que tomé como base fue el 0026, sobre dataset original, con la siguiente configuracion: PROB_MUT = 1/IND_SIZE(0.00006); PX = 0.75, cromosoma activo es p=0.1, GMAX=20.
Le siguieron la serie 27,28,29,30 que se informa más abajo, con acc mucho mas bajos que el 26.
-----
Foco en la generación de mas datos de entrenamiento y teteo. 
0027 Aumentado
0028 
0029
0030
0027: p = 0.1, 200 muestras sintéticas
0028: p = 0.1, 1400 muestras sintéticas
0029: p= 0.01,  1400 muestras sintéticas
0030: p= 0.01,  1400 muestras sintéticas, testeo (función de fitness) en la partición original.

Peores resultados con aumentación experimentos 27, 28, 29: entiendo que el motivo está en que el AG opera con un dataset que combina datos originales y sintéticos, siendo que los datos sintéticos no son de buena calidad (está trabajando con dos distribuciones de probabilidades disímiles). El 30 tiene un acc que encima de 0.4, que es el doble de los anteriores, la única diferencia importante es que en 30 se está trabajando con la partición original de testeo. 
Pero 0030: p= 0.01, 1400 muestras sintéticas, los resultados lograr acc mucho mayor que los otros tres. La particularidad del 30 es que el GA entrena con datos mezclados pero el testeo se realiza sobre la partición original de testeo. 
------

0031
0032
0033
Luego 31,32,33 fueron experimentos individuales, que al igual que el 30 entrenan con la unión de datos originales y sintéticos, y testean en la partición original de test. Se generan 1400 datos sintéticos. La particularidad del grupo es que adopto la estrategia de testear en partición original y pruebo en el experimento 32 un cromosoma construido con ind p=0.3 (5000 caracteríristicas aprox), pero sin mejores resultados que el experimento de referencia. 

----
0034
0035
0036
Este grupo de experimento centré la atención en la Prob.Mutacion. Para 34 y 35 fije PROB_MUT = 16, mientras que para 0036 160. (los demás parámetros: gen activos p=0.1, PX =0.75, muestras sinteticas 1400) Este grupo de experimentos registra los mejores resultados. Entrené con la partición original de train, y con todos los datos sintéticos. 
----

#Nueva arquitectura para GCM
Se realizó un experimento de clasificación con un MLP entrenado con los datos sintéticos generados por el VAE.
La precisión del modelo es de 99.52%, demostrando las distribuciones de probabilidad de las clases están bien definidas y separadas.
Sin embargo cuando se prueba este MLP con los datos reales la performance cae a 0.5, lo que indica que el modelo no es capaz de generalizar a datos reales. 
Esto está señalando que el VAE no está generando datos que sean representativos de los datos reales, y por lo tanto el MLP no puede generalizar a estos datos. 
Posiblemente esta circunstancia indica que la divergencia KL tiene un error menor que el error de reconstrucción, lo que hace que el VAE no sea capaz de generar datos que sean representativos de los datos reales, pero sí es capaz de generar un espacio latente que es capaz de separar las clases de los datos sintéticos. 
Para mejorar la calidad de los datos sintéticos generados por el VAE, se propone una nueva arquitectura que incluye la integración de pesos de clase en la función de pérdida personalizada, y la inclusión de un muestreador aleatorio ponderado en el cargador de datos.
Asismismo se usará una subselección de características para mejorar la calidad de los datos sintéticos generados por el VAE. Una estrategia de tipo 'stacking' donde un flujo con un CAVE, así: GA-CAV-GA,
'''
