# Modelos clásicos aplicados al espacio completo de características {#sec-Capitulo2}

En este capítulo revisamos el desempeño de *algoritmos clásicos* en la solución de los problemas elegidos para nuestra investigación, tomando como campo de búsqueda el espacio completo de características. El objetivo de esta exploración es doble: por un lado contar con métricas de base para comparar el desempeño de nuestras soluciones, y por el otro, estudiar las características de los datasets elegidos para nuestro estudio, procurando identificar aquellos rasgos que puedan influir en el desempeño de los modelos. Particularmente, nos centramos en la alta dimensionalidad y la escasez muestral, el desbalance de clases y el ruido.

## Datos elegidos en nuestro estudio

El conjunto de datos elegidos en este trabajo incluye cinco datasets: *Madelon*, *Gisette*, *Leukemia*, *GCM* y *All Leukemia*. El último de ellos, *All Leukemia*, es un dataset introducido en este trabajo iniciada la etapa de experimentación de integración entre AG y AV. Por esa razón, no está incluido en la revisión de desempeño de los algoritmos clásicos que presentaremos en este capítulo. No obstante ello, se incluye en el detalle de los datasets para que el lector tenga una idea de la variedad de problemas que se tuvieron en cuenta para validar los resultados de nuestra propuesta.

Como veremos a continuación, cada dataset plantea desafíos distintos en términos de aprendizaje, y posee distintos niveles de complejidad en su composición. El dataset *Madelon* es un conjunto artificial de datos con 2000 observaciones y 500 características (2000x500), donde el objetivo es resolver un problema XOR multidimensional con 5 características relevantes y 15 características corresponden a combinaciones lineales de aquellas (i.e. 15 características redundantes). Las otras 480 características fueron generadas aleatoriamente (no tienen poder predictivo). Madelon es un problema de clasificación de dos clases con variables de entrada binarias dispersas. Las dos clases están equilibradas, y los datos se dividen en conjuntos de entrenamiento y prueba. Fue creado para el desafío de Selección de Características [NIPS_2003](http://clopinet.com/isabelle/Projects/NIPS2003/), y está disponible en el Repositorio [UCI](https://archive.ics.uci.edu/dataset/171/madelon). Los datos están divididos en un conjunto de entrenamiento y un conjunto de testeo. 

Como es fácil de advertir, este es un problema donde la información relevante está presente junto a información redundante y otra sin valor predictivo. Es decir, existe un alto nivel de ruido en los datos, planteando importantes desafíos para los algoritmos de aprendizaje, y uno particularmente interesante para el problema de selección de características. Respecto de las dimensiones del problema, no se estaría en una situación crítica de alta dimensionalidad y escasez muestral, ya que el número de observaciones es mayor que el de características. Sin perjuicio de ello, aún queda por determinar si la cantidad de patrones disponibles es suficiente para que los algoritmos de aprendizaje puedan encontrar una solución en un contexto tan ruidoso.

El dataset *Gisette* es un dataset creado para trabajar el problema de reconocimiento de dígitos escritos a mano [@isabelleguyonGisette2004]. Este conjunto de datos forma parte de los cinco conjuntos utilizados en el desafío de selección de características NIPS 2003. Tiene 13500 observaciones y 5000 atributos (13500x5000). El desafío radica en diferenciar los dígitos '4' y '9', que suelen ser fácilmente confundibles entre sí. Los dígitos han sido normalizados en tamaño y centrados en una imagen fija de 28x28 píxeles. Además, se crearon nuevas características como combinación de las existentes para construir un espacio de mayor dimensión. También se añadieron características distractoras denominadas "sondas", que no tienen poder predictivo. El orden de las características y patrones fue aleatorizado. Los datos están divididos en un conjunto de entrenamiento y un conjunto de testeo.

En este caso, nos encontramos en un escenario similar al de *Madelon*, con un dataset ruidoso e información redundante. La particularidad de *Gisette* es que, pese a mantener una relación positiva entre obervaciones y características (las primeras son más que las segundas), posee un espacio de búsqueda sensiblemente más grande y, eventualmente, más complejo que el de *Madelon*. Por esa razón, esperamos que este dataset sea computacionalmente más exigente que el anterior.

El dataset *Leukemia* es un análisis de datos de expresión genética obtenidos de microarreglos de ADN, se estudia en Golub [-@golubMolecularClassificationCancer1999] para la clasificación de tipos de cáncer. Se construyó un conjunto de datos con 72 observaciones y 7129 mediciones (72x7129) de las clases ALL (leucemia linfocítica aguda) y AML (leucemia mielogénica aguda). El problema es distinguir entre estas dos variantes de leucemia (ALL y AML). Los datos se dividen originalmente en dos subconjuntos: un conjunto de entrenamiento de 38 observaciones y un conjunto de testeo de 34 observaciones.

Con este dataset nos encontramos, precisamente, en el escenario de alta dimensionalidad y escasez muestral. El número de observaciones es menor que el de características, y las dimensiones del problema son significativamente más altas que en los casos anteriores. Además, el dataset está desbalanceado, con 27 observaciones de la clase ALL y 11 de la clase AML en la partición de entrenamiento, lo que plantea un desafío adicional para los algoritmos de aprendizaje.

El dataset *All Leukemia* es un estudio de pacientes pediátricos con leucemia linfoblástica aguda (LLA). Incluye 327 muestras con información de 12600 genes (327x12600). Está compuesto por 14 clases desequilibradas. Fue compilado por Yeoh et al., en [link](https://www.cell.com/cancer-cell/fulltext/S1535-6108(02)00032-6?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS1535610802000326%3Fshowall%3Dtrue). 

Este caso es, sin duda, uno de los más complejos de todos, y plantea un desafío computacional significativo. El número de observaciones es significativamente menor que el de características, y las dimensiones del problema son significativamente más altas que en los casos anteriores. Además, el dataset está desbalanceado, con multiples clases desequilibradas, planteando no solo el desafío de las diversas fronteras de decisión que se deben encontrar, sino también el problema de posibles ambigüedades entre clases. Como vimos en el capítulo anterior, el desafío de procesar información genética es significativo considerando la alta correlación entre genes, y la gran cantidad de información redundante.

Finalmente, el dataset *GCM* fue compilado en Ramaswamy [-@ramaswamyMulticlassCancerDiagnosis2001] y contiene los perfiles de expresión de muestras de tumores que representan 14 clases comunes de cáncer humano. El dataset está compuesto por 190 muestras y 16063 atributos (biomarcadores), distribuidos en clases desequilibradas. Los datos están divididos en un conjunto de entrenamiento y un conjunto de testeo.

En GCM, como en el caso de *All Leukemia*, nos encontramos en un escenario de alta dimensionalidad y escasez muestral. El número de observaciones es significativamente menor que el de características, y las dimensiones del problema son significativamente más altas que en todos los casos anteriores. Además, el dataset está desbalanceado, eventualmente contiene abundante información ruidosa y redundante.

Entendemos que esta variedad de datasets cubre un amplio espectro de problemas de aprendizaje, y que los resultados obtenidos en cada uno de ellos nos permitirán evaluar el desempeño de nuestros algoritmos en contextos distintos.

## El desempeño de algoritmos clásicos

Pasando a la evaluación de los modelos clásicos, hemos seleccionado una serie de modelos ampliamente usados en el campo del aprendizaje automático para tomar su desempeño como indicador. Entre ellos, encontramos: *modelos lineales*, *modelos basados en árboles*, *modelos de Naive Bayes*, *modelos de vecinos más cercanos*, *modelos de redes neuronales* y *modelos de Máquinas de Soporte Vectorial*. Cuando fue posible dada la naturaleza y características de los datasets, hemos evaluado todos los modelos. En caso contrario, hemos seleccionado aquellos modelos más apropiados para el contexto, dejando de lado los que no resultaban adecuados para el problema (como es el caso de GCM y Gisette, dada la dimensionalidad de ambos, y la naturaleza multiclase del primero). Finalmente, a fin de estandarizar la implementación de estos algoritmos, hemos empleado la librería `scikit-learn` que provee abstracciones convenientes para nuestro entorno de experimentación.

Los modelos lineales se basan en la premisa de que la variable objetivo puede expresarse como una función lineal de los predictores o características. Normalmente asumen que, para cada observación, la suma ponderada de las características (potencialmente con un término independiente o “bias”) determina la respuesta. Entre los modelos lineales que incluimos en nuestro estudio se encuentran: el Análisis Discriminante Lineal (LDA), el Análisis Discriminante Cuadrático (QDA), la Regresión de Cresta (Ridge), y el Descenso de Gradiente Estocástico (SGD) [@hastieElementStatisticalLearning2009]. El Análisis Discriminante Lineal (LDA) asume que cada clase proviene de una distribución normal multivariada con igual matriz de covarianzas y distinta media, y busca el hiperplano que maximiza la separabilidad entre clases proyectando los datos a un subespacio. El Análisis Discriminante Cuadrático (QDA) relaja el supuesto de matriz de covarianzas compartida, permitiendo que cada clase tenga su propia matriz y mejorando así la capacidad de modelar fronteras de decisión más complejas, aunque con un mayor riesgo de sobreajuste cuando se cuenta con poca muestra. La Regresión de Cresta (Ridge) introduce una penalización L2 sobre los coeficientes para controlar la varianza de la solución y mitigar la multicolinealidad, lo cual es especialmente útil si existe una gran correlación entre características. El Descenso de Gradiente Estocástico (SGD), por su parte, consiste en un procedimiento incremental de optimización que actualiza los parámetros de un modelo lineal luego de cada observación (o mini-batch), resultando muy eficiente en problemas de alta dimensión o grandes volúmenes de datos.

En el caso de los modelos basados en árboles, todos comparten la idea de ir dividiendo recursivamente el espacio de características en regiones homogéneas. Este proceso se materializa en un árbol de decisión que, en cada nodo, escoge un umbral o criterio de partición para una característica. Los modelos incluidos en nuestro estudio son: Arbol de decisión clásico (DTC), AdaBoost, Bagging, Extra Trees Ensemble, Gradient Boosting, Random Forest, ETC, y Arboles Extremadamente Aleatorizados (ETC) [@hastieElementStatisticalLearning2009]. El Árbol de Decisión Clásico (Decision Tree Classifier, DTC) utiliza criterios como la ganancia de información o la reducción de la impureza para decidir la partición óptima en cada nivel, siendo fácilmente interpretable aunque con tendencia al sobreajuste si no se regula su profundidad. Algunas variantes se basan en la combinación de múltiples árboles. Bagging, por ejemplo, entrena árboles independientes a partir de muestras “bootstrap” y agrega las predicciones para reducir la varianza del modelo. Random Forest amplía esta idea, incorporando además la selección aleatoria de características en cada división, con lo cual reduce la correlación entre árboles y mejora la capacidad generalizadora. Extra Trees Ensemble adopta una estrategia aún más aleatoria, ya que define umbrales de corte aleatorios, lo que tiende a una mayor diversidad entre árboles y puede favorecer la reducción de la varianza. AdaBoost, en contraposición, entrena secuencialmente modelos débiles (a menudo árboles de baja profundidad) poniendo más peso en las observaciones mal clasificadas en iteraciones previas, de modo que cada nuevo modelo aprenda de los errores acumulados. Gradient Boosting también combina modelos débiles, pero en su caso cada etapa del entrenamiento se orienta a predecir el error residual del ensamble previo, optimizando una función de costo de forma aditiva y generalmente logrando modelos muy potentes. Cuando se habla de ETC (Extremely Randomized Trees Classifier), se hace referencia a un enfoque similar a Random Forest, pero que enfatiza la aleatorización tanto en la selección de subconjuntos de características como en los umbrales de partición, mejorando la diversidad de los árboles y mitigando así la varianza global.

Los modelos de Naive Bayes se basan en el teorema de Bayes para predecir la probabilidad de pertenencia a cada clase, asumiendo independencia condicional de las características [@hastieElementStatisticalLearning2009]. Aun cuando esta suposición rara vez se cumple por completo en problemas reales, la simplicidad computacional y la eficacia empírica suelen convertirlos en una elección sólida, especialmente en problemas de alta dimensión. En su versión Bernoulli (BNB), se asume que las variables predictoras son binarias, lo que se ajusta bien a datos dispersos donde cada característica indica la presencia o ausencia de cierta propiedad. En la versión Gaussiana (GNB), se asume que cada característica sigue una distribución normal, estimando media y varianza por clase para luego combinar esas estimaciones en la regla de decisión bayesiana.

En cuanto a los métodos basados en vecinos más cercanos, el clasificador K-Vecinos Más Cercanos (KNN) ejemplifica la estrategia de aprendizaje por vecindad, ya que no construye un modelo explícito durante la etapa de entrenamiento. En su lugar, para clasificar una nueva observación, identifica los K vecinos más cercanos en el espacio de características y asigna la clase mayoritaria de ese entorno local [@hastieElementStatisticalLearning2009]. Este enfoque es intuitivo y puede capturar relaciones complejas en los datos, aunque su desempeño se degrada en alta dimensión y requiere un costo computacional alto en predicción, pues debe calcular distancias a todos los puntos de entrenamiento.

Entre los modelos de redes neuronales, el Perceptrón Multicapa (MLP) es una arquitectura de red con múltiples capas densamente conectadas y funciones de activación no lineales [@hastieElementStatisticalLearning2009]. Su capacidad de aproximar funciones complejas lo convierte en un modelo flexible, pero también más exigente en términos de datos y calibración de hiperparámetros. Aunque en su versión más simple puede considerarse un “clásico”, el MLP con técnicas de regularización y optimización robustas forma parte fundamental de las estrategias de aprendizaje profundo.

Por último, las Máquinas de Soporte Vectorial (SVM) parten del principio de encontrar un hiperplano (en el caso lineal) u “frontera” (en el caso con núcleos no lineales) que maximice el margen de separación entre clases [@hastieElementStatisticalLearning2009]. En contextos de alta dimensión y con un número moderado de muestras, las SVM suelen mostrar un desempeño notable por su capacidad para controlar el sobreajuste mediante el parámetro de regularización y el uso de núcleos apropiados. Su versión lineal (LSVC) se centra en resolver una optimización con un límite que separa las clases en un espacio original de altas dimensiones sin necesidad de mapeos adicionales, resultando eficiente en muchos casos de datos dispersos. La variante NuSVC introduce un parámetro $nu$ que controla tanto el número de vectores de soporte como la proporción máxima de errores permitidos, proporcionando una forma alternativa de regularización y definición de la frontera de decisión. Estas particularidades hacen que las SVM sean especialmente populares en problemas donde la dimensionalidad de las características es grande con respecto al número de muestras disponibles.

### Configuración de los Modelos

Para evaluar el desempeño de los modelos clásicos y determinar la configuración más adecuada de sus hiperparámetros, hemos implementado un proceso de búsqueda sistemática en grilla (*Grid Search*). Este procedimiento consiste en seleccionar un conjunto de valores relevantes para cada hiperparámetro e iterar sobre todas las combinaciones posibles, entrenando y validando el modelo en cada caso.

En particular, establecimos un rango de parámetros numéricos que incluye de 3 a 20 valores, dependiendo de la sensibilidad del modelo a cada hiperparámetro y de los límites sugeridos en la literatura. Por ejemplo, en modelos lineales como Ridge o SGD, la regularización (*penalty*) y la tasa de aprendizaje (*learning rate*) se probaron en al menos tres niveles para capturar comportamientos distintos. En cambio, en algoritmos basados en árboles (como Random Forest o Gradient Boosting), ampliamos el rango hasta 20 valores en parámetros críticos (número de árboles, profundidad máxima, etc.) para reflejar la diversidad de configuraciones posibles.

Para los parámetros no numéricos (como funciones de activación en MLP, criterios de partición en árboles, tipo de kernel en SVM, entre otros) se utilizaron configuraciones estándar y reconocidas por la comunidad, con el fin de reducir la complejidad combinatoria. Aun así, para cada modelo se revisó la documentación de `scikit-learn` y la literatura relacionada, asegurando una cobertura apropiada de las variantes más importantes.

La métrica de evaluación que hemos seleccionado es **AUC** para los dataset de clasificación binaria (Madelon, Gisette y Leukemia), y **F1 score** para los dataset de clasificación multiclase (GCM). 

La métrica AUC mide el área bajo la curva ROC, donde la curva ROC traza la Tasa de Verdaderos Positivos (TPR o Sensibilidad) frente a la Tasa de Falsos Positivos (FPR) a distintos umbrales de decisión.  Matemáticamente, la AUC puede interpretarse (bajo ciertas condiciones) como la probabilidad de que el clasificador asigne una puntuación más alta a una muestra positiva que a una negativa. Un valor de AUC igual a 1 indica un modelo perfecto, mientras que un valor cercano a 0.5 sugiere un desempeño cercano al de un clasificador aleatorio.  

En *clasificación binaria*, la AUC ofrece un panorama amplio del desempeño del modelo al no depender de un umbral específico y tiene en cuenta la relación entre verdaderos positivos y falsos positivos. Esto es especialmente relevante en datasets como Madelon, Gisette o Leukemia, donde el desbalance y la alta dimensionalidad pueden sesgar otras métricas.  

La métrica F1 se define como la media armónica entre la Precisión (exactitud) y el Recall (sensibilidad):  F1 = 2 × (Precisión × Recall) / (Precisión + Recall).  La Precisión indica qué proporción de las predicciones positivas son realmente positivas, mientras que el Recall mide la proporción de positivos correctamente identificados respecto de todos los positivos reales. La F1 combina ambas, penalizando los modelos que desequilibran excesivamente la Precisión y el Recall. En problemas multiclase (por ejemplo, en GCM con 14 clases), la F1 puede calcularse por clase y luego promediarse (macro-F1) para evaluar la capacidad de cada modelo de reconocer de forma equilibrada todas las clases, incluso cuando existe desbalance.

En *clasificación multiclase*, la F1 proporciona una forma de resumir la Precisión y el Recall cuando hay varias clases involucradas y potencialmente desbalanceadas. La F1 balancea correctamente los falsos positivos y los falsos negativos para cada clase antes de promediar, lo que resulta más informativo para problemas con muchas clases desiguales (como GCM).

Las particiones originales de los datasets fueron concatenadas en un solo conjunto de datos, y luego se dividió en conjuntos de entrenamiento y testeo en proporción 80/20. 

## Resultados obtenidos

En la siguiente tabla resumimos los resultados en el dataset de testeo para cada modelo y dataset, con las métricas AUC y F1 score.  


| Modelo           | Leukemia Test    | Madelon Test   | Gisette Test      | GCM Test       |
|------------------|------------------|----------------|-------------------|----------------|
| LDA              | 0.85            | 0.60           | 0.96              | -              |
| QDA              | 0.50            | 0.66           | 0.70              | -              |
| Ridge            | 0.99            | 0.60           | 0.97              | -              |
| SGD              | 0.98            | 0.64           | 0.99              | 0.71           |
|------------------|------------------|----------------|-------------------|----------------|
| AdaBoost         | 0.91            | 0.84           | 0.99              | -              |
| Bagging          | **1.00**        | **0.91**       | -                 | -              |
| DTC              | 0.72            | 0.64           | 0.92              | 0.53           |
| ETC              | 0.54            | 0.57           | 0.94              | 0.48           |
| Ext.Trees.Ens.   | **1.00**        | 0.71           | 0.99              | 0.57           |
| Gradient Boost.  | 0.99            | 0.82           | **1.00**          | 0.58           |
| Random Forest    | **1.00**        | 0.78           | 0.99              | 0.62           |
|------------------|------------------|----------------|-------------------|----------------|
| LSVC             | 0.99            | 0.62           | 0.99              | 0.62           |
| NuSVC            | **1.00**        | 0.61           | 0.99              | 0.58           |
| SVC              | **1.00**        | 0.61           | 0.99              | 0.58           |
|------------------|------------------|----------------|-------------------|----------------|
| BNB              | 0.89            | 0.63           | 0.94              | -              |
| GNB              | 0.91            | 0.65           | 0.85              | -              |
|------------------|------------------|----------------|-------------------|----------------|
| KNN              | 0.86            | 0.65           | 0.99              | -              |
|------------------|------------------|----------------|-------------------|----------------|
| MLP              | 0.96            | 0.58           | 0.99              | **0.68**       |


El gráfico completo de resultados en las particiones de entrenamiento y testeo puede verse en la siguiente figura:  

![Desempeño de algoritmos clásicos](model_performance_heatmap_grouped.png)

Las métricas presentadas evidencian que los resultados de los modelos clásicos varían ampliamente según las características de cada dataset. En particular, los conjuntos de datos **Leukemia** y **Gisette** muestran, en términos generales, un desempeño sólido para la mayoría de los algoritmos analizados. La **AUC** alcanzada por varios modelos en *Leukemia* (en algunos casos 1.00) ilustra la capacidad de separar correctamente los casos, a pesar de tratarse de un problema con alta dimensionalidad y desbalance. Asimismo, *Gisette* se beneficia del hecho de contar con más observaciones que *features*, lo que facilita la labor de los clasificadores (algunos, como Gradient Boosting, alcanzan AUC = 1.00).

En contraste, **Madelon** y, muy especialmente, **GCM**, exhiben una dificultad sustancialmente mayor para casi todas las familias de modelos clásicos. En *Madelon*, la presencia de ruido y características irrelevantes afecta la capacidad de generalización, quedando reflejado en valores de AUC relativamente bajos (en general por debajo de 0.70). Esta situación concuerda con la naturaleza artificial de *Madelon*, donde únicamente un subconjunto pequeño de atributos tiene poder predictivo y los modelos se enfrentan a espacios de búsqueda con muchos distractores. Este resultado subraya la relevancia de algoritmos que incorporen estrategias que mitiguen el impacto del ruido en el aprendizaje o mecanismos de selección de características que permitan descartar las irrelevantes.

El caso de **GCM** es aún más crítico. Se trata de un problema multiclase severamente desbalanceado y con una dimensionalidad desproporcionadamente alta en comparación con el número de muestras disponibles. Dichas condiciones propician que todos los modelos clásicos evidencien dificultades para capturar las fronteras de decisión y generalizar correctamente. El hecho de que el **MLP** sea, en este caso, el método de mayor desempeño (si bien con un F1 todavía moderado) puede atribuirse a la flexibilidad de las redes neuronales y su capacidad de aproximar funciones complejas, a pesar del reducido número de muestras de entrenamiento. No obstante, la mayoría de clasificadores enfrenta limitaciones para generalizar en este escenario de desequilibrio tan marcado, reforzando la hipótesis central de la tesis sobre la importancia de enfoques generativos para rebalancear la asimetría entre clases y mejorar la clasificación.

Los resultados que el MLP obtiene por clase se muestran en la siguiente tabla:

| **Class**        | **Precision** | **Recall** | **F1-Score** | **Support** |
|------------------|--------------:|-----------:|------------:|------------:|
| Bladder          |         0.75  |       1.00 |       0.857 |           3 |
| Breast           |         0.00  |       0.00 |       0.000 |           3 |
| CNS              |         0.80  |       1.00 |       0.889 |           4 |
| Colorectal       |         0.75  |       1.00 |       0.857 |           3 |
| Leukemia         |         1.00  |       0.83 |       0.909 |           6 |
| Lung             |         1.00  |       0.33 |       0.500 |           3 |
| Lymphoma         |         0.83  |       0.83 |       0.833 |           6 |
| Melanoma         |         1.00  |       0.50 |       0.667 |           2 |
| Mesothelioma     |         1.00  |       1.00 |       1.000 |           3 |
| Ovary            |         0.50  |       0.67 |       0.571 |           3 |
| Pancreas         |         0.50  |       0.67 |       0.571 |           3 |
| Prostate         |         0.00  |       0.00 |       0.000 |           2 |
| Renal            |         1.00  |       0.33 |       0.500 |           3 |
| Uterus__Adeno    |         0.40  |       1.00 |       0.571 |           2 |
| **macro avg**    |         0.68  |       0.65 |       0.62  |          46 |
| **weighted avg** |         0.73  |       0.70 |       0.68  |          46 |
| **accuracy**     |              |            |       0.70  |          46 |

Obsérvese que ciertas clases con muy pocas muestras (por ejemplo, *Breast* o *Prostate*) presentan métricas nulas, mientras que otras con más muestras, como *Leukemia* o *Lymphoma*, muestran resultados más consistentes. La presencia de clases con tan baja representatividad hace que el modelo no cuente con suficiente evidencia estadística para aprender patrones característicos, conduciendo a predicciones erróneas (o directamente inexistentes) para dichas clases. Como anticipamos en el capítulo anterior, en un escenario multiclase desbalanceado, puede ser común que las clases minoritarias obtengan poco o ningún peso en la función de pérdida, lo que lleva al modelo a ignorarlas o predecir muy pocas (o ninguna) instancias de dichas clases. 

En consecuencia, en problemas donde la alta dimensionalidad y bajo número de muestras se combina con múltiples clases desequilibradas, aun los modelos con capacidad de generalización elevada (por ejemplo, redes neuronales) se ven limitados. La disparidad de resultados por clase que hemos reportado (desde métricas perfectas hasta valores nulos) ilustra la necesidad de una estrategia que aborde el desbalance de manera explícita, ya sea mediante incremento sintético de datos, la implementación de funciones de pérdida adaptativas (que penalicen más los errores en las clases minoritarias), o ambos. 

En síntesis, la combinación de alta dimensionalidad, escasez muestral y desbalance de clases constituye un serio obstáculo al entrenamiento estable y robusto de modelos clásicos, tal como se constata en *GCM*. Por otro lado, incluso en problemas más sencillos como *Madelon*, el ruido y la alta proporción de atributos irrelevantes pueden perjudicar significativamente la performance de estos algoritmos. Estos hallazgos abonan la necesidad e importancia de la estrategia que exploraremos en este trabajo, que se centra en la generación de datos sintéticos para incrementar la muestra de entrenamiento y mejorar la capacidad de generalización de los modelos. 

