
<!-- 
Elementos del capítulo 1

1) Selección de características Fisher (Fisher, The Experimental Design, cap.VI, The factorial design in experimentation, 37 The Single factor): 
"In expositions of the scientific use of experimentation it is frequent to find an excessive stress laid on the importance of varying the essential conditions only one at a time. The experimenter interested in the causes which contribute to a certain effect is supposed, by a process of abstraction, to isolate these causes into a number of elementary ingredients, or factors, and it is often supposed, at least for purposes of exposition, that to establish controlled conditions in which all of these factors except one can be held constant, and then to study the effects of this single factor, is the essentially scientific approach to an experimental investigation. This ideal doctrine seems to be more nearly related to expositions of elementary physical theory than to laboratory practice in any branch of research. In experiments merely designed to illustrate or demonstrate simple laws, connecting cause and effect, the relationships ofi which with the laws relating to other causes are already known, it provides a means by which the student may apprehend the relationship, with which he is to familiarise himself, in as simple a manner as possible. By contrast, in the state of knowledge or ignorance in which genuine research, intended to advance knowledge, has to be carried on, this simple formula is not very helpful. We are usually ignorant which, out of innumerable possible factors, may prove ultimately to be the most important, though we may have strong presuppositions that some few of them are particularly worthy of study. We have usually no knowledge that any one factor will exert its effects independently of all others that can be varied, or that its effects are particularly simply related to variations in these other factors. On the contrary, if single factors are chosen for investigation, it is not because we anticipate that the laws of nature can be expressed with any particular simplicity in terms of these variables, but because they are variables which can be controlled or measured with comparative ease. If the investigator, in these circumstances, confines his attention to any single factor, we may infer either that he is the unfortunate victim of a doctrinaire theory as to how experimentation should proceed, or that the time, material, or equipment at his disposal are too limited to allow him to give attention to more than one narrow aspect of his problem.
The modifications possible to any complicated apparatus, machine, or industrial process must always be considered as potentially interacting with one another, and must be judged by the probable effects of such interactions. If they have to be tested one at a time this is not because to do so is an ideal scientific procedure, but because to test them simultaneously  would sometimes be too troublesome, or too costly."


2) Selección de características Vignolo-Gerard: 
In any case, the purpose is the improvement of a machine learning model, either in terms of learning speed, computational complexity, simplicity/interpretability of the representation or generalization capability. 
The existing FS techniques can be categorized according to three aspects [2]: the search strategy; the evaluation criterion; and the methodology for the assessment of the performance. The first corresponds to the search strategy which is applied to select candidate solutions among the possible feature combinations. The last two refer to the methods and measures used to evaluate each feature subset, and based on this the FS strategies can be divided in two categories: wrapper methods and filter methods. Wrappers have the advantage that the  predictive performance of the selected subset is correlated with the relevance measure. The problem for wrapper methods is that the predictive performance has to be assessed (a classifier has to be trained) for each feature subset tested, being therefore impossible to evaluate all the possible combinations of features. This issue can be overcome using search heuristics like genetic algorithms (GA), particle swarm (PSO) and ant colony optimization (ACO), which are able to find adequate solutions without exploring the entire search space [3]. However, while ACO and PSO typically use encoding based on real numbers, GAs use binary codification, reason why these are more suitable for feature selection problems.


3) Selección de características Verónica Bolón-Canedo, Noelia Sánchez-Maroño, Amparo Alonso-Betanzos (Feature Selection for High-Dimensional Data, Sringer 2015): 
"
0. The Need for Feature Selection  
In recent years, most enterprises and organizations have stored large amounts of data in a systematic way, without any clear potential use. In addition, with the rapid growth of the Internet, lots of data come in different formats—text, multimedia, etc.—from many different sources—systems, sensors, mobile devices, etc.—and they require new tools in order to be analyzed and processed so as to enable the extraction of useful information. Most of these data have been generated in the last few years while we continue to create a quintillion bytes daily [1]. Therefore, big data of large volume and ultrahigh dimensionality have emerged in various machine learning applications, such as text mining and information retrieval [2]. For instance, Weinberger et al. [3] have studied a collaborative email/spam filtering task with 16 trillion unique features. These datasets have brought an interesting challenge to the research community, and, citing [4], as researchers, “our task is to find a needle in a haystack, teasing the relevant information out of a vast pile of glut.” The ultrahigh dimensionality not only incurs unbearable memory requirements and high computational cost in training, but also deteriorates the generalization ability because of the “curse of dimensionality” issue. According to [4], the colorful phrase the “curse of dimensionality” was apparently coined by Richard Bellman in [5], in connection with the difficulty of optimization by exhaustive enumeration on product spaces. This phenomena arises when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings. Any dataset is usually represented by a matrix where the rows are the instances or samples that have been recorded and the columns are the attributes or features that are required to represent the problem at hand. Therefore, to tackle the “curse of dimensionality” problem, the dataset can be summarized by finding “narrower” matrices that in some sense are close to the original. These narrow matrices have only a small number of samples and/or a small number of attributes, and therefore can be used much more efficiently than the original large matrix can. The process of finding these narrow matrices is called dimensionality reduction.  Among the dimensionality reduction techniques, feature extraction addresses the problem of finding the most compact and informative set of features of a given problem, to improve the efficiency or data storage or processing. The problem of feature extraction is decomposed into two steps: feature construction and feature selection. Feature construction methods complement the human expertise to convert “raw” data into a set of useful features. It may be considered a preprocessing transformation that may include standardization, normalization, discretization, signal enhancement, extraction of local features, etc. Some construction methods do not alter the space dimensionality, while others enlarge it, reduce it or can act in either direction. But one should be aware of not losing information at the feature construction stage. 1.2 When Features Are Born 3  Guyon and Elisseeff [6] argued that it is always better to err on the side of being too inclusive rather than risking discarding useful information. Adding many features seems reasonable but it comes at a price: it increases the dimensionality of the patterns and thereby immerses the relevant information into a sea of possibly irrelevant, noisy or redundant features. Feature selection is the process in which the number of initial features is reduced and a subset of those that retain enough information for obtaining good, or even better, performance results is selected. Given the growing number of high-dimensional datasets, feature selection has gained increasing interest in the field of machine learning, addressing many of its disciplines, such as clustering [7, 8], regression [9, 10] and classification [11, 12], in both supervised and unsupervised ways. This book would be too long if all of them were to be addressed, which is why we have focused on the application of feature selection in supervised classification problems.

1. Intrinsic Characteristics of Data
1.1 Small Sample Size  The small sample size problem is when “only m samples are available in an ndimensional vector space with m < n,” citing Fukunaga [20]. This author says that this problem is often encountered in pattern recognition, particularly when n is very large. Nowadays, high dimension, low sample size (HDLSS) data are becoming increasingly common in various fields. These include the genetic microarrays previously presented, chemometrics, medical imaging, text recognition, face recognition, finance, and so on. Many of the features of these problems do not facilitate an adequate classification; rather they hinder it. To avoid this problem, the importance of feature selection has been stressed and several methods of feature selection for HDLSS data have been proposed in the literature..
1.2 Class Imbalance  Data are said to suffer the Class Imbalance Problem when the class distributions are highly imbalanced. This occurs when a dataset is dominated by a major class or by classes which have significantly more instances than the other rare or minority classes in the data. For the two-class case, without loss of generality, one assumes that the minority or rare class is the positive class, and the majority class is the negative class. Often the minority class is very infrequent, such as 1% of the dataset. If one applies the most traditional classifiers on the dataset, they are likely to predict everything as negative (the majority class) [25]. However, typically, people have more interest in learning about rare classes. For example, applications such as medical diagnosis prediction of rare but important disease, such as cancer, where it is common to have fewer cancer patients than healthy patients. Similar situations are observed in other areas, such as detecting fraud in banking operations, detecting network intrusions, managing risk, predicting technical equipment failure, e-mail foldering and face recognition [26].
1.3 Data Complexity  According to [35], difficulties in classification can be traced to three sources:  1. Class ambiguity refers to a situation where there are cases in a classification problem whose classes cannot be distinguished by any classification algorithm using the given features. Classes can be ambiguous for two reasons: a) the class concepts are poorly defined and, so, intrinsically inseparable, and b) the features chosen to represent the different samples are not sufficient for indicating differences between instances. An example of the former would be the lowercase letter “l” and the numeral “1”, which are the same in many fonts. 2. Boundary complexity: a class boundary is complex if it requires a long description, possibly including a listing of all the points together with their class labels. This aspect of difficulty comes from the nature of the problem, and is unrelated to the sampling process. It exists even if a complete sample is given and if the classes are well defined. 3. Sample sparsity: small sample size and dimensionality-induced sparsity introduce another layer of difficulty through a lack of constraints of the generalization rules.
1.4 Dataset Shift
These datasets can suffer what is known as dataset shift, which is defined as “a challenging situation where the joint distribution of inputs and outputs differs between the training and test stages”
1.5 Noisy Data  It is almost inevitable that there is some noise in most of the collected data, except in the most structured and synthetic environments. This “imperfect data” can be due to many sources, for instance, faulty measuring devices, transcription errors, and transmission irregularities. However, the accuracy of a classifier may greatly depend on the quality of the data used during the training phase, so a classifier built from a noisy training set might be less accurate and less compact than one built from the noise-free version of the same dataset using an identical algorithm [40]. Imperfections in a dataset can be dealt with in four broad ways: 1) leave the noise in, 2) data cleaning, i.e., filter the noise out, 3) data transformation, i.e, correct the noise, and 4) data reduction, that is, reduce the amount of data by aggregating values or removing and clustering redundant attributes [41]. In the first approach, the dataset is taken as is, with the noisy instances left in place. Algorithms that make use of the data are designed to be robust; that is, they can tolerate a certain amount of noise in the data. One approach is to develop robust algorithms that allow for noise by avoiding overfitting the model to the data, for example, using pruning techniques in decision trees [42]. In the second approach, instances that are suspected of being noisy according to certain evaluation criteria are discarded. A classifier is then built using only the retained instances in a smaller but cleaner dataset. [43] explores four techniques intended for data cleaning to enhance data analysis in the presence of high noise levels. However, there are two weaknesses in using this approach. First, by eliminating the whole instance, potentially useful information could also be discarded, such as the uncorrupted feature values. Secondly, when there is a large amount of noise in the dataset, the amount of information in the remaining clean dataset may not be sufficient for building a good classifier. These drawbacks lead us to the third approach that attempts to correct the tuples, so the noisy instances are identified, but instead of tossing these instances out, they are repaired by replacing the corrupted values with more appropriate ones. These corrected instances are then reintroduced into the dataset. Regarding the fourth approach, dimensionality reduction—and consequently feature selection—is one of the most popular techniques to remove noisy (i.e., irrelevant) and redundant features. However, while feature selection has been the target of much research, very little study has been done to systematically address the issue of feature relevance in the presence of noisy data.
1.6 Outliers  Hawkins defines an outlier as “an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism” [44]. Outlier detection algorithms are useful in areas such as machine learning, data mining, pattern recognition, data cleansing and data warehousing [45]
1.7 Feature Cost  Test costs and misclassification costs are the two most important types of cost in cost-sensitive learning. The test cost is money, time, or other resources we pay for collecting a data item of an object. The misclassification cost is the penalty we receive after deciding that an object belongs to an incorrect class [47]. Feature selection works have mainly considered misclassification costs. However, most feature selection methods proposed in the literature do not cope with the test cost, although acquiring features in real-world applications may vary notably from one feature to another. For example, in medical applications, the feature acquisition can demand a clinical test that, in addition to its economics cost, poses a risk to the patient, or demand demographic data obtained from giving him a short questionnaire. In image texture analysis, the feature extraction process represents a significant computational cost that also depends on the method used [48].

1. Feature Selection  
Feature selection can be defined as the process of detecting the relevant features and discarding the irrelevant and redundant ones with the goal of obtaining a subset of features that describes properly the given problem with a minimum degradation of performance. It has several advantages [3], such as:  • Improving the performance of the machine learning algorithms. • Data understanding, gaining knowledge about the process and perhaps helping to visualize it. • General data reduction, limiting storage requirements and perhaps helping in reducing costs. • Feature set reduction, saving resources in the next round of data collection or during utilization. • Simplicity, possibility of using simpler models and gaining speed.  To all this, we will add that in the present scenario of Big Data analytics, feature selection plays a central role.
Feature Relevance  Intuitively, it can be determined that a feature is relevant if it contains some information about the target.

1. Feature Selection Methods  Feature selection methods can be divided according to two approaches: individual evaluation and subset evaluation [5]. Individual evaluation is also known as feature ranking and assesses individual features by assigning them weights according to their degrees of relevance. On the other hand, subset evaluation produces candidate feature subsets based on a certain search strategy. Each candidate subset is evaluated by a certain evaluation measure and compared with the previous best one with respect to this measure. While the individual evaluation is incapable of removing redundant features because redundant features are likely to have similar rankings, the subset evaluation approach can handle feature redundancy with feature relevance. 16 2 Foundations of Feature Selection  However, methods in this framework can suffer from an inevitable problem caused by searching through all feature subsets required in the subset generation step, and thus, both approaches are worth studying. Aside from this classification, three major approaches can be distinguished based upon the relationship between a feature selection algorithm and the inductive learning method used to infer a model [3]:  • Filters, which rely on the general characteristics of training data and carry out the feature selection process as a preprocessing step with independence of the induction algorithm. This model is advantageous for its low computational cost and good generalization ability. • Wrappers, which involve a learning algorithm as a black box and consist of using its prediction performance to assess the relative usefulness of subsets of variables. In other words, the feature selection algorithm uses the learning method as a subroutine with the computational burden that comes from calling the learning algorithm to evaluate each subset of features. However, this interaction with the classifier tends to give better performance results than filters. • Embedded methods, which perform feature selection in the process of training and are usually specific to given learning machines. Therefore, the search for an optimal subset of features is built into the classifier construction and can be seen as a search in the combined space of feature subsets and hypotheses. This approach is able to capture dependencies at a lower computational cost than wrappers.

3.1 Filter Methods  Filter methods are based on performance evaluation metrics calculated directly from the data, without direct feedback from predictors that will finally be used on data with a reduced number of features [3]. As mentioned above, these algorithms are usually computationally less expensive than wrappers or embedded methods. In this subsection, the most popular filters are described, which will be used throughout this book.
TableSummary of filters  Uni/Multivariate Ranker/Subset  Chi-Squared Univariate Ranker Information Gain Univariate Ranker ReliefF Multivariate Ranker mRMR Multivariate Ranker Md Multivariate Ranker CFS Multivariate Subset FCBF Multivariate Subset INTERACT Multivariate Subset Consistency Multivariate Subset
3.2 Embedded Methods  In contrast to filter and wrapper approaches, embedded methods do not separate the learning from the feature selection part. Embedded methods include algorithms, which optimize a regularized risk function with respect to two sets of parameters: 2.2 Feature Selection Methods 25  the parameters of the learning machine and the parameters indicating which features are selected. The embedded methods used are subsequently described.
3.3 Wrapper Methods  The idea of the wrapper approach is to select a feature subset using a learning algorithm as part of the evaluation function. Instead of using subset sufficiency, entropy or another explicitly defined evaluation function, a kind of “black box” function is used to guide the search. The evaluation function for each candidate feature subset returns an estimate of the quality of the model that is induced by the learning algorithm. This can be rather time consuming, since for each candidate feature subset evaluated during the search, the target learning algorithm is usually applied several times (e.g., in the case of ten-fold cross-validation used to estimate model quality). In this book the wrapper described as follows will be used; which can be combined with any learning algorithm.


Metáforas:
- la aguja en el pajar
- El problema siempre será pertinente pues la información disponible siempre será mayor que la información relevante.
 -->


# El problema de la selección de características

En el presente capítulo abordamos el problema de la selección de características, su importancia y desafíos. Repasaremos particularmente las dificultades asociadas con la escasez de datos y el desbalance de clases, debido al interés que ambas vicisitudes tienen para nuestra tesis. Luego describiremos brevemente los distintos enfoques para la selección de características, como así también ventajas y desventajas de cada uno. Finalmente, vincularemos la selección de características con el aporte fundamental de nuestro trabajo asociado a la generación sintética de datos. 

## Estrés, ignorancia y la selección de características

Un punto de partida difícil de controvertir en el mundo actual del aprendizaje automático es que la cantidad de información disponible para investigar ha crecido dramáticamente. Conforme la vida se digitaliza, y la cantidad de información en los sistemas aumenta, el problema de la generación de conocimiento parece menos determinado por la escasez de datos que por su abundancia. Esta *explosión* [@liWhenChatGPTComputer2023] hace más pertinente hoy las ideas de Fischer de lo que fueron hace 90 años, cuando reprochaba el *estrés* con el que se buscaba el conocimiento manipulando variables aisladas en vez de indagar en las complejas interacciones entre ellas, aceptando desde el inicio que ante tal complejidad primaba la ignorancia. [^fisherinteraccioneignorancia] 

[^fisherinteraccioneignorancia]: "In expositions of the scientific use of experimentation it is frequent to find an excessive stress laid on the importance of varying the essential conditions only one at a time. This ideal doctrine seems to be more nearly related to expositions of elementary physical theory than to laboratory practice in any branch of research. By contrast, in the state of knowledge or ignorance in which genuine research, intended to advance knowledge, has to be carried on, this simple formula is not very helpful. We are usually ignorant which, out of innumerable possible factors, may prove ultimately to be the most important, though we may have strong presuppositions that some few of them are particularly worthy of study. We have usually no knowledge that any one factor will exert its effects independently of all others that can be varied, or that its effects are particularly simply related to variations in these other factors." [@fisherDesignExperiments1935]

Así las cosas, desde aquellas ideas de Fisher hasta nuestros días, el problema de la selección de características no ha dejado de ser un tema de investigación en el campo del aprendizaje automático. La selección de características puede definirse como el proceso de detectar las características relevantes y descartar aquellas irrelevantes o redundantes, con el objetivo de obtener un subconjunto que describa adecuadamente el problema dado con una degradación mínima del rendimiento.

En general, los distintos enfoques para la selección de características adhieren a la premisa de que podemos separar la información relevante de aquella que no lo es para predecir la variable objetivo. Bajo esa perspectiva, la selección de características busca determinar un subconjunto de atributos que satisfaga uno de los siguientes criterios: 

1. El subconjunto con un tamaño especificado que maximice la precisión de la predicción.
2. El subconjunto de menor tamaño que satisfaga un requisito de precisión mínima.
3. El subconjunto que logre el mejor compromiso entre la dimensionalidad y la precisión.[@VignoloGerardFeatureSelection2012]

El criterio a elegir dependerá de los objetivos del estudio y de las características del problema. Sin perjuicio de ello, es fácil advertir las ventajas que ofrecen:

- La mejora del rendimiento de los algoritmos de aprendizaje automático,
- La comprensión de los datos, ganando conocimiento sobre el proceso subyacente y facilitando su visualización,
- La reducción del conjunto de características, economizando recursos en los procesos de recolección, almacenamiento y procesamiento, y
- La simplicidad, permitiendo el uso de modelos más simples y ganando velocidad de procesamiento.

Buscaremos precisar seguidamente los rasgos de la información que se han estudiado en la literatura y podemos encontrar en la práctica cotidiana del aprendizaje automático que confieren valor a la selección de características.

## Los datos y sus problemas

(falta citas

En el contexto actual del aprendizaje automático, los datos presentan una serie de desafíos que impactan directamente en la selección de características. Entre los principales problemas que enfrentamos al trabajar con conjuntos de datos, podemos destacar:

### Escasez de muestras

Un problema frecuente ocurre cuando disponemos de pocas muestras en un espacio vectorial de alta dimensionalidad, es decir, cuando el número de muestras ($m$) es menor que el número de características ($n$). Esta situación, conocida como "small sample size problem" [@fukunaga], es particularmente común en campos como el análisis genético, reconocimiento facial, procesamiento de imágenes médicas y reconocimiento de texto. La escasez de muestras dificulta la clasificación adecuada y resalta la importancia de una selección de características efectiva.

### Desbalance de clases

El desbalance de clases se presenta cuando la distribución de las clases en el conjunto de datos es altamente desigual. Típicamente, una clase mayoritaria domina significativamente sobre una o más clases minoritarias. Este problema es especialmente relevante en aplicaciones donde las clases minoritarias son precisamente las de mayor interés, como:

- Diagnóstico médico de enfermedades raras
- Detección de fraudes bancarios
- Identificación de intrusiones en redes
- Predicción de fallos en equipos técnicos

En estos casos, los clasificadores tradicionales tienden a sesgarse hacia la clase mayoritaria, pudiendo alcanzar una alta precisión global mientras fallan en la detección de los casos más importantes pero menos frecuentes.

### Complejidad intrínseca

La complejidad de los datos puede manifestarse en tres aspectos fundamentales:

1. **Ambigüedad de clases**: Surge cuando los casos no pueden distinguirse mediante ningún algoritmo de clasificación utilizando las características disponibles, ya sea porque los conceptos de clase están mal definidos o porque las características seleccionadas son insuficientes.

2. **Complejidad de fronteras**: Se refiere a situaciones donde los límites entre clases requieren descripciones extensas o complejas, independientemente del proceso de muestreo.

3. **Dispersión de muestras**: La combinación de pocas muestras y alta dimensionalidad genera una dispersión que dificulta la generalización de reglas.

### Ruido y valores atípicos

Los datos del mundo real raramente están libres de imperfecciones. El ruido puede provenir de diversas fuentes como dispositivos de medición defectuosos, errores de transcripción o irregularidades en la transmisión. Los valores atípicos (*outliers*), definidos como observaciones que se desvían significativamente del resto, pueden ser indicadores de errores en los datos o de casos especiales que requieren atención particular.

Estas características problemáticas de los datos no son mutuamente excluyentes y frecuentemente se presentan de manera simultánea, complicando aún más la tarea de selección de características. La comprensión de estos desafíos es fundamental para desarrollar estrategias efectivas de selección que permitan obtener modelos robustos y confiables.



