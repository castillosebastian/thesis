# Algoritmos Genéticos, datos sintéticos y selección de características {#sec-Capitulo4}

En este capítulo presentamos una descripción general de los *algoritmos genéticos* (AGs) como estrategia de selección de características, exponemos sus componentes y describimos la implementación realizada en nuestro trabajo. Seguidamente plantearemos la integración del AG con la generación sintética de datos mediante AV como etapa de preprocesamiento. Según el planteo hecho desde el inicio, la integración de ambas tecnologías permitiría resolver problemas de escasez muestral, ruido y alta dimensionalidad. Para terminar, entonces, expondremos los experimentos realizados con esta integración de AV-AG, y presentaremos los resultados obtenidos a fin de determinar hasta qué punto hemos podido resolver los problemas motivantes de este trabajo.

## Elementos básicos de los Algoritmos Genéticos

Los AGs son una clase de algoritmos de optimización inspirados en la evolución biológica y en la teoría de la selección natural. Los AGs se basan en el concepto de evolución de una población de soluciones potenciales a lo largo de múltiples generaciones, utilizando operadores genéticos como la selección, el cruce y la mutación para generar nuevas soluciones a partir de las disponibles, y mejorar la calidad de las mismas. Para clarificar el algoritmo, remitimos al lector a la implementación básica en `python` y otra más compleja implementada con la librería `DEAP` del **Anexo A**.

![Diagrama de un Algoritmo Genético](diagrama_ag.png)

A pesar de su extraordinaria simpleza -o quizás gracias a ella-, los AG constituyen algoritmos robustos, capaces de encontrar soluciones efectivas en una amplia variedad de problemas de optimización. Dicha robustez está determinada, como bien sostiene Goldberg [-@goldbergdavide.GeneticAlgorithmsSearch1989], por una serie de características distintivas, que fortalecen su configuración de búsqueda, a saber: a) operan sobre un espacio *codificado* del problema y no sobre el espacio en su representación original; b) realizan la exploración evaluando una *población de soluciones* y no soluciones individuales; c) tienen como guía una *función objetivo* (también llamada *función de aptitud*) que no requiere derivación u otras funciones de cálculo; y d) suponen *métodos probabilísticos de transición* (operadores estocásticos) y no reglas determinísticas. Estas características permiten a los AG superar restricciones que tienen otros métodos de optimización, condicionados -por ejemplo- a espacios de búsqueda continuos, diferenciables o unimodales. Por ello, su aplicación se ha difundido notablemente, trascendiendo los problemas clásicos de optimización, aplicándose en distintas tareas [@vieQualitiesChallengesFuture2021] y a lo largo de diversas industrias [@jiaoSurveyEvolutionaryMultiobjective2023].

Para comprender la eficacia que tienen los AGs como método de búsqueda, veamos en detalle los puntos mencionados anteriormente.

## a) Codificación del espacio de búsqueda

Como señalamos, los AGs se distinguen de otros algoritmos por su capacidad para operar en un espacio codificado del problema, en lugar de operar directamente sobre el espacio en su representación original. Esto sucede gracias a la transformación de las soluciones potenciales en cadenas binarias, comúnmente conocidas como **cromosomas**, que luego son objeto de transformación mediante operadores genéticos como la mutación y el cruce. La capacidad de los AGs para operar con estas representaciones codificadas determina su adaptabilidad y eficacia en una amplia gama de problemas de optimización.

La codificación adecuada del problema es un paso inicial clave para el correcto desempeño del algoritmo. La elección de la codificación depende de la naturaleza del problema y de las características de las soluciones que se buscan optimizar. Por ejemplo, en el tratamiento de información genética una opción frecuente es el uso de codificación binaria. En esta representación, cada cromosoma es una cadena de bits (0s y 1s) donde cada posición o bit representa la presencia (1) o ausencia (0) de una característica particular en la solución. Así, en un problema de selección de características con 100 variables, cada cromosoma sería una cadena de 100 bits donde un 1 indica que esa característica es seleccionada y un 0 que no lo es. Esta codificación binaria es particularmente eficiente para problemas de selección ya que permite representar de manera natural el espacio de búsqueda como un plano *n-dimensional*, donde cada dimensión representa una posible combinación de características. Además, los operadores genéticos como el cruce y la mutación pueden implementarse de manera directa y eficiente sobre estas cadenas binarias.

Dada la importancia que tiene la codificación, es fácil advertir que así como una elección adecuada de la estrategia de codificación puede facilitar la convergencia del AG hacia soluciones óptimas, una elección inadecuada puede tener consecuencias negativas en su desempeño. En efecto, una codificación inapropiada puede llevar a una exploración ineficaz del espacio de soluciones, generando soluciones redundantes o incluso inviables. Así, una codificación que no preserve la viabilidad de las soluciones durante la evolución, puede resultar en la convergencia prematura del AG hacia soluciones subóptimas.

La traducción entre la representación interna codificada (genotipo) y la solución en el contexto del problema (fenotipo) es un componente importante del diseño de los AGs [^genofenotipo]. Este mapeo no solo permite interpretar las soluciones generadas por el algoritmo, sino que también influye en la eficacia de los operadores genéticos. Ello así, por cuanto los operadores genéticos actúan directamente sobre la representación codificada, lo que puede afectar la exploración del espacio de soluciones y la convergencia del AG.

[^genofenotipo]: El *genotipo* se refiere a la representación interna de una solución en el Algoritmo Genético (AG). Es el "cromosoma" o la estructura de datos que codifica la información genética de un individuo, comúnmente representado como una cadena de bits (0s y 1s). Por otro lado, el *fenotipo* es la manifestación externa del genotipo en el contexto del problema, correspondiendo a la solución real en el espacio de búsqueda que se evalúa mediante la función de aptitud para determinar la calidad del individuo. Por ejemplo, en un problema de selección de características, cada bit del genotipo indica la inclusión (1) o exclusión (0) de una característica específica. Consideremos el genotipo 1100101, que representa la selección de las características 1, 2, 5 y 7, excluyendo las características 3, 4 y 6. El fenotipo asociado a este genotipo sería el subconjunto de características seleccionadas [X1, X2, X5, X7], el cual se utilizará para entrenar un modelo. El desempeño de este modelo, evaluado mediante la función de aptitud, determinará la calidad del individuo en el proceso evolutivo del AG.

Una de las principales ventajas de operar en un espacio codificado del problema radica en la posibilidad de aplicar operadores genéticos de manera eficiente, lo que permite una exploración exhaustiva del espacio de soluciones. En efecto, los operadores genéticos -que veremos en breve- son diseñados específicamente para actuar directamente sobre la representación codificada, generando nuevas soluciones de manera efectiva. 

Un proceso típico de codificación y decodificación en un AG incluye los siguientes pasos:

1.  **Espacio Original**: Representación directa del problema, por ejemplo, valores continuos o categóricos.
2.  **Codificación**: Traducción del espacio original a una forma binaria o simbólica.
3.  **Operadores Genéticos**: Aplicación de mutación, cruce y selección en la representación codificada.
4.  **Decodificación**: Traducción inversa de la solución codificada al espacio original para evaluación.

![Diagrama de codificación y decodificación](codificacion-decodificacion.png)

En el caso de nuestra investigación, dada la alta dimensionalidad de los datos y la complejidad de los modelos, la codificación adecuada de las soluciones fue un proceso fundamental para garantizar que los AGs puedan encontrar soluciones óptimas o cercanas al óptimo en tiempo razonable.

## b) Búsqueda por población de soluciones

Otra característica distintiva de los AGs es su enfoque en la evaluación de una **población** de soluciones en cada iteración, en lugar de centrarse en una única solución. Esta población de soluciones, también conocida como población de **individuos**, permite a los AGs explorar simultáneamente múltiples regiones del espacio de búsqueda, aumentando así la probabilidad de encontrar soluciones óptimas o cercanas al óptimo.

Como vimos en el ejemplo de más arriba, la población inicial regularmente se genera de manera aleatoria, y cada individuo dentro de esta población representa una solución potencial al problema. A lo largo de las generaciones, los AGs aplican operadores genéticos como selección, cruce y mutación para producir nuevas generaciones de individuos, mejorando iterativamente la calidad de las soluciones. 

La diversidad genética dentro de la población es fundamental para la eficacia de los AGs, ya que permite a los algoritmos explorar de manera más exhaustiva el espacio de características y evitar la convergencia prematura hacia soluciones subóptimas. En efecto, consideremos una población homogénea donde todos los individuos son idénticos. En este caso, la capacidad del AG para explorar nuevas regiones del espacio de búsqueda se ve severamente limitada, lo que puede resultar en una convergencia temprana hacia soluciones subóptimas. Por el contrario, una población diversa, donde cada individuo representa una solución única, permite al AG explorar una variedad de soluciones y adaptarse a las condiciones cambiantes del problema.

A modo de ejemplo consideremos estas dos poblaciones de 5 individuos codificados como sigue:

Población A, con 5 individuos de longitud 5, de alta diversidad:

-   Individuo 1: `11001`
-   Individuo 2: `10110`
-   Individuo 3: `01101`
-   Individuo 4: `11100`
-   Individuo 5: `00011`

Población B, con 5 individuos de longitud 5, de baja diversidad:

-   Individuo 1: `11111`
-   Individuo 2: `11111`
-   Individuo 3: `11011`
-   Individuo 4: `11010`
-   Individuo 5: `11010`

Como podemos advertir en este ejemplo, cada individuo representa una solución potencial al problema, donde cada bit en la cadena codificada corresponde a una característica que puede ser seleccionada o excluida. En estas poblaciones los individuos de A son distintos entre sí, lo que permite al AG combinar cromosomas distintos y explorar nuevas zonas del espacio de búsqueda. Por el contrario, los individuos de B son idénticos, lo que limita la capacidad del AG para explorar nuevas regiones del espacio de búsqueda.

## c) Función de aptitud y evaluación de soluciones

La función de aptitud es el núcleo que dirige el proceso evolutivo en los AGs, determinando qué soluciones sobreviven y se propagan a la siguiente generación. Su diseño y correcta implementación son esenciales para asegurar que el AG no solo converja hacia soluciones de alta calidad, sino que también lo haga de manera eficiente y efectiva, especialmente en problemas donde las evaluaciones de aptitud son costosas o complejas.

En el proceso evolutivo de los AGs, La función de aptitud se aplica al **fenotipo** de cada solución, es decir, a su manifestación en el contexto del problema a resolver, después de que el **genotipo** (la representación codificada de la solución) ha sido transformado. Esta evaluación cuantifica qué tan bien una solución potencial cumple con los objetivos del problema, asignándole un valor numérico que refleja su desempeño relativo en comparación con otras soluciones dentro de la población.

El diseño de la función de aptitud es un aspecto crítico del proceso de modelado en los AGs, ya que guía la dirección de la búsqueda evolutiva. Específicamente, la función de aptitud debe estar alineada con los objetivos del problema, reflejando correctamente las restricciones necesarias a satisfacer. En situaciones de optimización multiobjetivo, donde varios criterios deben ser optimizados simultáneamente, es común que funciones de aptitud individuales se combinen en una única métrica a través de técnicas como la suma ponderada de los valores de aptitud individuales. En el contexto de nuestra investigación, orientada a la selección de características, la función de aptitud combina la aptitud de un individuo en términos de precisión y el tamaño del conjunto de características seleccionadas (veremos un ejemplo en breve).

En línea con lo anterior, la evaluación precisa de las soluciones mediante la función de aptitud puede constituir un proceso sujeto a múltiples restricciones. Aunque la asignación de valores de aptitud más bajos a soluciones subóptimas y más altos a soluciones superiores pueda parecer un criterio ineludible, en la práctica, este proceso requiere comúnmente consideraciones adicionales. Por ejemplo, en problemas con restricciones, una solución cercana al óptimo global, pero que infrinja requerimientos del problema, debe recibir una calificación de aptitud inferior a una solución menos cercana al óptimo pero factible. De esa forma, el AG puede orientar la búsqueda hacia soluciones viables. Con esa lógica, en la optimización multiobjetivo es necesario establecer criterios para ponderar la proximidad al óptimo, especialmente cuando los distintos objetivos compiten entre sí.

## d) Operadores estocásticos y *esquemas* genéticos

Como hemos señalado los AGs emplean **métodos probabilísticos de transición** conformados por **operadores estocásticos**, que introducen aleatoriedad en el proceso evolutivo. Esto determina que las transformaciones dentro de un AG no siguen un camino determinista hacia la solución óptima; en su lugar, cada generación de soluciones es producto de un proceso estocástico controlado. 

Los **operadores genéticos** fundamentales en este proceso son la **selección**, el **cruce (crossover)** y la **mutación**. Los mismos son responsables de la generación de nuevas soluciones, e inciden directamente en la evolución de los  patrones genéticos que los AG tienden a preservar y reproducir. Patrones que se conocen como **esquemas** [-@goldbergdavide.GeneticAlgorithmsSearch1989].

Según explica Goldberg, los **esquemas** son estructuras genéticas que se repiten en la población y que influyen en la evolución de los individuos. Estos esquemas pueden ser de **orden bajo** (pocos genes) o de **orden alto** (más genes), y de **longitud de definición baja** (pocos bits) o de **longitud de definición alta** (más bits). En su operatoria, los AGs tienden a favorecer los esquemas de orden bajo y longitud de definición baja que muestran un rendimiento mejor que la media. Este fenómeno, conocido como **Teorema del Esquema**, proporciona una base para entender cómo la selección y los operadores estocásticos actúan en conjunto para guiar la evolución hacia soluciones óptimas. Veamos esto en detalle.

El operador de **selección** opera identificando y preservando los esquemas con aptitudes superiores a la media de la población. En términos probabilísticos, los esquemas con mejor aptitud tienen una mayor probabilidad de ser seleccionados y reproducidos en la siguiente generación. Esta selección basada en aptitud es clave para mantener y amplificar características beneficiosas dentro de la población. Dentro de los operadores de selección, se encuentran las estrategias de selección por ruleta, torneo y ventana, que veremos a continuación: 

- **selección por ruleta** asigna a cada individuo una probabilidad de ser seleccionado proporcional a su aptitud relativa dentro de la población. Imaginando una ruleta dividida en segmentos donde el tamaño de cada segmento corresponde a la aptitud del individuo, los individuos con mayor aptitud ocupan una mayor porción de la ruleta, aumentando sus probabilidades de ser elegidos. Este método favorece fuertemente a los individuos más aptos, promoviendo una rápida explotación de soluciones prometedoras. Sin embargo, puede llevar a una reducción de la diversidad genética si ciertos individuos dominan consistentemente el proceso de selección.
- **selección por torneo** consiste en seleccionar al azar un subconjunto de individuos de la población y elegir al mejor de este grupo para la reproducción. Este método introduce un equilibrio entre explotación y exploración, ya que permite que individuos con alta aptitud tengan una mayor probabilidad de ser seleccionados, mientras que también da oportunidad a individuos menos aptos de participar ocasionalmente. La presión de selección puede ajustarse variando el tamaño del torneo, donde torneos más grandes incrementan la probabilidad de seleccionar a los individuos más aptos, mientras que torneos más pequeños fomentan una mayor diversidad genética.
- **selección por ventana** divide la población en grupos o "ventanas" homogéneas y selecciona a los individuos más aptos dentro de cada ventana para la reproducción. Este enfoque asegura una representación equitativa de diferentes regiones del espacio de búsqueda, evitando que solo los individuos de alta aptitud global dominen el proceso de selección. Al mantener la diversidad entre las ventanas, la selección por ventana promueve una exploración más amplia del espacio de soluciones, lo que puede ser especialmente beneficioso en problemas con múltiples óptimos locales.

La selección por sí sola no es suficiente para garantizar la exploración global del espacio de búsqueda, de ahí la importancia del cruce y la mutación.

El operador de **cruce** permite la recombinación de material genético entre dos o más soluciones. En un AG, la función principal del cruce es preservar y mejorar las características exitosas encontradas en los padres, mientras introduce suficiente variación para explorar nuevas áreas del espacio de búsqueda. Por ejemplo, en la representación binaria, un cruce de un punto dividirá dos soluciones en una posición elegida aleatoriamente y combinará segmentos de ambas para crear nuevos individuos. Este proceso asegura la transmisión de esquemas de orden bajo y longitud de definición baja, mientras introduce nuevas combinaciones genéticas que pueden llevar a soluciones más adaptativas.

Un ejemplo de cruce de un punto entre dos soluciones binarias sería:

-   Padre 1: `110010`
-   Padre 2: `101101`
-   Punto de Cruce: `3`
-   Hijo 1: `110101`
-   Hijo 2: `101010`

En este caso, el cruce de un punto en la posición 3 divide los padres en dos segmentos y combina los segmentos para generar dos nuevos individuos. Este proceso de cruce permite la recombinación de material genético entre los padres, preservando y mejorando las características exitosas encontradas en ellos.

El operador de **mutación**, por su parte, introduce cambios aleatorios en las soluciones existentes, actuando como un mecanismo de perturbación que permite al AG escapar de óptimos locales y explorar más exhaustivamente el espacio de soluciones. La mutación puede variar desde simples alteraciones de bits en cadenas binarias hasta ajustes en representaciones continuas mediante la adición de ruido gaussiano. La mutación es crucial para asegurar que el AG mantenga la capacidad de descubrir nuevas áreas del espacio de búsqueda.

Un ejemplo de mutación en una solución binaria sería:

-   Solución Original: `110010`
-   Posición de Mutación: `4`
-   Solución Mutada: `110110`

A esta altura ha de ser evidente que la preservación de ciertos patrones genéticos de aptitud superior es fundamental para la evolución de la población en un AG. La teoría de los esquemas, que se basa en el concepto de esquemas genéticos, proporciona un marco formal para entender cómo los operadores genéticos actúan en conjunto para guiar la evolución hacia soluciones óptimas. @goldbergdavide.GeneticAlgorithmsSearch1989
nos presenta, en relación a este punto, lo que se conoce como la **Ecuación del Esquema**, que es una herramienta teórica que permite predecir la evolución de los esquemas en una población a lo largo de múltiples generaciones. Esta ecuación tiene en cuenta factores como la aptitud de los esquemas, la probabilidad de cruce y mutación, la longitud de definición y el orden de los esquemas, y proporciona una guía para entender cómo los esquemas se propagan y se mantienen en la población. Para una revisión más detallada de la ecuación del esquema, remitimos al lector al **Apéndice B**.

Hasta aquí hemos expuestos los fundamentos teóricos de los AGs, que nos permitirán entender la implementación de los mismos en la siguiente sección. 

## Integración de Autocodificadores Variacionales y Algoritmos Genéticos 

En esta sección decribimos la integración de AVs y AGs como estrategia para la selección de características en contexto donde los datos poseen alta dimensionalidad, escasez muestral y ruido. 

La estrategia propuesta se basa en la generación de datos sintéticos mediante AVs, y la selección de características con AGs empleando datos aumentados que incluyen observaciones originales y sintéticas. El proceso comienza con la etapa de generación de datos sintéticos donde se emplea un AV o AVC según el tipo de problema a resolver (clasificación binaria o multiclase). En esta etapa el modelo de *autocodificador* es entrenado con los datos originales,y parametrizado según una configuración óptima definida. En nuestro caso, dicha configuración se obtuvo mediante el proceso de optimización de hiperparámetros descrito en el capítulo anterior. Seguidamente, se emplea el modelo de *autocodificador* entrenado para generar *N* datos sintéticos, que se concatenan con los datos originales. El resultado es un dataset aumentado que incluye las instancias originales y también instancias sintéticas. Finalmente, se emplea un AG para la selección de características empleando como entrada ese dataset aumentado. Detallamos este proceso en el diagrama de flujo que se presenta mas abajo.

![Diagrama de flujo de procesamiento](AV-AG.png)

El primer paso, vinculado a la generación de datos sintéticos mediante AVs, tiene como objetivo aumentar el número de muestras disponibles, y de esa forma superar el problema de la escasez muestral. Al mismo tiempo, dado que en la tarea de generación los *autocodificadores* involucran un proceso de codificación y decodificación de la distribución de probabilidad de los datos originales, se espera que las nuevas instancias sintéticas reduzcan el ruido presente en los datos originales. 

En términos de arquitectura se configuraron los modelos de AV y AVC según los parámetros óptimos obtenidos de la optimización de hiperparámetros que se describió en el capítulo anterior. El entrenamiento de los modelos en la etapa generativa es un entrenamiento estándar: se utilizan las particiones originales de entrenamiento y testeo, subdividiendo la primera en dos subconjuntos, uno para entrenamiento y otro para validación; se entrenan los modelos por *N* épocas, donde *N* es el número de épocas que se obtuvo de la optimización de hiperparámetros, y se utiliza un término de paciencia de 10 épocas. Presentamos a continuación los parámetros de los modelos de AV y AVC configurados para cada conjunto de datos:

| dataset        | modelo | capa_1 | capa_2 | capa_3 | capa_latente | tasa_aprend. | epocas |
|----------------|--------|--------|--------|--------|--------------|--------------|--------|
| Leukemia       | AV     | 7129   | 346    | 178    | 108          | 0.00026927   | 2889   |
| Madelon        | AV     | 500    | 835    | 308    | 25           | 0.00015503   | 1364   |
| Gisette        | AV     | 5000   | 3870   | 2987   | 18           | 0.00094609   | 2739   |
| GCM            | AVC    | 16063  | 358    | 189    | 35           | 0.00068850   | 3613   |
| ALL-Leukemia   | AVC    | 12600  | 434    | 176    | 35           | 0.00040148   | 949    |

Las mismas dimensiones se emplearon para capas lineales de las redes codificadora y decodificadora que componen los modelos de AV y AVC.

Respecto al segundo paso, la selección de características se realiza utilizando un AG con datos aumentados. La implementación se centra en la optimización simultánea de la precisión de un modelo MLP y la reducción del número de características seleccionadas, utilizando operadores genéticos clásicos como el cruce y la mutación, junto con técnicas avanzadas de evaluación y selección.

La configuración del AG a lo largo de los distintos experimentos que formaron parte de esta investigación (y que revisaremos en detalle en el próximo punto) incluyó: 

- **Población inicial**: individuos generados aleatoriamente, cada uno representado como una lista de bits de longitud igual al número de características.
- **Función de aptitud**: Maximización de la precisión del modelo de clasificación y minimización del número de características activas.
- **Operadores genéticos**: Selección por torneo, cruce de dos puntos y mutación por inversión de bits.
- **Parámetros del AG**: Probabilidad de mutación (e.g.`PROB_MUT = 0.1`), probabilidad de cruce (e.g. `PX = 0.75`), número máximo de generaciones (e.g.`GMAX = 15`).
- **Evaluación de características**: Análisis de la frecuencia de activación de las características a lo largo de las generaciones.
- **Criterio de terminación**: Convergencia o número máximo de generaciones alcanzado.
- **Análisis de resultados**: Selección de las características más relevantes basadas en su frecuencia de activación.

En cada experimento, la implementación del AG comienza con la definición de los componentes básicos del algoritmo. Se define una función de aptitud (`fitness`) orientada a maximizar, que evalúa cada individuo en función de dos criterios: la precisión (`accuracy`) del modelo de clasificación entrenado con las características seleccionadas, y la fracción de características activas. Esta función de aptitud está diseñada para balancear la necesidad de un modelo predictivo preciso con la simplicidad y la eficiencia del modelo, evitando el sobreajuste y facilitando la interpretación del modelo final.

Los individuos, representados como listas de bits, se construyen utilizando una función de construcción de genes que genera un bit aleatorio basado en una probabilidad definida (`p_indpb`). Estos individuos se agrupan en una población inicial, que luego se somete a un proceso evolutivo. Durante la evolución, los individuos se seleccionan mediante la técnica de torneo, donde aquellos con mejor aptitud tienen una mayor probabilidad de ser elegidos para reproducción. Los individuos seleccionados se cruzan utilizando un operador de cruce de dos puntos (`cxTwoPoint`), que intercambia segmentos de los cromosomas de los padres para generar descendientes con combinaciones genéticas novedosas. Posteriormente, se aplica un operador de mutación que invierte los bits en el cromosoma según la probabilidad de mutación definida, asegurando que el AG mantenga la capacidad de explorar nuevas regiones del espacio de búsqueda.

A lo largo de las generaciones, el AG monitoriza y registra diversas estadísticas de la población, como la aptitud promedio, la precisión y el número de genes activos. Estas métricas permiten evaluar el progreso del algoritmo y la convergencia hacia soluciones óptimas. Al final del proceso evolutivo, se realiza un análisis de las características seleccionadas, calculando la frecuencia de activación de cada característica a lo largo de las generaciones y seleccionando las más recurrentes como las más relevantes. Este enfoque permite identificar un subconjunto óptimo de características que no solo maximiza la precisión del modelo, sino que también minimiza su complejidad.

Con fines ilustrativos, en el **Apéndice A** se presenta un script genérico de un Algoritmo Genético implementado con la librería `DEAP` de Python, que puede ser adaptado para la selección de características en problemas de alta dimensionalidad. 

## Experimentos realizados y sus resultados 

A continuación presentamos los experimentos realizados en el marco de la investigación, cuyo objetivo principal fue evaluar la efectividad de la técnica de aumentación de datos en la selección de características mediante Algoritmos Genéticos (AGs) en contextos de escasez muestral, alta dimensionalidad y ruido. Para ello, se diseñaron y ejecutaron experimentos utilizando cuatro conjuntos de datos distintos: *Leukemia*, *Gisette*, *Madelon* y *GCM*, cada uno representando diferentes desafíos en términos de tamaño y características de los datos. Un quinto conjunto de datos, *ALL-Leukemia*, fue utilizado para validar los resultados obtenidos en el contexto de clasificación multiclase.

El enfoque experimental adoptado fue comparativo, contrastando el desempeño de los AGs en la selección de características utilizando datos originales frente a la misma tarea utilizando datos aumentados con muestras sintéticas generados por AVs. Los parámetros de los AGs fueron mantenidos constantes entre los grupos de experimentos con y sin aumento, permitiendo una evaluación directa del impacto de la aumentación.

La metodología seguida para cada experimento se describe a continuación. Se presentan los resultados obtenidos, incluyendo métricas como precisión en la clasificación, número de características seleccionadas y estabilidad de la selección de características a lo largo de las generaciones. Se discuten las implicaciones de los resultados y se proponen posibles ajustes y mejoras para futuros experimentos.

## Metodología

Los experimentos se diseñaron para comparar el rendimiento de los AGs aplicados a los datos originales y a un conjunto de datos ampliado con muestras sintéticas. El objetivo de esta comparación era evaluar el impacto de la aumentación de datos en el proceso de selección de características. Para ello, se utilizaron los siguientes criterios de evaluación: precisión en la clasificación, número de características seleccionadas y estabilidad de la selección de características. Con el fin de asegurar la comparabilidad de los resultados, siempre se empleó la partición original de testeo para medir la precisión del algoritmo.

Se llevaron a cabo experimentos utilizando el modelo de AG descrito previamente, adoptando una representación binaria para las características. La función de aptitud se basó en un clasificador de perceptrón multicapa (MLP), que evaluaba la precisión de la clasificación y penalizaba el número de características seleccionadas. El objetivo del algoritmo genético era identificar un subconjunto óptimo de características que maximizaran la precisión y minimizaran la dimensionalidad del espacio de características.

El aspecto innovador de este enfoque está en el uso de un AV para enriquecer el proceso de entrenamiento del AG. El AV, según vimos en el Capítulo 3, fue optimizado para generar muestras sintéticas que preservaran la estructura subyacente de los datos originales, y permitiera al AG explorar un espacio de características más amplio y diverso. La hipóstesis que se buscó validar en el presente trabajo fue que: al aumentar sintéticamente el número de muestras disponibles el AG podría identificar un subconjunto más efectivo de características, que se traduciría en una mejora en la precisión y la eficiencia de la selección.

![Comparación de experimentos](diseño_experimentos.png)

## Leukemia

Según lo visto en el Capítulo 1, el conjunto de datos *Leukemia* de expresión génica obtenidos de micro-datos de ADN es reconocido por su alta dimensionalidad (7129 mediciones) relativa al número de muestras (38 para entrenamiento y 34 para testeo). Esto lo convierte en un candidato apropiado para evaluar la capacidad de los AVs para generar datos sintéticos, a partir de los cuales aumentar el número de muestras disponibles y mejorar el desempeño de los AGs en la selección de características. 

Como vimos, se realizaron dos conjuntos de experimentos para comparar los resultados de los AGs aplicados a los datos originales frente a los datos aumentados:

1. **Datos originales:** Se trabajó directamente con las muestras disponibles en el conjunto *leukemia*, siguiendo la partición original en un conjunto de entrenamiento y un conjunto de prueba.
2. **Datos aumentados:** Se realizarons experimentos con 100, 200 y 1000 muestras sintéticas adicionales (a las 38 originales) mediante un AV entrenado específicamente para este conjunto de datos.

La primera serie de experimentos con datos originales y datos aumentados tuvo por objetivo contar con una primera aproximación al problema y establecer una línea base a partir de la cual iterar con ajustes y mejoras en la configuración del AG y el AV. Luego, se realizaron experimentos adicionales para explorar diferentes configuraciones del AG y el AV, con el objetivo de identificar las condiciones óptimas para la selección de características en este conjunto de datos. Se investigó el impacto de variar la probabilidad de mutación, la probabilidad de cruce, la proporción de genes activos en el cromosoma y el número de muestras generadas por el AV. Particularmente se exploró el impacto de la reducción de la proporción de genes activos en el cromosoma en la eficiencia de la selección de características, pasando de p = 0.01 a p = 0.005.

En el primer conjunto de experimentos, se estableció una configuración base utilizando una probabilidad de mutación de 0.01, una probabilidad de cruce del 0.75, y una proporción de genes activos en el cromosoma del 10% del total de características, limitado a un máximo de 20 generaciones. Esta configuración fue seleccionada para equilibrar la exploración y explotación del espacio de búsqueda, asegurando que el AG pudiera explorar adecuadamente las posibles combinaciones de características sin prologar la búsqueda en exceso. Para los experimentos con aumentación de datos, se generaron 100 muestras sintéticas adicionales mediante un AV. En estas primeras pruebas los resultados mostraron que tanto la precisión como el número de genes seleccionados fueron similares entre los grupos con y sin aumentación de datos. Sin embargo, se destacó una menor dispersión en la variaza de la precisión en los resultados del grupo con aumentación, lo que sugirió que la generación de datos sintéticos contribuyó a una mayor estabilidad del modelo.

En un segundo grupo de experimentos exploratorios, se investigaron variaciones en la configuración de la proporción de genes activos en el cromosoma y el tamaño del conjunto de datos, para examinar en profundidad los efectos de la aumentación. Específicamente, se realizaron pruebas con conjuntos de datos aumentados que incluían 200 y 1000 muestras sintéticas adicionales, y se redujo agresivamente la proporción de genes activos en el cromosoma, en algunos casos hasta un 0.5% de las características totales. Estas configuraciones más extremas fueron seleccionadas para evaluar la robustez del AG frente a diferentes tamaños del espacio de búsqueda, especialmente en escenarios donde se esperaba que la reducción dimensional comprometiera la capacidad del AG para encontrar soluciones óptimas. 

### Resultados

En los experimentos realizados sobre el conjunto de datos leukemia, los resultados mostraron leves diferencias en favor de la aumentación, tanto en lo que respecta a precisión, como al número de características seleccionadas. Aunque la diferencia en términos de precisión no es estadísticamente significativa -como puede verse en el gráfico de caja-, si se observa una mayor estabilidad.

![Precisión en Leukemia](boxplot_leukemia_combined.png)

Sin perjuicio de lo señalado, entendemos posible que, en conjuntos de datos donde los modelos alcanzan una precisión alta (como es el caso que nos ocupa), la aumentación no produce diferencias sustanciales en la performance del AG.

Un punto a resaltar sobre este dataset es la alta correlación entre las características, lo que se refleja en la cantidad significativa de características seleccionadas al menos una vez durante todas las pruebas realizadas. Tanto en los experimentos con datos aumentados como en los originales, el AG seleccionó al menos una vez un número muy similar de características: 6779 y 6772. Este comportamiento sugiere que la mayoría de las características están fuertemente vinculadas, lo que permite que el AG identifique soluciones efectivas utilizando diferentes subconjuntos de características. Asimismo, se observó que un pequeño porcentaje de características (aproximadamente el 10%) es suficiente para resolver el problema de clasificación, independientemente de cuáles sean esas características, debido a la redundancia en la información aportada por las correlaciones. El análisis cuantitativo de las correlaciones muestra que, aunque solo el 0.32% de los pares de características posibles tienen una correlación absoluta mayor a 0.7, estos representan un número considerable (80742) de correlaciones significativas. Es decir: aunque el número de características altamente correlacionadas es una fracción pequeña del total, su impacto en la selección de características es notable, dado que el AG tiende a seleccionar conjuntos de características que son efectivamente intercambiables debido a su alta redundancia.

## Gisette

El conjunto de datos *Gisette* es un problema de clasificación binaria con alta dimensionalidad y un número equilibrado de muestras en ambas clases. Es un set de datos creado para trabajar el problema de reconocimiento de dígitos escritos a mano [@isabelleguyonGisette2004], y tiene 13500 observaciones y 5000 atributos. Este conjunto de datos fue seleccionado para evaluar cómo la aumentación de datos afecta la selección de características en un contexto donde el espacio de características es grande, pero la relación señal-ruido es moderada.

La metodología seguida en los experimentos con *gisette* fue similar a la utilizada en *leukemia*. La configuracion del AG estuvo establecida en: probabilidad de mutación de 0.0002, probabilidad de cruce de 0.75, proporción de genes activos en el cromosoma de 0.1 y un número máximo de generaciones de 30. Se prestó especial atención a la reducción del espacio de búsqueda mediante una proporción de genes activos en el cromosoma equivalente a 500 características. Se investigó también el impacto de generar un número mucho mayor de muestras sintéticas (6000).

### Resultados

Los resultados en Gisette fueron similares a los observados en leukemia. La precisión media fue ligeramente superior en los experimentos con datos aumentados (0.960) en comparación con los datos originales (0.959), pero sin diferencia estadística significativa.

![Precisión en Gisette](boxplot_gisette_combined.png)

Este nuevo hallazgo refuerza la hipótesis de que la aumentación de datos tiene un impacto limitado en conjuntos de datos donde los modelos ya alcanzan una alta precisión. Sin embargo, al igual que en leukemia, la estabilidad de la selección de características mejoró con la aumentación, como se refleja en la menor desviación estándar y el rango intercuartílico (IQR):

|                    | Datos aumentados    | Datos originales    |
|--------------------|---------------------|---------------------|
|**Características seleccionadas (media)** | 424.231             | 436.667             |
|**Desviación Estándar**                  | 17.796              | 14.355              |
|**Rango Intercuartil (IQR)**             | 8.000               | 21.500              |


En efecto, en Gisette se observa una diferencia más importante en la eficiencia de la selección de características. En los experimentos con datos aumentados, el número promedio de características seleccionadas fue menor (424) en comparación con los datos originales (436), señalando que el AG fue más efectivo en el reconocimiento de un subconjunto relevantes. 

## Madelon

El conjunto de datos *Madelon* es un caso especial donde solo cinco características son relevantes, mientras que otras quince son combinaciones lineales de estas, y el resto son datos aleatorios. Así, este conjunto representa un desafío interesante para evaluar la capacidad de los AGs para identificar características útiles en un entorno donde la señal está oculta entre una gran cantidad de ruido.

La metodología seguida en los experimentos con *Madelon* fue similar a la utilizada en *Leukemia* y *Gisette*. La configuración del AG fue la siguiente:

- **Mutación:** PROB_MUT = 1/IND_SIZE (~0.002)
- **Probabilidad de cruce:** PX = 0.75
- **proporción de genes activos en el cromosoma:** p = 0.1
- **Número máximo de generaciones:** GMAX = 30

Se generaron 2000 muestras sintéticas para incrementar el número de observaciones disponibles y evaluar si esto mejoraba la capacidad del AG para encontrar las características relevantes. Como en los experimentos precedentes, se exploraron diferentes configuraciones de la proporción de genes activos en el cromosoma y se investigó cómo la aumentación de datos, en el contexto de un espacio de búsqueda complejo y ruidoso, afectaba la eficiencia de la selección de características.

### Resultados

Los experimentos en Madelon mostraron resultados significativamente diferentes a Leukemia y Gisette. La precisión media en los experimentos con datos aumentados fue de 0.83, lo que representa un aumento del 10.4% en comparación con la precisión de los datos originales de 0.75. Esta diferencia es estadísticamente significativa, lo que indica que la aumentación de datos tuvo un impacto positivo en el desempeño del AG.

![Precisión en Madelon](boxplot_madelon_combined.png)

Estos resultados sugieren que la aumentación de datos puede ser especialmente efectiva en conjuntos de datos con características complejas y un alto nivel de ruido, como es el caso de Madelon. La generación de muestras sintéticas permitió al AG identificar mejor las características relevantes, lo que se tradujo en una mejora significativa en la precisión del modelo.

Por otro lado, el análisis de selección de características reveló que, en promedio, el número de características seleccionadas fue menor en los experimentos con datos aumentados (29) en comparación con los datos originales (35). Este hallazgo, también resalta la eficiencia de la aumentación de datos en la selección de características.

Finalmente, podemos destacar también que de los algoritmos clásicos evaluados (cuyo reporte hicimos en el Capítulo 1) solo 2 de los 18 logra superan los valores de precisión obtenidos con el AG en los datos aumentados. Ambos modelos, AdaBoost y Baggind, lograron esos resultados luego de un proceso de optimización de hiperparámetros. El resto de los modelos clásicos oscila entre 0.5 y 0.7 de precisión, lo que refuerza la idea de que la aumentación de datos puede ser una estrategia efectiva para mejorar el desempeño de los AGs en conjuntos de datos complejos.

## GCM

### Metodología parte 1

El conjunto de datos *GCM* representa un desafío aún mayor que los anteriores no solo debido a la alta dimensionalidad y bajo número de muestras, sino también por las múltiples clases y el desbalance entre ellas.  El dataset consiste en 16063 atributos (biomarcadores), sobre 190 muestras de tumores que refieren a 14 clases de cáncer humano.

El proceso de experimentación en GCM estuvo dividido en dos partes. En la primera parte, se realizaron experimentos exploratorios con diferentes configuraciones del AVC y el AG. El objetivo de esta etapa fue establecer una línea base para evaluar la efectividad de la aumentación de datos. En la segunda parte, se realizaron ajustes en la metodología, el modelo de AVC y el diseño de la integración AVC-AG. Esta vez, el objetivo era mejorar la calidad de los datos sintéticos y la eficiencia de la selección de características.

Aunque se realizaron una gran cantidad de experimentos (algunos exploratorios y otros más específicos), nos centraremos en los resultados de la primera y segunda parte, que nos permitieron identificar los desafíos y oportunidades en la utilización de AVs en conjuntos de datos complejos como GCM.

La cantidad de muestras sintéticas generadas en los experimentos fue ajustada a lo largo de las pruebas, conforme se exploraban opciones de mejora. Ello dio lugar a la siguiente tabla de experimentos por cantidad de muestras analizadas, donde la primera fila representa la cantidad experimentos con las muestras originales (sin aumentación):

| n_muestras        | cantidad_experimentos |
|-------------------|-----------------------|
| 133               | 20                    |
| 3129              | 15                    |
| 1322              | 4                     |
| 1323              | 4                     |
| 1304              | 2                     |
| 254               | 2                     |
| 6106              | 1                     |

Al enfrentarnos a GCM asumíamos que la generación sintética de muestras y ulterior proceso de selección de características plantearía un desafío significativo al AG debido a la complejidad del dataset. La calidad de reconstrucción lograda en los datos  sintéticos generados por el AVC presentado en el Capítulo 2, aunque capaz de preservar la estructura subyacente del conjuntos de datos de GCM, no permitía suponer una mejora significativa en la precisión de la clasificación con un AG. Por otro lado, el desbalance que tienen las clases en GCM, determinaba que la calidad de reconstrucción de las clases minoritarias fuera menor, lo que podría afectar la capacidad del AG para identificar un subconjunto relevante de características. Por todo ello, esperabamos que los experimentos con datos aumentados presentaran resultados mixtos, con posibles mejoras en la estabilidad de la selección de características, pero sin mejoras significativa en la precisión. 

Respecto de los recursos de hardware utilizados, dado que la aumentación de datos requeriría un poder de cómputo proporcional a al tamaño del dataset por la cantidad de muestras sintéticas generadas, se optó por utilizar un entorno en la nube para ejecutar los experimentos de manera eficiente. Para este propósito, se trabajó con una máquina virtual en Google Cloud Platform con 8 vCPUs y 30 GB de RAM.

La configuración inicial del AG fue la siguiente:

- **Mutación:** PROB_MUT = 1,16,160/IND_SIZE ~(0.0006, 0.01, 0.1)
- **Probabilidad de cruce:** PX = 0.75 
- **Cromosoma activo:** p = 0.1
- **Número máximo de generaciones:** GMAX = 20

En la primera etapa se experimentó con una proporción de genes activos en el cromosoma que representaba el 10% de las características totales, al rededor de 1600 genes por cromosoma, tanto para el grupo de experimentos con datos originales como al grupo de experimentos con datos aumentados. Esta medida se adoptó luego de realizados dos experimentos exploratorios con cromosomas activos en valores del 20% y 30% de las características totales, donde se observó una acentuada degradación en la precisión (en el rango de 0.2 y 0.35). 

Asimismo se varió la probabilidad de mutación en tres valores: 0.0006, 0.01 y 0.1, para instar una exploración más amplia del espacio de búsqueda. La probabilidad de cruce se mantuvo constante en 0.75, y el número máximo de generaciones fue de 20.

Respecto del grupo de experimentos con datos aumentados, el dataset mixto de entrenamiento incluía 1400 muestras sintéticas (100 instancias por clase) y la partición original de entrenamiento (con 133 observaciones). La evaluación se realizó sobre los datos originales de testeo (57 observaciones). 

### Resultados parte 1

Los resultados, como anticipabamos, nos fueron favorables. La precisión media fue ligeramente superior en los experimentos con datos originales (0.538) en comparación con los datos aumentados (0.512). La estabilidad de la selección de características fue similar en ambos grupos, con una dispersión similar en la cantidad de características seleccionadas, como se puede observar en el siguiente gráfico.

![GCM Resultados exploratorios](boxplot_gcm_combined_part1.png)

Como resultado de esta primera etapa de experimentos se identificó una dificultad importante en la metodología utilizada. La calidad de los datos sintéticos generados por el AVC no contribuyó a mejorar la precisión de la clasificación, lo que sugiere que el AG no fue capaz de identificar un subconjunto relevante de características en los datos aumentados. Ante esta circunstancia se procedió a realizar un test de diagnóstico para evaluar la calidad de los datos sintéticos. Se generaron 3000 muestras, se entrenó y evaluó un MLP con estos datos. Los resultados en la partición sintética de testeo fueron los siguientes:


| Clase | Precisión | Recall | F1-Score | Support |
|-------|-----------|--------|----------|---------|
| 0     | 1.00      | 1.00   | 1.00     | 23      |
| 1     | 1.00      | 1.00   | 1.00     | 28      |
| 2     | 1.00      | 1.00   | 1.00     | 31      |
| 3     | 1.00      | 1.00   | 1.00     | 31      |
| 4     | 0.97      | 1.00   | 0.98     | 32      |
| 5     | 1.00      | 0.97   | 0.99     | 34      |
| 6     | 0.96      | 1.00   | 0.98     | 26      |
| 7     | 1.00      | 0.97   | 0.98     | 31      |
| 8     | 1.00      | 1.00   | 1.00     | 30      |
| 9     | 1.00      | 1.00   | 1.00     | 38      |
| 10    | 1.00      | 1.00   | 1.00     | 30      |
| 11    | 1.00      | 1.00   | 1.00     | 31      |
| 12    | 1.00      | 1.00   | 1.00     | 26      |
| 13    | 1.00      | 1.00   | 1.00     | 29      |
| **Exactitud**  |           |        | 1.00     | 420     |
| **Macro avg**  | 1.00      | 1.00   | 1.00     | 420     |
| **Weighted avg** | 1.00      | 1.00   | 1.00     | 420     |

Estos resultados sugieren que el MLP fue capaz de aprender correctamente la estructura de los datos sintéticos, lo que se tradujo en una precisión del 100% en la clasificación, sin embargo, el mismo modelo evaluado en los datos de test originales arrojó una precisión de 0.5. Esta discrepancia sugiere que la distribución de probabilidad de los datos sintéticos no es representativa de la que caracteriza a los datos reales, circuanstancia que estaría limitando la capacidad del AG para identificar un subconjunto relevante de características. Esto explicaría las motivos por los cuales los experimentos con datos aumentados no lograron mejorar la precisión de la clasificación.

En este punto, asumimos que un problema importante que tenía nuestro modelo de AVC se explicaba por la función de pérdida. En efecto,como vimos en el capítulo 2, dicha función resulta de la combinación de dos términos: la pérdida de reconstrucción y la divergencia KL. La divergencia KL es una medida de la diferencia entre dos distribuciones de probabilidad, y se utiliza para regularizar el espacio latente, mantenerlo distribuido y compacto; mientras que la pérdida de reconstrucción mide la diferencia entre los datos originales y los datos reconstruidos. En el caso de GCM, la divergencia KL podría estar dominando la función de pérdida, lo que resultaría en una distribución latente regularizada, pero no necesariamente representativa de los datos reales. Esto significa que el modelo está produciendo una distribución convenientemente discriminada, a expensas de la precisión en la reconstrucción de los datos originales. Situación particularmente problemática cuando los datos presentan relaciones complejas como en GCM. Ante esta problemática, entendimos que se abrían distintos caminos: podíamos ajustar los pesos de la divergencia KL y la reconstrucción para que el AVC sea capaz de generar datos que sean representativos de los datos reales, o bien, explorar la aplicación del AVC a un subespacio de características seleccionadas por un AG, lo que podría reducir la complejidad del problema y mejorar la calidad de los datos sintéticos. En la próxima parte de los experimentos, exploramos esta segunda opción. 

### Metodología Parte 2

Para abordar las limitaciones observadas, se realizaron ajustes en distintas configuraciones. En primer lugar, se adaptó la función de pérdida para incluir un factor de peso por clase que permitiera rebalancear el desequilibrio entre las clases. Ello con el fin de penalizar con mayor severidad los errores en las clases minoritarias, y así procurar una representación más fiel de la distribución real de los datos. En segundo lugar, se integró un muestreador aleatorio ponderado en la etapa de entrenamiento del AVC, que permitiría ajustar los lotes de entrenamiento con igual objetivo de mejorar la representación de las clases. Finalmente, se ajustó la etapa de inicio del proceso generativo, aplicando el AVC ya no al conjunto de datos completo, sino a un subconjunto de características seleccionadas por un AG. La idea era reducir la complejidad del problema y mejorar la calidad de los datos sintéticos, permitiendo al AVC enfocarse en un espacio de características reducido y relevante. Veamos esto último en detalle.

Como vimos, una de las limitaciones observadas en los experimentos iniciales fue la dificultad del AVC para generar datos sintéticos representativos de la distribución de los datos reales. Asumimos entonces, que aplicar el AVC a un subconjunto de características seleccionadas tendría como resultado una simplificación de la distribución de probabilidad original, y por lo tanto una mejora en la calidad de los datos sintéticos. Para ello, se diseñó un flujo de procesamiento que partía de un subespacio de características relavantes, y luego procedía a la generación de datos sintéticos con un AVC y la selección de características mediante un AG, tal cual el flujo de trabajo presentado en el Capítulo 3. 

La definición del subespacio de entrada partió de las características más frecuentes encontradas en los experimentos iniciales, donde se trabajó con una proporción de genes activos en el cromosoma que representaba el 10% de las características totales, al rededor de 1600 genes por cromosoma. Este subconjunto de características, que ya habían sido preseleccionadas por un AG, se utilizó como entrada para el AVC. A partir de este subconjunto, se aplicó el modelo generativo a la creación datos sintéticos, y luego se procedió a la selección de características mediante un AG. Este flujo de trabajo, consistente en el encademiento de tres procesos: *selección-generación-selección*, permitió concentrar el aprendizaje de los modelos en un subespacio de características.

Respecto de la configuración de los experimentos se implementaron las siguientes modificaciones: se ajustó el total de muestras sintéticas generadas por el AVC a 3000.Se redujo la proporción de genes activos en el cromosoma a 0.05 y 0.025 respecto del espacio original de características (~750 y ~450 atributos respectivamente). Se mantuvo constante la probabilidad de cruce en 0.75, como también la probabilidad de mutación en 0.1 (~0.0006). El número máximo de generaciones se redujo a 15, considerando la reducción en la complejidad del problema.

Como resultado de todas estas modificaciones, se esperaba que la calidad de los datos sintéticos mejorara, lo que se traduciría en una mejora en la precisión de la clasificación, y en la eficiencia de la selección de características.

### Resultados Parte 2

Efectivamente, los experimentos realizados en la segunda parte de la investigación mostraron una mejora significativa en la precisión de la clasificación. 

La precisión media en los experimentos con datos aumentados fue de 0.541, lo que representa un aumento de 8.85% en comparación con la precisión de los datos originales de 0.497. Esta diferencia es estadísticamente significativa, lo que indica que la aumentación de datos tuvo un impacto positivo en el desempeño del AG. 

![GCM Resultados parte 2](boxplot_gcm_combined.png)

Respecto de la selección de características, no se observa gran diferencia en el número de características seleccionadas entre los experimentos con datos originales y los datos aumentados. En efecto, el número de características promedio seleccionadas en los experimentos con aumentación y cromosomas de tamaño 0.05 y 0.025 fue de 453 y 784 respectivamente, en comparación con 461 y 781 en los experimentos con datos originales.

Para validar los resultados obtenidos, se realizó un grupo de experimentos adicionales donde se disminuyó la proporción de genes activos en el cromosoma a 0.015 y 0.003, lo que representó una selección de atributos en torno a ~200 y ~45. Los resultados mostraron que, pese a la importante reducción en el espacio de búsqueda, la precisión se mantuvo en un nivel aceptable en ambos grupos. La mejor performance del grupo con datos aumentados tiene lugar en los experimentos con reducción más agregisiva del cromosomas, con un tamaño 0.003 (~45). 

A continuación presentamos la serie completa de experimentos realizados en GCM, donde se puede observar la evolución de la precisión en función dla proporción de genes activos en el cromosoma. 

![GCM Resultados completos](boxplot_gcm_ngenes_accuracy.png)

Finalmente, se realizó un experimento con 6000 muestras sintéticas, donse se observó una degradación de los resultados. Esto sugiere que la generación de un número excesivo de muestras sintéticas puede comprometer la calidad de los datos y afectar negativamente la precisión de la clasificación.

Todo lo anterior sugiere que, si bien la aumentación de datos mediante AVs puede enfrentar desafíos significativos en conjuntos de datos complejos como gcm, es posible mejorar la calidad de los datos sintéticos mediante ajustes en la arquitectura del AV y en la metodología de selección de características. La combinación de AVs y AGs en un flujo de trabajo encadenado demostró ser una estrategia prometedora para mejorar la precisión y la eficiencia en la selección de características.

## All-Leukemia

Con el fin de validar el tratamiento dado a GCM, particularmente la estrategia de encadenamiento de procesos de *selección-generación-selección*, se decidió aplicar la misma metodología a un nuevo dataset *All-Leukemia*. Este dataset, que contiene 12600 variables (i.e. genes) y 327 muestras, es un caso de estudio clásico en la literatura de clasificación de tumores.

Se repitió el procedimiento de busqueda de la mejor combinación de hiperparámetros para el AVC, con el fin de lograr la mejor reconstrucción de los datos originales. Se mantuvo la misma configuración del modelo AVC utilizado en GCM, descripto en el Capítulo 2.

Al igual que en GCM, se realizaron experimentos con datos originales y aumentados. Los experimentos con aumentación consistieron en la generación de 1000 muestras sintéticas creadas por un AVC entrenado en un subespacio de características relevantes, seleccionadas por un AG. Tanto para la generación como para la selección siempre se trabajó con las particiones de datos originales de entrenamiento y teteo. 

Se probaron 3 escenarios de reducción de características, con un ratio de genes activos de 0.003, 0.015 y 0.025, dando como resultado un tamaño de cromosoma a torno a las 35, 180 y 300 variables. El algoritmo genético utilizado en esta oportunidad realizaba 15 generaciones.

Los resultados generales se pueden observar en el siguiente gráfico.

![All-Leukemia Resultados](boxplot_all_leukemia_ngenes_accuracy.png)

Como puede advertirse, los experimentos muestran que la estrategia de encadenamiento de procesos de *selección-generación-selección* presenta una ligera mejora en la precisión de clasificación en los casos de drástica reducción de características en torno a los 35 y 200 genes activos. No se evidencia diferencia significativa entre los experimentos con 300 genes activos. 

Respecto de la eficacia en la selección de características, no se observa una diferencia significativa entre los experimentos con datos originales y los datos aumentados como puede apreciarse en los siguientes gráficos.

![All-Leukemia Resultados](boxplot_all_leukemia_35_med_combined.png)

![All-Leukemia Resultados](boxplot_all_leukemia_180_med_combined.png)

En el caso del experimento con 35 genes activos (promedio) se advierte, pese a la mejora en la precisión, una leve degradación en la estabilidad de los resultados. Entendemos que esto se explica por la reducción en la cantidad de genes activos y la sensibilidad del AG a parámetros como la probabilidad de mutación. En este caso, el experimento tuvo una probabilidad de mutación de 0.028, operando una importante variabilidad en la estructura de los cromosomas y afectando la estabilidad de la selección.

## Resumen de los resultados

Los experimentos realizados en los cuatro conjuntos de datos (*Leukemia*, *Gisette*, *Madelon* y *GCM*) evidencian una tendencia general sobre el impacto de la aumentación de datos mediante Autocodificacadores Variacionales en la selección de características utilizando Algoritmos Genéticos. 

![Precisión en los 4 datasets](boxplot_resultados_precision.png)

![Número de características seleccionadas en los 4 datasets](boxplot_resultados_ngenes.png)


Dicha tendencia tiene confirmación en los resultados obtenidos en el dataset *All-Leukemia*:

![Precisión en el dataset de validación](boxplot_all_leukemia_ngenes_accuracy.png)

El uso de datos aumentados no siempre produjo mejoras estadísticamente significativas en la precisión de clasificación, pero sí marcó una diferencia relevante ante los casos más difíciles. En efecto, en problemas con métricas saturadas su aporte agrega un valor marginal, asociado a la estabilidad de los resultados, como se vió en *Leukemia* y *Gisette*. Ambos datasets representan desafíos donde los modelos ya alcanzan una alta performance procesando los datos en su estado original. Sin embargo, en dataset más complejos, donde el margen de majora en las predicciones es mayor, como los casos de *Madelon*, *GCM* y *All-Leukemia*, la técnica de aumentación generó resultados positivos, particularmente en los dos primeros donde se observó un salto del 9% y 10% en el resultado final. Este hallazgo sugiere que la aumentación de datos puede contribuir a generar modelos más consistentes a lo largo de múltiples generaciones, lo cual es particularmente relevante en escenarios de alta dimensionalidad y escasez muestral.

En el caso del conjunto de datos *Madelon*, donde el problema de selección de características presenta una clara distinción entre señales relevantes y ruido, la aumentación de datos mostró una mejora significativa en la precisión, incrementando la capacidad del AG para identificar las características correctas en un espacio de búsqueda extremadamente ruidoso. Este resultado resalta la utilidad de la aumentación en problemas donde la presencia de señales distractoras dificulta la tarea de selección.

Por otro lado, los experimentos con *GCM* ilustran las limitaciones de la aumentación en conjuntos de datos con una distribución de clases desbalanceada y de alta complejidad. Sin embargo los ajustes en la arquitectura del AVC y en la configuración experimental permitieron mejoras importantes, alcanzando una generación de datos sintéticos de mayor calidad y una selección de características más eficiente. Resultados que se validaron mediante los experimentos en *All-Leukemia*. En este sentido, la integración de estrategias más complejas, como la combinación de selección de características previa a la aumentación, demostró ser una vía prometedora para mejorar los resultados.

En conjunto, los hallazgos de este capítulo sugieren que la aumentación de datos, cuando se utiliza en combinación con AGs, puede ser una herramienta efectiva en la selección de características, especialmente en contextos donde la dimensionalidad y el ruido dificultan la tarea. Asimismo, se destaca que el éxito de esta técnica depende críticamente de la calidad de los datos sintéticos generados y de la adecuada configuración de los modelos subyacentes, lo que subraya la importancia de ajustar las metodologías de manera específica para cada tipo de conjunto de datos.

<!-- 

# Experimentos de selección de características con AGs y datos sintéticos


Otro aspecto importante en la función de aptitud es la minimización del número de evaluaciones necesarias para alcanzar el óptimo o una solución lo suficientemente cercana a este. En muchos casos, cada evaluación de aptitud puede ser costosa en términos de tiempo y recursos computacionales, especialmente cuando la evaluación implica la simulación de modelos complejos o el entrenamiento de algoritmos de aprendizaje automático. Por ello, reducir el número de evaluaciones de aptitud es fundamental para mejorar la eficiencia del AG, sin sacrificar la calidad de las soluciones generadas. Este aspecto fue particularmente relevante en nuestra investigación, donde la evaluación de la función de aptitud implicaba el entrenamiento y validación de modelos de clasificación en conjuntos de datos de alta dimensionalidad. La técnica de paralelización y la evaluación diferencial de las soluciones fueron estrategias clave para reducir el tiempo de ejecución, aunque demandó una infraestructura computacional adecuada (más sobre esto en el próximo capítulo).

 -->

